{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TruscaPetre/AAIT-Nosy-Missing-Labels/blob/main/tutorial%20colab%20noisy%20labels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaRT8CyQPpcv"
      },
      "source": [
        "# Noisy labels problem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOThg22n_IX7"
      },
      "source": [
        "## Introduction theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJxksI3i_IX-"
      },
      "source": [
        "Before starting to implement anything we should think about the problem first. We don't know which of the training samples are wrong, and we cannot try to correct them by hand because there are too many of them and the images are not very clear anyway.\n",
        "\n",
        "We need some Noise-aware training techniques. Some ideas are:\n",
        "- training using a loss function that is designed to be robust to label noise. Such as:\n",
        "  - focal loss, it down-weighs the contribution of easy examples and puts more ephasis on difficult examples, which can make it resistant to noise. `torch.nn.FocalLoss`\n",
        "  - Generalized Cross-Entropy loss, which includes additional hyperparameters that allow the model to learn the noise rate and label corruption matrix. `torch.nn.GCE`\n",
        "- bootstrapping\n",
        "- self-ensembling to learn multiple models that are more robust to noise\n",
        "- Confidence based method, because predicting the confidence may allow humans to discard the least confidence examples. ( but we are not creating a real world model, we are getting tested automatically with a test set, so this method cannot work.)\n",
        "- Pseudo-labeling, using the model's own predictions to re-label a portion of the training data. Similar to self-training, we eliminate completely labels of some samples and we try to predict them. In case the labels are the same after relabling, there is a higher chance they are correct. We would gradually relable the images into a better dataset. \n",
        "    - Or Computing a probability that the labels are misslabeled, than using that probability to weigh heavier on the labels that have a higher chance of being correct during training. `torch.nn.CrossEntropyLoss`\n",
        "- Noise tolerant algorithms: complex deep learning models are more tolerant to noise. (this is not a very solid technique but should be used together with others.)\n",
        "\n",
        "References:\n",
        "- Focal loss: This loss function was first introduced in the paper \"Focal Loss for Dense Object Detection\" by Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár (https://arxiv.org/abs/1708.02002).\n",
        "- Generalized cross-entropy loss: This loss function was proposed in the paper \"Learning with Noisy Labels\" by Mengye Ren, Elad Hazan, and Yoram Singer (https://arxiv.org/abs/1609.03683).\n",
        "- Bootstrapping: This technique involves training multiple models on different subsets of the training data, and then combining their predictions to make a final prediction. It was first introduced in the paper \"Bagging Predictors\" by Leo Breiman (https://link.springer.com/article/10.1023/A:1018054314350).\n",
        "- Self-ensembling: This technique involves training multiple models on the same data, and then using their predictions to create a final prediction. It was first introduced in the paper \"Self-Ensembling for Visual Domain Adaptation\" by Sergey Zagoruyko and Nikos Komodakis (https://arxiv.org/abs/1706.05208).\n",
        "- Pseudo-labeling: This technique involves using the model's own predictions to label a portion of the training data. It was first introduced in the paper \"Semi-Supervised Learning with Deep Generative Models\" by Diederik Kingma, Danilo Jimenez Rezende, Shakir Mohamed, and Max Welling (https://arxiv.org/abs/1406.5298).\n",
        "- Noise-aware training: This refers to techniques that are specifically designed to handle label noise in the training data. One example of such a technique is the generalized cross-entropy loss function, which was introduced in the paper \"Learning with Noisy Labels\" by Mengye Ren, Elad Hazan, and Yoram Singer (https://arxiv.org/abs/1609.03683).\n",
        "- Confidence based methods: This refers to techniques that involve training a model to predict the confidence or probability of each class label, rather than just the class label itself. This can allow the model to identify examples that are less confident and potentially more prone to noise. One example of a confidence-based method is the method of \"bootstrapping,\" which was introduced in the paper \"Bagging Predictors\" by Leo Breiman (https://link.springer.com/article/10.1023/A:1018054314350)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeBHHsZt_IYU"
      },
      "source": [
        "## Pseudo Labeling solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnkncVA__IYW"
      },
      "source": [
        "\n",
        "This technique is barely described above\n",
        "\n",
        "The first question is what portion of the dataset should I \"forget\" the labels and try predict new ones(by training on the remaining ones)? \n",
        "\n",
        "Making the worse assumption that half of the labels are wrong. Than we should leave out a smaller portion of the labels to be re-labeled.\n",
        "But because we have a rather large dataset of 500 images per class and that the model we are using is a rather complex one which is already pretrained, we may increase the number of labels for re-labeling. \n",
        "\n",
        "According to previous argument I chose to separate the dataset in 10 parts. \n",
        "I will leave 10% of the dataset out and only use the rest of 90%. \n",
        "Similar to a 10-fold cross validation, but instead of validating we are re-labeling the remaining 10% of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmzRZhui_IYY"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x_I1fk99_IYZ"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "import shutil\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import SubsetRandomSampler, Dataset, DataLoader\n",
        "from torchvision import transforms, datasets \n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import itertools\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yyOZc59J_IYc"
      },
      "outputs": [],
      "source": [
        "task2_id = \"1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCcacF4f_IYe",
        "outputId": "c19a4eb9-5373-489c-b1d2-b2b932ea3a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-19 18:37:33--  https://docs.google.com/uc?export=download&confirm=t&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.10.138, 142.251.10.101, 142.251.10.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.10.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-8k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/l6i8f33bttsueieqhgf7ckour82p22me/1674153450000/08997952672865575084/*/1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL?e=download&uuid=a5475b9c-b253-45e2-bc63-9e72e3af5048 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-01-19 18:37:33--  https://doc-04-8k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/l6i8f33bttsueieqhgf7ckour82p22me/1674153450000/08997952672865575084/*/1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL?e=download&uuid=a5475b9c-b253-45e2-bc63-9e72e3af5048\n",
            "Resolving doc-04-8k-docs.googleusercontent.com (doc-04-8k-docs.googleusercontent.com)... 142.251.12.132, 2404:6800:4003:c11::84\n",
            "Connecting to doc-04-8k-docs.googleusercontent.com (doc-04-8k-docs.googleusercontent.com)|142.251.12.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75324736 (72M) [application/x-gzip]\n",
            "Saving to: ‘task2.tar.gz’\n",
            "\n",
            "task2.tar.gz        100%[===================>]  71.83M  28.8MB/s    in 2.5s    \n",
            "\n",
            "2023-01-19 18:37:36 (28.8 MB/s) - ‘task2.tar.gz’ saved [75324736/75324736]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# replace here your ide &id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\"\n",
        "# replace here your id 'https://docs.google.com/uc?export=download&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL'\n",
        "# replace here your target name -O task1.tar.gz &&\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\" -O task2.tar.gz && rm -rf /tmp/cookies.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1bkbvqeF_IYf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!mkdir data\n",
        "!mv task2.tar.gz ./data\n",
        "!tar -xzvf \"/content/data/task2.tar.gz\" -C \"/content/data/\"     #[run this cell to extract tar.gz files]\n",
        "# this may take 12 seconds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_info = {\n",
        "    \"iteration\": 1,\n",
        "    \"image_processing\":{\n",
        "        \"resize\":224,\n",
        "        \"mean\":[0.485, 0.456, 0.406],\n",
        "        \"std\":[0.229, 0.224, 0.225],\n",
        "    },\n",
        "    \"hyperparameters_data\": {\n",
        "        \"batch_size\":32,\n",
        "        \"shuffle_dataloader\":True,\n",
        "        \"num_workers\":4\n",
        "    },\n",
        "    \"random_seeds\":{\n",
        "        \"torch_seed\":42,\n",
        "        \"numpy_seed\":42,\n",
        "        \"cuda_seed\":42,\n",
        "\n",
        "    },\n",
        "    \"hyperparameters_training\":{\n",
        "        \"learning_rate\": 0.0001,\n",
        "        \"scheduler_step_size\":7,\n",
        "        \"scheduler_gamma\":0.1,\n",
        "        \"num_epochs\":10, \n",
        "    },\n",
        "    \"total_unlabeled\":26445,\n",
        "}\n",
        "     "
      ],
      "metadata": {
        "id": "DLqmpUlecPCx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euHUUz0w_IYg"
      },
      "source": [
        "## Mount drive \n",
        "( in order to save each iteration of leave out labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R4ZCnSB_IYh",
        "outputId": "7f647fa6-850a-4b74-fed6-72c40c8c58fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j0gmw68_IYi",
        "outputId": "3ccc82e0-c591-4dab-c42d-fde67184ef77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Google Drive!"
          ]
        }
      ],
      "source": [
        "with open('/gdrive/MyDrive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat '/gdrive/MyDrive/foo.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0w0JOuB5_IYj"
      },
      "outputs": [],
      "source": [
        "!rm '/gdrive/MyDrive/foo.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "-g7qmzT4Wkbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShuffledImageFolder(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.indices = torch.randperm(len(dataset))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path, _ = self.dataset.samples[self.indices[index]]\n",
        "        image, label = self.dataset[self.indices[index]]\n",
        "        return image, label, path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "metadata": {
        "id": "Ci_KHZYMWqgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "toHy-rW4_IYr"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, scheduler, optimizer, criterion, dataset_size, dataloader):\n",
        "                     \n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over data.\n",
        "    for inputs, labels, _ in tqdm(dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "    \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_loss = running_loss / dataset_size\n",
        "    epoch_acc = running_corrects.double() / dataset_size\n",
        "    return model, epoch_loss, epoch_acc\n",
        "\n",
        "def train_model(model, *args, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()  # Set model to training mode\n",
        "        model, epoch_loss, epoch_acc = train_loop(model, *args)\n",
        "        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # deep copy the model\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_number, device='cuda'):\n",
        "    # Download a pytorch MobileNet pretrained model\n",
        "    model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "    # change the Linear output to fit our dataset ( because model initially has 1000 classes)\n",
        "    model.classifier[1] = nn.Linear(1280, 100)\n",
        "    # Read the models from drive that need to be used for the predictions\n",
        "    i=0\n",
        "    PATH = f'/gdrive/MyDrive/checkpoints/noisy_labels/model_it_{model_number}.pt'\n",
        "    model.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))\n",
        "    model = model.to(device) \n",
        "    model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "-gh3Y2c5PFYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_paths):\n",
        "        self.image_paths = image_paths\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # Load the image and return it as a tensor\n",
        "        image_path = self.image_paths[index]\n",
        "        image = Image.open(image_path)\n",
        "        image = image.convert('RGB')\n",
        "        image = transforms.ToTensor()(image)\n",
        "        label = int(image_path.split(\"/\")[-2])\n",
        "        return image, label, image_path"
      ],
      "metadata": {
        "id": "Z7xQs8c7bkQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batches_to_list(batches):\n",
        "    paths_list = []\n",
        "    for batch in batches:\n",
        "        paths_list += batch\n",
        "    return paths_list\n",
        "\n",
        "def generate_new_labels(model, loader, device='cuda'):\n",
        "    # create new predictions\n",
        "    \n",
        "    predicted_labels = {\n",
        "        \"path\":[],\n",
        "        \"confidence\":[],\n",
        "        \"existing_label\":[],\n",
        "        \"label_predicted\":[]\n",
        "    }\n",
        "\n",
        "    # Each epoch has a training and validation phase \n",
        "    model.eval()   # Set model to evaluate mode \n",
        "\n",
        "    # Iterate over data. \n",
        "\n",
        "    for image, label, image_path in tqdm(loader):\n",
        "        image = image.to(device)\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(False): # we don't want to train\n",
        "            outputs = model(image)\n",
        "            confidence, preds = torch.max(outputs, 1) \n",
        "\n",
        "        predicted_labels[\"path\"].append(image_path) \n",
        "        predicted_labels[\"confidence\"].append(confidence.item())\n",
        "        predicted_labels[\"label_predicted\"].append(preds.item())\n",
        "        predicted_labels[\"existing_label\"].append(label.item())\n",
        "\n",
        "    return predicted_labels"
      ],
      "metadata": {
        "id": "ASudnmb56uIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop2(model, scheduler, optimizer, criterion, dataset_size, dataloader1, dataloader2):\n",
        "                     \n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over data for confident data.\n",
        "    for inputs, labels in tqdm(dataloader1):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "    \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over data for relabling.\n",
        "    for inputs, labels, _ in tqdm(dataloader2):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "    \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_loss = running_loss / dataset_size\n",
        "    epoch_acc = running_corrects.double() / dataset_size\n",
        "    return model, epoch_loss, epoch_acc\n",
        "\n",
        "def train_model2(model, *args, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()  # Set model to training mode\n",
        "        model, epoch_loss, epoch_acc = train_loop2(model, *args)\n",
        "        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # deep copy the model\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "tO_cnZt4O-ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21Vc-VIr_IYk"
      },
      "source": [
        "## Seting up things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JQr89Fhv_IYl"
      },
      "outputs": [],
      "source": [
        "# Set up seeds for reproducibility\n",
        "np.random.seed(experiment_info[\"random_seeds\"][\"numpy_seed\"])\n",
        "torch.manual_seed(experiment_info[\"random_seeds\"][\"torch_seed\"])\n",
        "torch.cuda.manual_seed_all(experiment_info[\"random_seeds\"][\"cuda_seed\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "M51baF3B_IYn"
      },
      "outputs": [],
      "source": [
        "dir_data = 'data/task2/train_data/'\n",
        "# Read the annotations file into a DataFrame\n",
        "df = pd.read_csv(f'{dir_data}annotations.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "n9G-lt1HbzUp",
        "outputId": "2e09f3f4-c997-466e-f7be-d2e6a48731db"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     renamed_path  label_idx\n",
              "0          task2/train_data/images/labeled/0.jpeg          0\n",
              "1          task2/train_data/images/labeled/1.jpeg          0\n",
              "2          task2/train_data/images/labeled/2.jpeg          0\n",
              "3          task2/train_data/images/labeled/3.jpeg          0\n",
              "4          task2/train_data/images/labeled/4.jpeg          0\n",
              "...                                           ...        ...\n",
              "49995  task2/train_data/images/labeled/49995.jpeg          5\n",
              "49996  task2/train_data/images/labeled/49996.jpeg         94\n",
              "49997  task2/train_data/images/labeled/49997.jpeg         24\n",
              "49998  task2/train_data/images/labeled/49998.jpeg         85\n",
              "49999  task2/train_data/images/labeled/49999.jpeg          5\n",
              "\n",
              "[50000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-433c70c7-aa9e-4a7d-aba7-b3add59b5295\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>renamed_path</th>\n",
              "      <th>label_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>task2/train_data/images/labeled/0.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>task2/train_data/images/labeled/1.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>task2/train_data/images/labeled/2.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>task2/train_data/images/labeled/3.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>task2/train_data/images/labeled/4.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>task2/train_data/images/labeled/49995.jpeg</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>task2/train_data/images/labeled/49996.jpeg</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>task2/train_data/images/labeled/49997.jpeg</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>task2/train_data/images/labeled/49998.jpeg</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>task2/train_data/images/labeled/49999.jpeg</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-433c70c7-aa9e-4a7d-aba7-b3add59b5295')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-433c70c7-aa9e-4a7d-aba7-b3add59b5295 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-433c70c7-aa9e-4a7d-aba7-b3add59b5295');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhxmBea3_IYo",
        "outputId": "0d95ee4e-14ee-4880-eecf-f5e95c9078d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/task2/images_by_class does not exist\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:15<00:00, 3158.18it/s]\n"
          ]
        }
      ],
      "source": [
        "# Define the base directory\n",
        "\n",
        "base_dir = 'data/task2/train_data/images'\n",
        "# Check if the directory exists, to recreate it instead of messing it up\n",
        "target_dir = 'data/task2/images_by_class'\n",
        "\n",
        "if os.path.exists(target_dir):\n",
        "    # Use rmtree to delete the directory and all its contents\n",
        "    shutil.rmtree(target_dir)\n",
        "    print(f'{target_dir} has been deleted')\n",
        "else:\n",
        "    print(f'{target_dir} does not exist')\n",
        "\n",
        "# Iterate over the rows in the DataFrame\n",
        "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    # Extract the path and class from the row\n",
        "    path = row['renamed_path']\n",
        "    label = row['label_idx']\n",
        "    \n",
        "    # Create the directory for the class\n",
        "    class_dir = f'{target_dir}/{label}'\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    \n",
        "    # Copy the file to the class directory\n",
        "    shutil.copy(f\"data/{path}\", class_dir) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iteration 1"
      ],
      "metadata": {
        "id": "ZtQ5oHh6XdJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create datset splits for pseudo-labeling"
      ],
      "metadata": {
        "id": "FSoVcX3tdmlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(experiment_info[\"image_processing\"][\"resize\"]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=experiment_info[\"image_processing\"][\"mean\"],\n",
        "                         std=experiment_info[\"image_processing\"][\"std\"]),\n",
        "    ])"
      ],
      "metadata": {
        "id": "pOGkTh1scJR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'data/task2/images_by_class'\n",
        "image_dataset_unshuffled = datasets.ImageFolder(data_dir, preprocess)\n",
        "image_dataset = ShuffledImageFolder(image_dataset_unshuffled)"
      ],
      "metadata": {
        "id": "7iflapmiM4QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = len(image_dataset)\n",
        "\n",
        "# Split the image_dataset into 10 parts\n",
        "part_size = num_samples // 10\n",
        "parts = [list(range(i * part_size, (i + 1) * part_size)) for i in range(10)]\n",
        "\n",
        "batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"]\n",
        "# Use one part for creating new labels and the other 9 parts for training the model\n",
        "loaders = {}\n",
        "for i in range(10):\n",
        "    # Get the indices for the training set, which are not in the labeling set\n",
        "    train_indices = [index for j, part in enumerate(parts) if j != i for index in part]\n",
        "    labeling_indices = parts[i]\n",
        "\n",
        "    # Create a sampler for the training set and the labeling set\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    labeling_sampler = SubsetRandomSampler(labeling_indices)\n",
        "\n",
        "    # Use the samplers to create data loaders for the training set and the labeling set\n",
        "    train_loader = DataLoader(image_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    labeling_loader = DataLoader(image_dataset, batch_size=batch_size, sampler=labeling_sampler)\n",
        "    loaders[i] = {\n",
        "        \"train\":train_loader, \n",
        "        \"labeling\":labeling_loader\n",
        "    }\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data_sizes = {\n",
        "    \"train\" : len(loaders[0][\"train\"]), # this is the number of batches not images\n",
        "    \"labeling\" : len(loaders[0][\"labeling\"]),\n",
        "}"
      ],
      "metadata": {
        "id": "aMpLE-QAvxFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sanity check\n",
        "See how many labels are in each training set\n",
        "Visualize the distribution of labels"
      ],
      "metadata": {
        "id": "J1WLWbz0VBiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(len(loaders[0][\"train\"])*batch_size)\n",
        "print(len(loaders[0][\"labeling\"])*batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrjSwJ29xbgv",
        "outputId": "6736d904-0171-410a-9774-726ea84f64a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45024\n",
            "5024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check, verify the distribution of labels in training and labeling sets\n",
        "labels_for_distribution = [label_batch for _,label_batch,_ in tqdm(loaders[0][\"train\"])]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GIbW5b8TW-B",
        "outputId": "f8d448c9-864d-4df6-d739-070cf41ceec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [01:23<00:00, 16.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_distrib = torch.cat(labels_for_distribution)"
      ],
      "metadata": {
        "id": "VkOog_OqUEON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts, bins, patches = plt.hist(labels_distrib, bins = 100)\n",
        "\n",
        "# Set x-axis label\n",
        "plt.xlabel('Labels')\n",
        "plt.xticks(bins, bins.astype(int))\n",
        "# Set y-axis label\n",
        "plt.ylabel('Number of Observations')\n",
        "\n",
        "# Set plot title\n",
        "plt.title('Distribution of Labels')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "z1Mj6kPMuCPN",
        "outputId": "7eea9428-09c3-49f3-fbfd-6d84b798d4a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Distribution of Labels')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on the remaining data "
      ],
      "metadata": {
        "id": "dgSVdENHZedr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a pytorch MobileNet pretrained model\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "# change the Linear output to fit our dataset ( because model initially has 1000 classes)\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "# Setting hyperparameters\n",
        "\n",
        "model = model.to(device)\n",
        "print(model.classifier)\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "eb68d62d599a462f92aaca0ab0b854e3",
            "6eed1376f1f24e8abbe2d4ea0ca3fcb1",
            "07c37bb6b22b4c8abe8d16b61e28e1db",
            "15a23332521441cab29f7acedc079b16",
            "816420dd910b475faa99a8e9ef6eebd4",
            "71a16d1906924e738fc1baa304d94811",
            "a74028e913de43d3b1eae97433d91882",
            "07868acd1960497680b904b81fd0eaf9",
            "a56ed7ebe21c4011a23a63cad606d40c",
            "ba437891973d4629b4881ccb11f044ae",
            "a75d7f60d1c24475a1540bc1cfe94f7b"
          ]
        },
        "id": "C69DiUz3ZkEV",
        "outputId": "2eab0188-0052-48d7-de51-10af01ec0845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb68d62d599a462f92aaca0ab0b854e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=False)\n",
            "  (1): Linear(in_features=1280, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=experiment_info[\"hyperparameters_training\"][\"learning_rate\"],\n",
        "    )\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(\n",
        "    optimizer, \n",
        "    step_size=experiment_info[\"hyperparameters_training\"][\"scheduler_step_size\"], \n",
        "    gamma=experiment_info[\"hyperparameters_training\"][\"scheduler_gamma\"],\n",
        "    )\n",
        "     "
      ],
      "metadata": {
        "id": "8Mprko1yaAI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(model, \n",
        "                    exp_lr_scheduler, \n",
        "                    optimizer, \n",
        "                    criterion,  \n",
        "                    data_sizes[\"train\"], \n",
        "                    loaders[0][\"train\"], \n",
        "                    num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmCqNZ-saHr3",
        "outputId": "badfeed4-5771-4c1f-eca6-47f93af79d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:26<00:00,  5.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 72.5423 Acc: 13.7484\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 45.0769 Acc: 19.0071\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 36.1820 Acc: 20.9026\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 29.3312 Acc: 22.5622\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 23.8855 Acc: 24.1471\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 19.0713 Acc: 25.5572\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 15.4470 Acc: 26.7960\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 8.7784 Acc: 29.4200\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 6.8999 Acc: 30.1770\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 5.9939 Acc: 30.5394\n",
            "\n",
            "Training complete in 42m 54s\n",
            "Best val Acc: 30.539446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repeat process for the other iterations"
      ],
      "metadata": {
        "id": "q8xaVIRwm8PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# because unlike the missing labels example, the iterations are independent from eachother\n",
        "# We can train all the models at once, than make predictions for all the labels at once\n",
        "# this makes the things simpler"
      ],
      "metadata": {
        "id": "jIZWJhNplVOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), '/gdrive/MyDrive/checkpoints/noisy_labels/model_it_0.pt')"
      ],
      "metadata": {
        "id": "L5s4zlU1j_Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,10): \n",
        "    model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "    model.classifier[1] = nn.Linear(1280, 100)\n",
        "    model = model.to(device) \n",
        "    model = train_model(model, \n",
        "                    exp_lr_scheduler, \n",
        "                    optimizer, \n",
        "                    criterion,  \n",
        "                    data_sizes[\"train\"], \n",
        "                    loaders[i][\"train\"], \n",
        "                    num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])\n",
        "    torch.save(model.state_dict(), f'/gdrive/MyDrive/checkpoints/noisy_labels/model_it_{i}.pt')"
      ],
      "metadata": {
        "id": "Zq3Ax7_Pm6By",
        "outputId": "8328961f-1389-437e-f643-02c9b3034311",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.3708 Acc: 0.3042\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.4221 Acc: 0.3113\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.4296 Acc: 0.3149\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.4003 Acc: 0.3369\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.4245 Acc: 0.3028\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.4086 Acc: 0.3220\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.4538 Acc: 0.3298\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.3897 Acc: 0.3390\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.4531 Acc: 0.3177\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.4248 Acc: 0.3383\n",
            "\n",
            "Training complete in 43m 0s\n",
            "Best val Acc: 0.339019\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5652 Acc: 0.2687\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5455 Acc: 0.2694\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5926 Acc: 0.2608\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5716 Acc: 0.2580\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5509 Acc: 0.2523\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5032 Acc: 0.2587\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5528 Acc: 0.2601\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5674 Acc: 0.2715\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5859 Acc: 0.2694\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5112 Acc: 0.2559\n",
            "\n",
            "Training complete in 42m 32s\n",
            "Best val Acc: 0.271500\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1574 Acc: 0.2992\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2140 Acc: 0.3021\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2206 Acc: 0.2822\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1759 Acc: 0.2878\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1663 Acc: 0.2999\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2137 Acc: 0.2978\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2461 Acc: 0.2701\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1719 Acc: 0.3205\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2607 Acc: 0.2857\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1828 Acc: 0.2935\n",
            "\n",
            "Training complete in 42m 33s\n",
            "Best val Acc: 0.320540\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5430 Acc: 0.3326\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6434 Acc: 0.3603\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5310 Acc: 0.3497\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6065 Acc: 0.3319\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5749 Acc: 0.3539\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5546 Acc: 0.3490\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5303 Acc: 0.3632\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6104 Acc: 0.3340\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5767 Acc: 0.3653\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5474 Acc: 0.3547\n",
            "\n",
            "Training complete in 42m 58s\n",
            "Best val Acc: 0.365316\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.9253 Acc: 0.4030\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.8417 Acc: 0.4108\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.9271 Acc: 0.4200\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.9143 Acc: 0.4335\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.9639 Acc: 0.4172\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.8806 Acc: 0.4193\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.9342 Acc: 0.4151\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.9048 Acc: 0.4080\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.9162 Acc: 0.3980\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.8818 Acc: 0.4271\n",
            "\n",
            "Training complete in 42m 57s\n",
            "Best val Acc: 0.433547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.7060 Acc: 0.3689\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6360 Acc: 0.3234\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6773 Acc: 0.3369\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6889 Acc: 0.3383\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6507 Acc: 0.3525\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6708 Acc: 0.3333\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6872 Acc: 0.3184\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6516 Acc: 0.3198\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6673 Acc: 0.3276\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6903 Acc: 0.3611\n",
            "\n",
            "Training complete in 42m 51s\n",
            "Best val Acc: 0.368870\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2038 Acc: 0.3362\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1416 Acc: 0.3326\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1803 Acc: 0.3362\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1441 Acc: 0.3170\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1624 Acc: 0.3227\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1674 Acc: 0.3291\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1685 Acc: 0.3468\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1829 Acc: 0.3255\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1622 Acc: 0.3390\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1886 Acc: 0.3177\n",
            "\n",
            "Training complete in 43m 9s\n",
            "Best val Acc: 0.346837\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8979 Acc: 0.2402\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.9184 Acc: 0.2082\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8989 Acc: 0.2701\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8834 Acc: 0.2274\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8732 Acc: 0.2459\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8074 Acc: 0.2530\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8743 Acc: 0.2672\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8564 Acc: 0.2367\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8534 Acc: 0.2360\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8483 Acc: 0.2523\n",
            "\n",
            "Training complete in 43m 8s\n",
            "Best val Acc: 0.270078\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2236 Acc: 0.2345\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2041 Acc: 0.2537\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2436 Acc: 0.2687\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2801 Acc: 0.2495\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2507 Acc: 0.2900\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2444 Acc: 0.2509\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1874 Acc: 0.2580\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1854 Acc: 0.2786\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2250 Acc: 0.2679\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2056 Acc: 0.2694\n",
            "\n",
            "Training complete in 42m 57s\n",
            "Best val Acc: 0.289979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the labels that will be used for relabling in a file, such that you can leave the training of the model alone, and later when you return, even if the session has finished, you have the models trained and you know which are the images that need new labels. And you can use the checkpoints to predict them"
      ],
      "metadata": {
        "id": "ZsTeJsCocwyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paths_labeling = {}\n",
        "for i in tqdm(range(10)):\n",
        "    paths_labeling[i] = []\n",
        "    for images, labels, paths in loaders[i]['labeling']:\n",
        "        paths_labeling[i].append(paths)"
      ],
      "metadata": {
        "id": "iXT_CNZKny20",
        "outputId": "6af4be55-4996-48f9-9617-4d58fa769aa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:23<00:00,  8.37s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the experiment information to a JSON file\n",
        "with open(f'/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_images_for_models.json', 'w') as f:\n",
        "    json.dump(paths_labeling, f)"
      ],
      "metadata": {
        "id": "-Z4e4GA3o37v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict new labels"
      ],
      "metadata": {
        "id": "3K66q0bu_NGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the json of the labels that need to be predicted\n",
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_images_for_models.json', 'r') as f:\n",
        "    paths_labeling = json.load(f)\n"
      ],
      "metadata": {
        "id": "2qtPuBm_OwUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create new dataset for pseudo-labeling\n",
        "all_predicted_labels = {}\n",
        "for iteration, paths in paths_labeling.items(): \n",
        "    paths_list = batches_to_list(paths)\n",
        "    dataset = ImageDataset(paths_list)    \n",
        "    labeling_loader = DataLoader(dataset, batch_size=1)\n",
        "    model = load_model(iteration, 'cuda')\n",
        "    all_predicted_labels[iteration] = generate_new_labels(model, labeling_loader, 'cuda')  \n",
        "    # 00:33<00:00, 149.59it/s on standard GPU\n",
        "    # [00:18<02:44, 27.31it/s] on standard TPU\n"
      ],
      "metadata": {
        "id": "n4o4h4r5bseB",
        "outputId": "fce9abfa-4ba3-49c8-ccfd-e823bbdd40e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 164.74it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:31<00:00, 157.75it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 163.49it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 163.92it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:31<00:00, 156.30it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:31<00:00, 161.24it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 161.34it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 161.48it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:31<00:00, 160.84it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 162.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "for i in all_predicted_labels.items():\n",
        "    print(i[1]['path'][0])\n",
        "    print(i[1]['confidence'][0])\n",
        "    print(i[1]['label_predicted'][0])\n",
        "    print(i[1]['existing_label'][0])\n",
        "    break"
      ],
      "metadata": {
        "id": "Os7BEdDFiI_0",
        "outputId": "d85f068c-fc7a-427e-e510-51676b1c8d22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('data/task2/images_by_class/3/1383.jpeg',)\n",
            "0.3645797073841095\n",
            "19\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the new predictions and the old predictions in a file for future \n",
        "with open(f'/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_first_iteration.json', 'w') as f:\n",
        "    json.dump(all_predicted_labels, f)"
      ],
      "metadata": {
        "id": "KuNeoaTNhFEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze results pseudo-labeling"
      ],
      "metadata": {
        "id": "Y5GTYn7k4WzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have created new predictions for the labels. But I have only 1 label extra label for each prediction. In order to figure out which labels are correct and which ones are not. I want to use the confidence too. Not only rely on more sessions of pseudo-labeling. Because 1 session costs me about 6 hours anyway. Using the confidence for labeling would be to use a rule similar to these ones: If the labels have the same label and there is a high confidence, I will consider the previous label as valid. If the labels are similar and the confidence is very low, I will try to run this label in another session of relabeling. If the label is different but the confidence is very low, it is probably my mistake. But if the label is different and the confidence is very high. I should probably try to create a new label for that image in another session. "
      ],
      "metadata": {
        "id": "lgF62cD4h2JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_first_iteration.json', 'r') as f:\n",
        "    all_predicted_labels = json.load(f)"
      ],
      "metadata": {
        "id": "fwLsMoYz4pCh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_predicted_labels.keys()"
      ],
      "metadata": {
        "id": "AoKd2zR1tXc3",
        "outputId": "22d7d35d-a32d-42a6-961b-1560cceaa957",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the predicted labels and confidence scores\n",
        "iteration = '0'\n",
        "predictions = pd.DataFrame.from_dict(all_predicted_labels[iteration],orient='index').transpose()\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "6zbom3Zmtf_4",
        "outputId": "e8b7be2e-96d8-4c1a-f5f8-dcd998379320",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            path confidence existing_label  \\\n",
            "0       [data/task2/images_by_class/3/1383.jpeg]    0.36458              3   \n",
            "1     [data/task2/images_by_class/76/29313.jpeg]   0.098717             76   \n",
            "2     [data/task2/images_by_class/30/12277.jpeg]   0.203538             30   \n",
            "3     [data/task2/images_by_class/59/22781.jpeg]    0.13695             59   \n",
            "4     [data/task2/images_by_class/88/34072.jpeg]   0.259119             88   \n",
            "...                                          ...        ...            ...   \n",
            "4995  [data/task2/images_by_class/56/21497.jpeg]   0.544975             56   \n",
            "4996  [data/task2/images_by_class/32/12788.jpeg]   0.115707             32   \n",
            "4997   [data/task2/images_by_class/22/8574.jpeg]   0.101662             22   \n",
            "4998  [data/task2/images_by_class/38/14964.jpeg]   0.125019             38   \n",
            "4999  [data/task2/images_by_class/82/48602.jpeg]   0.089067             82   \n",
            "\n",
            "     label_predicted  \n",
            "0                 19  \n",
            "1                 59  \n",
            "2                 34  \n",
            "3                 61  \n",
            "4                 34  \n",
            "...              ...  \n",
            "4995              34  \n",
            "4996              87  \n",
            "4997               7  \n",
            "4998              44  \n",
            "4999              19  \n",
            "\n",
            "[5000 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"How many labels have the same value after relabling ?\")\n",
        "print((predictions['existing_label'] == predictions['label_predicted']).value_counts())"
      ],
      "metadata": {
        "id": "TyeK6Ll-t09m",
        "outputId": "27533cb1-f1ac-4032-a0b1-7b5de250915b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How many labels have the same value after relabling ?\n",
            "False    4939\n",
            "True       61\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"What is the confidence of those ?\")\n",
        "print(predictions[predictions['existing_label'] == predictions['label_predicted']]['confidence'])\n",
        "predictions[predictions['existing_label'] == predictions['label_predicted']]['confidence'].plot.hist(bins = 20)"
      ],
      "metadata": {
        "id": "X-9AbJrHugPA",
        "outputId": "48821f09-ad15-48c2-b28d-d24b31d542aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the confidence of those ?\n",
            "191     1.041596\n",
            "220     0.176706\n",
            "226     0.309039\n",
            "272     1.208265\n",
            "378     1.241412\n",
            "          ...   \n",
            "4771    0.202634\n",
            "4789    0.619976\n",
            "4853    0.182252\n",
            "4931    0.596758\n",
            "4982    0.264925\n",
            "Name: confidence, Length: 61, dtype: object\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f04bec745e0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_same_label = predictions[predictions['existing_label'] == predictions['label_predicted']]\n",
        "predictions_correct_label = predictions_same_label[predictions_same_label['confidence']>0.5]\n",
        "display(predictions_correct_label)"
      ],
      "metadata": {
        "id": "GZqnYaMM58Nf",
        "outputId": "95130f2f-03c2-46c7-bbd7-cc65a22bd2f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                            path confidence existing_label  \\\n",
              "191    [data/task2/images_by_class/19/7468.jpeg]   1.041596             19   \n",
              "272   [data/task2/images_by_class/30/12107.jpeg]   1.208265             30   \n",
              "378   [data/task2/images_by_class/34/13499.jpeg]   1.241412             34   \n",
              "945   [data/task2/images_by_class/92/35180.jpeg]  11.464233             92   \n",
              "1176  [data/task2/images_by_class/63/23960.jpeg]   1.529731             63   \n",
              "1253   [data/task2/images_by_class/11/4422.jpeg]   1.074163             11   \n",
              "1356  [data/task2/images_by_class/34/13544.jpeg]   0.647533             34   \n",
              "1409  [data/task2/images_by_class/17/42317.jpeg]   1.304711             17   \n",
              "1506  [data/task2/images_by_class/38/15092.jpeg]   0.958325             38   \n",
              "1591   [data/task2/images_by_class/19/7451.jpeg]   0.862999             19   \n",
              "1908    [data/task2/images_by_class/7/3188.jpeg]   0.658398              7   \n",
              "2020  [data/task2/images_by_class/34/13546.jpeg]   0.983313             34   \n",
              "2417  [data/task2/images_by_class/34/13435.jpeg]   1.450222             34   \n",
              "2649   [data/task2/images_by_class/22/8810.jpeg]   0.520593             22   \n",
              "2836  [data/task2/images_by_class/34/13624.jpeg]   0.652207             34   \n",
              "3122  [data/task2/images_by_class/34/13490.jpeg]   1.297747             34   \n",
              "3125  [data/task2/images_by_class/21/44836.jpeg]   0.701892             21   \n",
              "3262  [data/task2/images_by_class/44/17250.jpeg]   0.576789             44   \n",
              "3371   [data/task2/images_by_class/19/7640.jpeg]   1.699485             19   \n",
              "3980   [data/task2/images_by_class/11/4320.jpeg]   1.343193             11   \n",
              "4299  [data/task2/images_by_class/34/13596.jpeg]   0.848232             34   \n",
              "4789  [data/task2/images_by_class/34/48202.jpeg]   0.619976             34   \n",
              "4931  [data/task2/images_by_class/34/13542.jpeg]   0.596758             34   \n",
              "\n",
              "     label_predicted  \n",
              "191               19  \n",
              "272               30  \n",
              "378               34  \n",
              "945               92  \n",
              "1176              63  \n",
              "1253              11  \n",
              "1356              34  \n",
              "1409              17  \n",
              "1506              38  \n",
              "1591              19  \n",
              "1908               7  \n",
              "2020              34  \n",
              "2417              34  \n",
              "2649              22  \n",
              "2836              34  \n",
              "3122              34  \n",
              "3125              21  \n",
              "3262              44  \n",
              "3371              19  \n",
              "3980              11  \n",
              "4299              34  \n",
              "4789              34  \n",
              "4931              34  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3975d65-8304-46e5-9042-58ab15e4b130\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>confidence</th>\n",
              "      <th>existing_label</th>\n",
              "      <th>label_predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>[data/task2/images_by_class/19/7468.jpeg]</td>\n",
              "      <td>1.041596</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>[data/task2/images_by_class/30/12107.jpeg]</td>\n",
              "      <td>1.208265</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>[data/task2/images_by_class/34/13499.jpeg]</td>\n",
              "      <td>1.241412</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>945</th>\n",
              "      <td>[data/task2/images_by_class/92/35180.jpeg]</td>\n",
              "      <td>11.464233</td>\n",
              "      <td>92</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1176</th>\n",
              "      <td>[data/task2/images_by_class/63/23960.jpeg]</td>\n",
              "      <td>1.529731</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1253</th>\n",
              "      <td>[data/task2/images_by_class/11/4422.jpeg]</td>\n",
              "      <td>1.074163</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1356</th>\n",
              "      <td>[data/task2/images_by_class/34/13544.jpeg]</td>\n",
              "      <td>0.647533</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1409</th>\n",
              "      <td>[data/task2/images_by_class/17/42317.jpeg]</td>\n",
              "      <td>1.304711</td>\n",
              "      <td>17</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1506</th>\n",
              "      <td>[data/task2/images_by_class/38/15092.jpeg]</td>\n",
              "      <td>0.958325</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1591</th>\n",
              "      <td>[data/task2/images_by_class/19/7451.jpeg]</td>\n",
              "      <td>0.862999</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1908</th>\n",
              "      <td>[data/task2/images_by_class/7/3188.jpeg]</td>\n",
              "      <td>0.658398</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020</th>\n",
              "      <td>[data/task2/images_by_class/34/13546.jpeg]</td>\n",
              "      <td>0.983313</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2417</th>\n",
              "      <td>[data/task2/images_by_class/34/13435.jpeg]</td>\n",
              "      <td>1.450222</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2649</th>\n",
              "      <td>[data/task2/images_by_class/22/8810.jpeg]</td>\n",
              "      <td>0.520593</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2836</th>\n",
              "      <td>[data/task2/images_by_class/34/13624.jpeg]</td>\n",
              "      <td>0.652207</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3122</th>\n",
              "      <td>[data/task2/images_by_class/34/13490.jpeg]</td>\n",
              "      <td>1.297747</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3125</th>\n",
              "      <td>[data/task2/images_by_class/21/44836.jpeg]</td>\n",
              "      <td>0.701892</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3262</th>\n",
              "      <td>[data/task2/images_by_class/44/17250.jpeg]</td>\n",
              "      <td>0.576789</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3371</th>\n",
              "      <td>[data/task2/images_by_class/19/7640.jpeg]</td>\n",
              "      <td>1.699485</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3980</th>\n",
              "      <td>[data/task2/images_by_class/11/4320.jpeg]</td>\n",
              "      <td>1.343193</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4299</th>\n",
              "      <td>[data/task2/images_by_class/34/13596.jpeg]</td>\n",
              "      <td>0.848232</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4789</th>\n",
              "      <td>[data/task2/images_by_class/34/48202.jpeg]</td>\n",
              "      <td>0.619976</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4931</th>\n",
              "      <td>[data/task2/images_by_class/34/13542.jpeg]</td>\n",
              "      <td>0.596758</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3975d65-8304-46e5-9042-58ab15e4b130')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c3975d65-8304-46e5-9042-58ab15e4b130 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c3975d65-8304-46e5-9042-58ab15e4b130');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[predictions['existing_label'] != predictions['label_predicted']]['confidence'].plot.hist(bins = 20)"
      ],
      "metadata": {
        "id": "BFIOtoxFvLjz",
        "outputId": "31b80c1c-bce4-4ad0-f457-dc74e7c95b62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f04beb3ef10>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the predictions have very low score, even if they are right or wrong. \n",
        "But a very small number of images have a high confidence of being correct or incorrect. \n",
        "We would take those images with a very high confidence of being right from each iteration and put them aside as true. Than we will repeat the process of forgetting some images and trying to train on the remaining images. But now we have a little bit better model, since it has some images for which we have a high confidence that they are correct. "
      ],
      "metadata": {
        "id": "lZy2StjD5PgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repeat thresholding pseudo-labels for the following iterations. \n"
      ],
      "metadata": {
        "id": "9R2wBaePAfZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the predicted labels and confidence scores\n",
        "correct_labels_df = []\n",
        "for i in range(10):\n",
        "    predictions=pd.DataFrame.from_dict(all_predicted_labels[str(i)],orient='index').transpose() \n",
        "    predictions_same_label = predictions[predictions['existing_label'] == predictions['label_predicted']]\n",
        "    predictions_correct_label = predictions_same_label[predictions_same_label['confidence']>0.5] \n",
        "    correct_labels_df.append(predictions_correct_label)"
      ],
      "metadata": {
        "id": "O5ghSTjFAkIW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_correct = pd.concat(correct_labels_df)"
      ],
      "metadata": {
        "id": "UVQdrSySCx3z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_correct)"
      ],
      "metadata": {
        "id": "GlOHKGEyCOle",
        "outputId": "8b4f877f-a817-4873-fe34-1c685ac15ff2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "431"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: After the first iteration of the algorithm we found 431 images to have correct labels with high confidence. This means we should repeat this process 100 times. Which would take a very large amount of time. But hopefully the futher iterations will become faster as we find that more images are correctly labeled. \n",
        "\n",
        "NOTE: Also, because we still have the previous results from the first iteration. If we get the same labels on the second or third iteration, we are much more likely to believe that the respective labels are true, and we can include them in the \"pseudo ground truth dataset\" even if they don't have a high confidence, but they have the same label on multiple runs. \n",
        "\n",
        "NOTE: Finally. In the end we can try again the whole experiment from the beginning. The expectation would be that we would find much fewer wrongly labeled images. "
      ],
      "metadata": {
        "id": "Wvwdmh-BC2_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iteration 2\n",
        "considering previously found labels as true, and not reconsidering to label them anymore. "
      ],
      "metadata": {
        "id": "q_emV0k5Dv9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_correct"
      ],
      "metadata": {
        "id": "xAD_T9-zG515",
        "outputId": "0e94b16b-4171-4267-a71e-3df4f6ad725e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            path confidence existing_label  \\\n",
              "191    [data/task2/images_by_class/19/7468.jpeg]   1.041596             19   \n",
              "272   [data/task2/images_by_class/30/12107.jpeg]   1.208265             30   \n",
              "378   [data/task2/images_by_class/34/13499.jpeg]   1.241412             34   \n",
              "945   [data/task2/images_by_class/92/35180.jpeg]  11.464233             92   \n",
              "1176  [data/task2/images_by_class/63/23960.jpeg]   1.529731             63   \n",
              "...                                          ...        ...            ...   \n",
              "4310  [data/task2/images_by_class/50/19476.jpeg]   1.982109             50   \n",
              "4365  [data/task2/images_by_class/52/19993.jpeg]   2.803925             52   \n",
              "4545  [data/task2/images_by_class/38/48931.jpeg]    3.05493             38   \n",
              "4587  [data/task2/images_by_class/38/15138.jpeg]   1.706635             38   \n",
              "4938  [data/task2/images_by_class/93/35654.jpeg]   2.009159             93   \n",
              "\n",
              "     label_predicted  \n",
              "191               19  \n",
              "272               30  \n",
              "378               34  \n",
              "945               92  \n",
              "1176              63  \n",
              "...              ...  \n",
              "4310              50  \n",
              "4365              52  \n",
              "4545              38  \n",
              "4587              38  \n",
              "4938              93  \n",
              "\n",
              "[431 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-493c34e1-f441-4460-bb34-b4f890700df4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>confidence</th>\n",
              "      <th>existing_label</th>\n",
              "      <th>label_predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>[data/task2/images_by_class/19/7468.jpeg]</td>\n",
              "      <td>1.041596</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>[data/task2/images_by_class/30/12107.jpeg]</td>\n",
              "      <td>1.208265</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>[data/task2/images_by_class/34/13499.jpeg]</td>\n",
              "      <td>1.241412</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>945</th>\n",
              "      <td>[data/task2/images_by_class/92/35180.jpeg]</td>\n",
              "      <td>11.464233</td>\n",
              "      <td>92</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1176</th>\n",
              "      <td>[data/task2/images_by_class/63/23960.jpeg]</td>\n",
              "      <td>1.529731</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4310</th>\n",
              "      <td>[data/task2/images_by_class/50/19476.jpeg]</td>\n",
              "      <td>1.982109</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4365</th>\n",
              "      <td>[data/task2/images_by_class/52/19993.jpeg]</td>\n",
              "      <td>2.803925</td>\n",
              "      <td>52</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4545</th>\n",
              "      <td>[data/task2/images_by_class/38/48931.jpeg]</td>\n",
              "      <td>3.05493</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4587</th>\n",
              "      <td>[data/task2/images_by_class/38/15138.jpeg]</td>\n",
              "      <td>1.706635</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4938</th>\n",
              "      <td>[data/task2/images_by_class/93/35654.jpeg]</td>\n",
              "      <td>2.009159</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>431 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-493c34e1-f441-4460-bb34-b4f890700df4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-493c34e1-f441-4460-bb34-b4f890700df4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-493c34e1-f441-4460-bb34-b4f890700df4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_dir = 'data/task2/images_by_class'"
      ],
      "metadata": {
        "id": "j9QqSxUiLlPu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(target_dir):\n",
        "    # Use rmtree to delete the directory and all its contents\n",
        "    shutil.rmtree(target_dir)\n",
        "    print(f'{target_dir} has been deleted')\n",
        "else:\n",
        "    print(f'{target_dir} does not exist')"
      ],
      "metadata": {
        "id": "nYWytvbjLeJi",
        "outputId": "f149e39e-34c8-4ed5-b1b6-e9df5164da2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/task2/images_by_class does not exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the directory exists, to recreate it instead of messing it up\n",
        "target_dir = 'data/task2/training_confident'\n",
        "\n",
        "if os.path.exists(target_dir):\n",
        "    # Use rmtree to delete the directory and all its contents\n",
        "    shutil.rmtree(target_dir)\n",
        "    print(f'{target_dir} has been deleted') \n",
        "else:\n",
        "    print(f'{target_dir} does not exist')"
      ],
      "metadata": {
        "id": "xNK5li4VMQae",
        "outputId": "012da0e6-ae00-4ac7-f90e-6934f08063b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/task2/training_confident does not exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_dir = 'data/task2/images_by_class'\n",
        "dir_data = 'data/task2/train_data/'\n",
        "# Read the annotations file into a DataFrame\n",
        "df = pd.read_csv(f'{dir_data}annotations.csv')\n",
        "\n",
        "# Iterate over the rows in the DataFrame\n",
        "for _, row in tqdm(df.iterrows(), total= df.shape[0]):\n",
        "    # Extract the path and class from the row\n",
        "    path = row['renamed_path']\n",
        "    label = row['label_idx']\n",
        "    \n",
        "    # Create the directory for the class\n",
        "    class_dir = f'{target_dir}/{label}'\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    \n",
        "    # Copy the file to the class directory\n",
        "    shutil.copy(f\"data/{path}\", class_dir) \n",
        "\n"
      ],
      "metadata": {
        "id": "Ri4BEas4HE8u",
        "outputId": "69d48e21-a96b-44af-d4fc-5b4c7e704021",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:12<00:00, 3852.15it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on 2 datasets at the same time, one with high confidence labels\n",
        "# this dataset will only be used during training, never forgetting labels\n",
        "# the second dataset will iteratively forget labels and train on the remaining labels.\n",
        "# Define the base directory\n",
        "target_dir = 'data/task2/training_confident'\n",
        "# Iterate over the rows in the DataFrame\n",
        "for _, row in tqdm(df_correct.iterrows(), total=df_correct.shape[0]):\n",
        "    # Extract the path and class from the row\n",
        "    path = row['path'][0]\n",
        "    label = row['existing_label'] \n",
        "\n",
        "    # Create the directory for the class\n",
        "    class_dir = f'{target_dir}/{label}'\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    \n",
        "    # move the file to the class directory\n",
        "    shutil.move(f\"{path}\", class_dir)\n",
        "    \n",
        "\n",
        "# we move the images because we don't want them to remain in the dataset for relabeling.  \n"
      ],
      "metadata": {
        "id": "ZKxIxGUz5a8X",
        "outputId": "f12b2432-b3aa-4c7a-96f4-fbf055399dbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 431/431 [00:00<00:00, 7569.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: create the 2 datasets\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(experiment_info[\"image_processing\"][\"resize\"]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=experiment_info[\"image_processing\"][\"mean\"],\n",
        "                         std=experiment_info[\"image_processing\"][\"std\"]),\n",
        "    ])\n",
        "\n",
        "data_dir_train = 'data/task2/training_confident'\n",
        "image_dataset_train_only = datasets.ImageFolder(data_dir_train, preprocess) \n",
        "dataloader_train_only  = DataLoader(\n",
        "    image_dataset_train_only, \n",
        "    batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"],\n",
        "    shuffle = experiment_info[\"hyperparameters_data\"][\"shuffle_dataloader\"], \n",
        "    num_workers = experiment_info[\"hyperparameters_data\"][\"num_workers\"]\n",
        "    )\n",
        "\n",
        "class_names = image_dataset_train_only.classes\n",
        "dataset_size = len(image_dataset_train_only)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "K7KktFggEt_p",
        "outputId": "bd30b0fe-b32f-4c46-b37c-f848881e9ce2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_size)"
      ],
      "metadata": {
        "id": "6i8BvAlyH110",
        "outputId": "df09c0ec-5d9e-4621-b525-1073b3e4d16a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create datasets splits for pseudo-labeling round 2"
      ],
      "metadata": {
        "id": "iG8vZn7mFmRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'data/task2/images_by_class'\n",
        "image_dataset_unshuffled = datasets.ImageFolder(data_dir, preprocess)\n",
        "image_dataset = ShuffledImageFolder(image_dataset_unshuffled)"
      ],
      "metadata": {
        "id": "XfnzLXF_FqMa"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = len(image_dataset)\n",
        "\n",
        "# Split the image_dataset into 10 parts\n",
        "splits = 2\n",
        "part_size = num_samples // splits\n",
        "parts = [list(range(i * part_size, (i + 1) * part_size)) for i in range(splits)]\n",
        "\n",
        "batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"]\n",
        "# Use one part for creating new labels and the other 9 parts for training the model\n",
        "loaders2 = {}\n",
        "for i in range(splits):\n",
        "    # Get the indices for the training set, which are not in the labeling set\n",
        "    train_indices = [index for j, part in enumerate(parts) if j != i for index in part]\n",
        "    labeling_indices = parts[i]\n",
        "\n",
        "    # Create a sampler for the training set and the labeling set\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    labeling_sampler = SubsetRandomSampler(labeling_indices)\n",
        "\n",
        "    # Use the samplers to create data loaders2 for the training set and the labeling set\n",
        "    train_loader = DataLoader(image_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    labeling_loader = DataLoader(image_dataset, batch_size=batch_size, sampler=labeling_sampler)\n",
        "    loaders2[i] = {\n",
        "        \"train\":train_loader, \n",
        "        \"labeling\":labeling_loader\n",
        "    }\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data_sizes = {\n",
        "    \"train\" : len(loaders2[0][\"train\"]), # this is the number of batches not images\n",
        "    \"labeling\" : len(loaders2[0][\"labeling\"]),\n",
        "}"
      ],
      "metadata": {
        "id": "gEMNSHMBFtLa"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_sizes)\n",
        "print(\"num samples train\", data_sizes['train']*batch_size)"
      ],
      "metadata": {
        "id": "a95SXdQCH8GL",
        "outputId": "eb79f48c-f2ea-4100-e828-1b8059bf5820",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': 775, 'labeling': 775}\n",
            "num samples train 24800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on remaining data"
      ],
      "metadata": {
        "id": "vohgejEvNNLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a pytorch MobileNet pretrained model\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "# change the Linear output to fit our dataset ( because model initially has 1000 classes)\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "# Setting hyperparameters\n",
        "\n",
        "model = model.to(device)\n",
        "print(model.classifier)\n",
        "     "
      ],
      "metadata": {
        "id": "bk_xDk2MNPk7",
        "outputId": "04ce156f-2d44-4405-bc1e-b2d78fea20ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "0b6b0668f3764a7296945547803e2688",
            "c39c58da42ce4c2ca76546673443f0fd",
            "b02ccf11c1714786a67bfca3446e9b9d",
            "284de89238b54efd8d834cfc5cdef224",
            "4d1e4cd757a749bda0f6fbcede536076",
            "dd637c7b9a5645dfb198f017092d098a",
            "afee6006e2324bac8bbc9c291e638617",
            "8e3263acdec647e594b2140e749bd110",
            "4dd784f55bb14f6ba9274ec956ec8061",
            "7adf26ae5d2e430e946d5576135d893a",
            "69db9e55793a4d87bd7b0c96efc53170"
          ]
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b6b0668f3764a7296945547803e2688"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=False)\n",
            "  (1): Linear(in_features=1280, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=experiment_info[\"hyperparameters_training\"][\"learning_rate\"],\n",
        "    )\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(\n",
        "    optimizer, \n",
        "    step_size=experiment_info[\"hyperparameters_training\"][\"scheduler_step_size\"], \n",
        "    gamma=experiment_info[\"hyperparameters_training\"][\"scheduler_gamma\"],\n",
        "    )"
      ],
      "metadata": {
        "id": "LRzyP-FuNhhh"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model2(model, \n",
        "                    exp_lr_scheduler, \n",
        "                    optimizer, \n",
        "                    criterion,  \n",
        "                    data_sizes[\"train\"], \n",
        "                    dataloader_train_only,\n",
        "                    loaders2[0][\"train\"], \n",
        "                    num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])"
      ],
      "metadata": {
        "id": "VmT0N9O9NUIX",
        "outputId": "1fa0bc4c-7baf-4cee-c213-846f55d6dcf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:09<00:00,  1.42it/s]\n",
            "100%|██████████| 775/775 [02:21<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 86.6548 Acc: 11.1110\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.00it/s]\n",
            "100%|██████████| 775/775 [02:20<00:00,  5.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 51.9183 Acc: 17.8194\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  4.72it/s]\n",
            "100%|██████████| 775/775 [02:23<00:00,  5.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 39.8597 Acc: 20.5716\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.05it/s]\n",
            "100%|██████████| 775/775 [02:18<00:00,  5.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 31.2823 Acc: 22.5729\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.90it/s]\n",
            "100%|██████████| 775/775 [02:15<00:00,  5.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 24.2291 Acc: 24.5897\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.96it/s]\n",
            "100%|██████████| 775/775 [02:16<00:00,  5.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 18.5088 Acc: 26.2800\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.97it/s]\n",
            "100%|██████████| 775/775 [02:15<00:00,  5.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 13.6184 Acc: 27.8787\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.91it/s]\n",
            "100%|██████████| 775/775 [02:15<00:00,  5.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 7.1740 Acc: 30.4065\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.04it/s]\n",
            "100%|██████████| 775/775 [02:17<00:00,  5.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 5.7742 Acc: 30.9032\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.01it/s]\n",
            "100%|██████████| 775/775 [02:15<00:00,  5.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 5.1758 Acc: 31.1458\n",
            "\n",
            "Training complete in 23m 33s\n",
            "Best val Acc: 31.145806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), '/gdrive/MyDrive/checkpoints/noisy_labels/model_2_it_0.pt')"
      ],
      "metadata": {
        "id": "1XhOuuNyNvtU"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Repeat for the other half"
      ],
      "metadata": {
        "id": "3S_tI-1ZNstT"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "model = model.to(device) \n",
        "model = train_model2(model, \n",
        "                exp_lr_scheduler, \n",
        "                optimizer, \n",
        "                criterion,  \n",
        "                data_sizes[\"train\"], \n",
        "                dataloader_train_only,\n",
        "                loaders2[1][\"train\"], \n",
        "                num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])\n",
        "torch.save(model.state_dict(), f'/gdrive/MyDrive/checkpoints/noisy_labels/model_2_it_{1}.pt')"
      ],
      "metadata": {
        "id": "-wMaIPVTN-KN",
        "outputId": "d388f3b4-f6e4-4f21-e092-7f0f79e9af11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.00it/s]\n",
            "100%|██████████| 775/775 [02:15<00:00,  5.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 151.0671 Acc: 0.2813\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.14it/s]\n",
            "100%|██████████| 775/775 [02:16<00:00,  5.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 151.0593 Acc: 0.2606\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.15it/s]\n",
            "100%|██████████| 775/775 [02:17<00:00,  5.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 151.0380 Acc: 0.2606\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.04it/s]\n",
            "100%|██████████| 775/775 [02:26<00:00,  5.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.9949 Acc: 0.2839\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  4.90it/s]\n",
            " 54%|█████▎    | 415/775 [01:14<01:02,  5.73it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save labels for later predicting"
      ],
      "metadata": {
        "id": "4r4x6giiTYqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paths_labeling = {}\n",
        "for i in tqdm(range(2)):\n",
        "    paths_labeling[i] = []\n",
        "    for images, labels, paths in loaders[i]['labeling']:\n",
        "        paths_labeling[i].append(paths)"
      ],
      "metadata": {
        "id": "zwmcZrQyWISg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the experiment information to a JSON file\n",
        "with open(f'/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_images_for_models_2.json', 'w') as f:\n",
        "    json.dump(paths_labeling, f)"
      ],
      "metadata": {
        "id": "mi2U16qVWKJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict new labels 2"
      ],
      "metadata": {
        "id": "_9TaBnOkWX-s"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cM3GCPEaWZ0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the threshold for the confidence score\n",
        "confidence_threshold = 0.7\n",
        "\n",
        "confidence_above_threshold = 0\n",
        "c = 0\n",
        "# Iterate through the predictions\n",
        "for p in predictions:\n",
        "    print(p)\n",
        "    break\n",
        "    if confidence >= confidence_threshold:\n",
        "        # Use the predicted label if the confidence is above the threshold\n",
        "        confidence_above_threshold +=1\n",
        "        # chosen_label = label\n",
        "    else:\n",
        "        # Otherwise, run the label through another session of pseudo-labeling\n",
        "        confidence_above_threshold += 1\n",
        "        # new_label = run_pseudo_labeling(label)\n",
        "        if new_label == label:\n",
        "            # If the new label is the same as the original label, use it\n",
        "            chosen_label = label\n",
        "        else:\n",
        "            # Otherwise, use the new label if the confidence is above the threshold\n",
        "            if confidence >= confidence_threshold:\n",
        "                chosen_label = new_label\n",
        "            else:\n",
        "                # Otherwise, run the label through another session of pseudo-labeling\n",
        "                chosen_label = run_pseudo_labeling(new_label)\n",
        "    \n",
        "    # Use the chosen label for the current prediction\n",
        "    # ..."
      ],
      "metadata": {
        "id": "LOuJsU-z3Ayl",
        "outputId": "86798884-2ba3-4e59-db71-9511bb063f29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "path\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: at the end of the training phase\n",
        "# use the predictions made by the models \n",
        "# to choose the images with the highest confidence\n",
        "# by selecting the images that consistently have the same label\n",
        "# this set of images will become an always used \n",
        "# set in the next training phase, it should not be forgetten again\n",
        "# because there is a high confidence that these labels have the \n",
        "# correct labels \n",
        "# because I have leave 9 out technique,\n",
        "# I will have 9 labels for each image\n",
        "# and also the original label.\n",
        "# the probability that those labels are correct can be \n",
        "# simply counting the label that occurs most often\n",
        "# than dividing that number by 10"
      ],
      "metadata": {
        "id": "1DsisAptHYxA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
      }
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb68d62d599a462f92aaca0ab0b854e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6eed1376f1f24e8abbe2d4ea0ca3fcb1",
              "IPY_MODEL_07c37bb6b22b4c8abe8d16b61e28e1db",
              "IPY_MODEL_15a23332521441cab29f7acedc079b16"
            ],
            "layout": "IPY_MODEL_816420dd910b475faa99a8e9ef6eebd4"
          }
        },
        "6eed1376f1f24e8abbe2d4ea0ca3fcb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71a16d1906924e738fc1baa304d94811",
            "placeholder": "​",
            "style": "IPY_MODEL_a74028e913de43d3b1eae97433d91882",
            "value": "100%"
          }
        },
        "07c37bb6b22b4c8abe8d16b61e28e1db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07868acd1960497680b904b81fd0eaf9",
            "max": 14212972,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a56ed7ebe21c4011a23a63cad606d40c",
            "value": 14212972
          }
        },
        "15a23332521441cab29f7acedc079b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba437891973d4629b4881ccb11f044ae",
            "placeholder": "​",
            "style": "IPY_MODEL_a75d7f60d1c24475a1540bc1cfe94f7b",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 62.9MB/s]"
          }
        },
        "816420dd910b475faa99a8e9ef6eebd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71a16d1906924e738fc1baa304d94811": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a74028e913de43d3b1eae97433d91882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07868acd1960497680b904b81fd0eaf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a56ed7ebe21c4011a23a63cad606d40c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba437891973d4629b4881ccb11f044ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a75d7f60d1c24475a1540bc1cfe94f7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b6b0668f3764a7296945547803e2688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c39c58da42ce4c2ca76546673443f0fd",
              "IPY_MODEL_b02ccf11c1714786a67bfca3446e9b9d",
              "IPY_MODEL_284de89238b54efd8d834cfc5cdef224"
            ],
            "layout": "IPY_MODEL_4d1e4cd757a749bda0f6fbcede536076"
          }
        },
        "c39c58da42ce4c2ca76546673443f0fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd637c7b9a5645dfb198f017092d098a",
            "placeholder": "​",
            "style": "IPY_MODEL_afee6006e2324bac8bbc9c291e638617",
            "value": "100%"
          }
        },
        "b02ccf11c1714786a67bfca3446e9b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e3263acdec647e594b2140e749bd110",
            "max": 14212972,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4dd784f55bb14f6ba9274ec956ec8061",
            "value": 14212972
          }
        },
        "284de89238b54efd8d834cfc5cdef224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7adf26ae5d2e430e946d5576135d893a",
            "placeholder": "​",
            "style": "IPY_MODEL_69db9e55793a4d87bd7b0c96efc53170",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 46.0MB/s]"
          }
        },
        "4d1e4cd757a749bda0f6fbcede536076": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd637c7b9a5645dfb198f017092d098a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afee6006e2324bac8bbc9c291e638617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e3263acdec647e594b2140e749bd110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dd784f55bb14f6ba9274ec956ec8061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7adf26ae5d2e430e946d5576135d893a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69db9e55793a4d87bd7b0c96efc53170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}