{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WaRT8CyQPpcv"
   },
   "source": [
    "# Noisy labels problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to implement anything we should think about the problem first. We don't know which of the training samples are wrong, and we cannot try to correct them by hand because there are too many of them and the images are not very clear anyway.\n",
    "\n",
    "We need some Noise-aware training techniques. Some ideas are:\n",
    "- training using a loss function that is designed to be robust to label noise. Such as:\n",
    "  - focal loss, it down-weighs the contribution of easy examples and puts more ephasis on difficult examples, which can make it resistant to noise. `torch.nn.FocalLoss`\n",
    "  - Generalized Cross-Entropy loss, which includes additional hyperparameters that allow the model to learn the noise rate and label corruption matrix. `torch.nn.GCE`\n",
    "- bootstrapping\n",
    "- self-ensembling to learn multiple models that are more robust to noise\n",
    "- Confidence based method, because predicting the confidence may allow humans to discard the least confidence examples. ( but we are not creating a real world model, we are getting tested automatically with a test set, so this method cannot work.)\n",
    "- Pseudo-labeling, using the model's own predictions to re-label a portion of the training data. Similar to self-training, we eliminate completely labels of some samples and we try to predict them. In case the labels are the same after relabling, there is a higher chance they are correct. We would gradually relable the images into a better dataset. \n",
    "    - Or Computing a probability that the labels are misslabeled, than using that probability to weigh heavier on the labels that have a higher chance of being correct during training. `torch.nn.CrossEntropyLoss`\n",
    "- Noise tolerant algorithms: complex deep learning models are more tolerant to noise. (this is not a very solid technique but should be used together with others.)\n",
    "\n",
    "References:\n",
    "- Focal loss: This loss function was first introduced in the paper \"Focal Loss for Dense Object Detection\" by Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll√°r (https://arxiv.org/abs/1708.02002).\n",
    "- Generalized cross-entropy loss: This loss function was proposed in the paper \"Learning with Noisy Labels\" by Mengye Ren, Elad Hazan, and Yoram Singer (https://arxiv.org/abs/1609.03683).\n",
    "- Bootstrapping: This technique involves training multiple models on different subsets of the training data, and then combining their predictions to make a final prediction. It was first introduced in the paper \"Bagging Predictors\" by Leo Breiman (https://link.springer.com/article/10.1023/A:1018054314350).\n",
    "- Self-ensembling: This technique involves training multiple models on the same data, and then using their predictions to create a final prediction. It was first introduced in the paper \"Self-Ensembling for Visual Domain Adaptation\" by Sergey Zagoruyko and Nikos Komodakis (https://arxiv.org/abs/1706.05208).\n",
    "- Pseudo-labeling: This technique involves using the model's own predictions to label a portion of the training data. It was first introduced in the paper \"Semi-Supervised Learning with Deep Generative Models\" by Diederik Kingma, Danilo Jimenez Rezende, Shakir Mohamed, and Max Welling (https://arxiv.org/abs/1406.5298).\n",
    "- Noise-aware training: This refers to techniques that are specifically designed to handle label noise in the training data. One example of such a technique is the generalized cross-entropy loss function, which was introduced in the paper \"Learning with Noisy Labels\" by Mengye Ren, Elad Hazan, and Yoram Singer (https://arxiv.org/abs/1609.03683).\n",
    "- Confidence based methods: This refers to techniques that involve training a model to predict the confidence or probability of each class label, rather than just the class label itself. This can allow the model to identify examples that are less confident and potentially more prone to noise. One example of a confidence-based method is the method of \"bootstrapping,\" which was introduced in the paper \"Bagging Predictors\" by Leo Breiman (https://link.springer.com/article/10.1023/A:1018054314350)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Labeling solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This technique is barely described above\n",
    "\n",
    "The first question is what portion of the dataset should I \"forget\" the labels and try predict new ones(by training on the remaining ones)? \n",
    "\n",
    "Making the worse assumption that half of the labels are wrong. Than we should leave out a smaller portion of the labels to be re-labeled.\n",
    "But because we have a rather large dataset of 500 images per class and that the model we are using is a rather complex one which is already pretrained, we may increase the number of labels for re-labeling. \n",
    "\n",
    "According to previous argument I chose to separate the dataset in 10 parts. \n",
    "I will leave 10% of the dataset out and only use the rest of 90%. \n",
    "Similar to a 10-fold cross validation, but instead of validating we are re-labeling the remaining 10% of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import SubsetRandomSampler, Dataset\n",
    "from torchvision import transforms, datasets \n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import itertools\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task2_id = \"1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace here your ide &id=1dO1vqCoJm2xwrnr171A6_eW7ikd-alrd\"\n",
    "# replace here your id 'https://docs.google.com/uc?export=download&id=1dO1vqCoJm2xwrnr171A6_eW7ikd-alrd'\n",
    "# replace here your target name -O task1.tar.gz &&\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\" -O task2.tar.gz && rm -rf /tmp/cookies.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!mkdir data\n",
    "!mv task1.tar.gz ./data\n",
    "!tar -xzvf \"/content/data/task2.tar.gz\" -C \"/content/data/\"     #[run this cell to extract tar.gz files]\n",
    "# this may take 12 seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount drive \n",
    "( in order to save each iteration of leave out labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/gdrive/MyDrive/foo.txt', 'w') as f:\n",
    "  f.write('Hello Google Drive!')\n",
    "!cat '/gdrive/MyDrive/foo.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm '/gdrive/MyDrive/foo.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seting up things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = 'data/task2/train_data/'\n",
    "# Read the annotations file into a DataFrame\n",
    "df = pd.read_csv(f'{dir_data}annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data for pytorch training ( only once )\n",
    "# Define the base directory\n",
    "base_dir = 'data/task1/labeled'\n",
    "\n",
    "# Iterate over the rows in the DataFrame\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    # Extract the path and class from the row\n",
    "    path = row['sample']\n",
    "    label = row['label']\n",
    "    \n",
    "    # Create the directory for the class\n",
    "    class_dir = f'{base_dir}/{label}'\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy the file to the class directory\n",
    "    shutil.copy(f\"data/{path}\", class_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up the place where you save each iteration labels\n",
    "# preferably a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Random sample cross re-labling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move out of the dataset those images that I considered \n",
    "# to be forgotten\n",
    "# they will be relabled at the end of training and put back in the data\n",
    "# but they their annotations will be saved in a separate dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, during the training, for each iteration, \n",
    "# add a new set of labels to the csv that tells us the annotations\n",
    "# at the end of the training phase\n",
    "# use the predictions made by the models \n",
    "# to choose the images with the highest confidence\n",
    "# by selecting the images that consistently have the same label\n",
    "# this set of images will become an always used \n",
    "# set in the next training phase, it should not be forgetten again\n",
    "# because there is a high confidence that these labels have the \n",
    "# correct labels \n",
    "# because I have leave 9 out technique,\n",
    "# I will have 9 labels for each image\n",
    "# and also the original label.\n",
    "# the probability that those labels are correct can be \n",
    "# simply counting the label that occurs most often\n",
    "# than dividing that number by 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
