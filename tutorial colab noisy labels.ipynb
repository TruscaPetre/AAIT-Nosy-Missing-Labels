{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TruscaPetre/AAIT-Nosy-Missing-Labels/blob/main/tutorial%20colab%20noisy%20labels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaRT8CyQPpcv"
      },
      "source": [
        "# Noisy labels problem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOThg22n_IX7"
      },
      "source": [
        "## Introduction theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJxksI3i_IX-"
      },
      "source": [
        "Before starting to implement anything we should think about the problem first. We don't know which of the training samples are wrong, and we cannot try to correct them by hand because there are too many of them and the images are not very clear anyway.\n",
        "\n",
        "We need some Noise-aware training techniques. Some ideas are:\n",
        "- training using a loss function that is designed to be robust to label noise. Such as:\n",
        "  - focal loss, it down-weighs the contribution of easy examples and puts more ephasis on difficult examples, which can make it resistant to noise. `torch.nn.FocalLoss`\n",
        "  - Generalized Cross-Entropy loss, which includes additional hyperparameters that allow the model to learn the noise rate and label corruption matrix. `torch.nn.GCE`\n",
        "- bootstrapping\n",
        "- self-ensembling to learn multiple models that are more robust to noise\n",
        "- Confidence based method, because predicting the confidence may allow humans to discard the least confidence examples. ( but we are not creating a real world model, we are getting tested automatically with a test set, so this method cannot work.)\n",
        "- Pseudo-labeling, using the model's own predictions to re-label a portion of the training data. Similar to self-training, we eliminate completely labels of some samples and we try to predict them. In case the labels are the same after relabling, there is a higher chance they are correct. We would gradually relable the images into a better dataset. \n",
        "    - Or Computing a probability that the labels are misslabeled, than using that probability to weigh heavier on the labels that have a higher chance of being correct during training. `torch.nn.CrossEntropyLoss`\n",
        "- Noise tolerant algorithms: complex deep learning models are more tolerant to noise. (this is not a very solid technique but should be used together with others.)\n",
        "\n",
        "References:\n",
        "- Focal loss: This loss function was first introduced in the paper \"Focal Loss for Dense Object Detection\" by Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár (https://arxiv.org/abs/1708.02002).\n",
        "- Generalized cross-entropy loss: This loss function was proposed in the paper \"Learning with Noisy Labels\" by Mengye Ren, Elad Hazan, and Yoram Singer (https://arxiv.org/abs/1609.03683).\n",
        "- Bootstrapping: This technique involves training multiple models on different subsets of the training data, and then combining their predictions to make a final prediction. It was first introduced in the paper \"Bagging Predictors\" by Leo Breiman (https://link.springer.com/article/10.1023/A:1018054314350).\n",
        "- Self-ensembling: This technique involves training multiple models on the same data, and then using their predictions to create a final prediction. It was first introduced in the paper \"Self-Ensembling for Visual Domain Adaptation\" by Sergey Zagoruyko and Nikos Komodakis (https://arxiv.org/abs/1706.05208).\n",
        "- Pseudo-labeling: This technique involves using the model's own predictions to label a portion of the training data. It was first introduced in the paper \"Semi-Supervised Learning with Deep Generative Models\" by Diederik Kingma, Danilo Jimenez Rezende, Shakir Mohamed, and Max Welling (https://arxiv.org/abs/1406.5298).\n",
        "- Noise-aware training: This refers to techniques that are specifically designed to handle label noise in the training data. One example of such a technique is the generalized cross-entropy loss function, which was introduced in the paper \"Learning with Noisy Labels\" by Mengye Ren, Elad Hazan, and Yoram Singer (https://arxiv.org/abs/1609.03683).\n",
        "- Confidence based methods: This refers to techniques that involve training a model to predict the confidence or probability of each class label, rather than just the class label itself. This can allow the model to identify examples that are less confident and potentially more prone to noise. One example of a confidence-based method is the method of \"bootstrapping,\" which was introduced in the paper \"Bagging Predictors\" by Leo Breiman (https://link.springer.com/article/10.1023/A:1018054314350)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeBHHsZt_IYU"
      },
      "source": [
        "## Pseudo Labeling solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnkncVA__IYW"
      },
      "source": [
        "\n",
        "This technique is barely described above\n",
        "\n",
        "The first question is what portion of the dataset should I \"forget\" the labels and try predict new ones(by training on the remaining ones)? \n",
        "\n",
        "Making the worse assumption that half of the labels are wrong. Than we should leave out a smaller portion of the labels to be re-labeled.\n",
        "But because we have a rather large dataset of 500 images per class and that the model we are using is a rather complex one which is already pretrained, we may increase the number of labels for re-labeling. \n",
        "\n",
        "According to previous argument I chose to separate the dataset in 10 parts. \n",
        "I will leave 10% of the dataset out and only use the rest of 90%. \n",
        "Similar to a 10-fold cross validation, but instead of validating we are re-labeling the remaining 10% of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmzRZhui_IYY"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x_I1fk99_IYZ"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "import shutil\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import SubsetRandomSampler, Dataset, DataLoader\n",
        "from torchvision import transforms, datasets \n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import itertools\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yyOZc59J_IYc"
      },
      "outputs": [],
      "source": [
        "task2_id = \"1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCcacF4f_IYe",
        "outputId": "c19a4eb9-5373-489c-b1d2-b2b932ea3a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-19 18:37:33--  https://docs.google.com/uc?export=download&confirm=t&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.10.138, 142.251.10.101, 142.251.10.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.10.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-8k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/l6i8f33bttsueieqhgf7ckour82p22me/1674153450000/08997952672865575084/*/1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL?e=download&uuid=a5475b9c-b253-45e2-bc63-9e72e3af5048 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-01-19 18:37:33--  https://doc-04-8k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/l6i8f33bttsueieqhgf7ckour82p22me/1674153450000/08997952672865575084/*/1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL?e=download&uuid=a5475b9c-b253-45e2-bc63-9e72e3af5048\n",
            "Resolving doc-04-8k-docs.googleusercontent.com (doc-04-8k-docs.googleusercontent.com)... 142.251.12.132, 2404:6800:4003:c11::84\n",
            "Connecting to doc-04-8k-docs.googleusercontent.com (doc-04-8k-docs.googleusercontent.com)|142.251.12.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75324736 (72M) [application/x-gzip]\n",
            "Saving to: ‘task2.tar.gz’\n",
            "\n",
            "task2.tar.gz        100%[===================>]  71.83M  28.8MB/s    in 2.5s    \n",
            "\n",
            "2023-01-19 18:37:36 (28.8 MB/s) - ‘task2.tar.gz’ saved [75324736/75324736]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# replace here your ide &id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\"\n",
        "# replace here your id 'https://docs.google.com/uc?export=download&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL'\n",
        "# replace here your target name -O task1.tar.gz &&\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\" -O task2.tar.gz && rm -rf /tmp/cookies.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1bkbvqeF_IYf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!mkdir data\n",
        "!mv task2.tar.gz ./data\n",
        "!tar -xzvf \"/content/data/task2.tar.gz\" -C \"/content/data/\"     #[run this cell to extract tar.gz files]\n",
        "# this may take 12 seconds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_info = {\n",
        "    \"iteration\": 1,\n",
        "    \"image_processing\":{\n",
        "        \"resize\":224,\n",
        "        \"mean\":[0.485, 0.456, 0.406],\n",
        "        \"std\":[0.229, 0.224, 0.225],\n",
        "    },\n",
        "    \"hyperparameters_data\": {\n",
        "        \"batch_size\":32,\n",
        "        \"shuffle_dataloader\":True,\n",
        "        \"num_workers\":4\n",
        "    },\n",
        "    \"random_seeds\":{\n",
        "        \"torch_seed\":42,\n",
        "        \"numpy_seed\":42,\n",
        "        \"cuda_seed\":42,\n",
        "\n",
        "    },\n",
        "    \"hyperparameters_training\":{\n",
        "        \"learning_rate\": 0.0001,\n",
        "        \"scheduler_step_size\":7,\n",
        "        \"scheduler_gamma\":0.1,\n",
        "        \"num_epochs\":10, \n",
        "    },\n",
        "    \"total_unlabeled\":26445,\n",
        "}\n",
        "     "
      ],
      "metadata": {
        "id": "DLqmpUlecPCx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euHUUz0w_IYg"
      },
      "source": [
        "## Mount drive \n",
        "( in order to save each iteration of leave out labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R4ZCnSB_IYh",
        "outputId": "7f647fa6-850a-4b74-fed6-72c40c8c58fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j0gmw68_IYi",
        "outputId": "3ccc82e0-c591-4dab-c42d-fde67184ef77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Google Drive!"
          ]
        }
      ],
      "source": [
        "with open('/gdrive/MyDrive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat '/gdrive/MyDrive/foo.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0w0JOuB5_IYj"
      },
      "outputs": [],
      "source": [
        "!rm '/gdrive/MyDrive/foo.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "-g7qmzT4Wkbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShuffledImageFolder(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.indices = torch.randperm(len(dataset))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path, _ = self.dataset.samples[self.indices[index]]\n",
        "        image, label = self.dataset[self.indices[index]]\n",
        "        return image, label, path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "metadata": {
        "id": "Ci_KHZYMWqgD"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "toHy-rW4_IYr"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, scheduler, optimizer, criterion, dataset_size, dataloader):\n",
        "                     \n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over data.\n",
        "    for inputs, labels, _ in tqdm(dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "    \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_loss = running_loss / dataset_size\n",
        "    epoch_acc = running_corrects.double() / dataset_size\n",
        "    return model, epoch_loss, epoch_acc\n",
        "\n",
        "def train_model(model, *args, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()  # Set model to training mode\n",
        "        model, epoch_loss, epoch_acc = train_loop(model, *args)\n",
        "        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # deep copy the model\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_number, device='cuda'):\n",
        "    # Download a pytorch MobileNet pretrained model\n",
        "    model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "    # change the Linear output to fit our dataset ( because model initially has 1000 classes)\n",
        "    model.classifier[1] = nn.Linear(1280, 100)\n",
        "    # Read the models from drive that need to be used for the predictions\n",
        "    i=0\n",
        "    PATH = f'/gdrive/MyDrive/checkpoints/noisy_labels/model_it_{model_number}.pt'\n",
        "    model.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))\n",
        "    model = model.to(device) \n",
        "    model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "-gh3Y2c5PFYo"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_paths):\n",
        "        self.image_paths = image_paths\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # Load the image and return it as a tensor\n",
        "        image_path = self.image_paths[index]\n",
        "        image = Image.open(image_path)\n",
        "        image = image.convert('RGB')\n",
        "        image = transforms.ToTensor()(image)\n",
        "        label = int(image_path.split(\"/\")[-2])\n",
        "        return image, label, image_path"
      ],
      "metadata": {
        "id": "Z7xQs8c7bkQq"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batches_to_list(batches):\n",
        "    paths_list = []\n",
        "    for batch in batches:\n",
        "        paths_list += batch\n",
        "    return paths_list\n",
        "\n",
        "def generate_new_labels(model, loader, device='cuda'):\n",
        "    # create new predictions\n",
        "    \n",
        "    predicted_labels = {\n",
        "        \"path\":[],\n",
        "        \"confidence\":[],\n",
        "        \"existing_label\":[],\n",
        "        \"label_predicted\":[]\n",
        "    }\n",
        "\n",
        "    # Each epoch has a training and validation phase \n",
        "    model.eval()   # Set model to evaluate mode \n",
        "\n",
        "    # Iterate over data. \n",
        "\n",
        "    for image, label, image_path in tqdm(loader):\n",
        "        image = image.to(device)\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(False): # we don't want to train\n",
        "            outputs = model(image)\n",
        "            confidence, preds = torch.max(outputs, 1) \n",
        "\n",
        "        predicted_labels[\"path\"].append(image_path) \n",
        "        predicted_labels[\"confidence\"].append(confidence.item())\n",
        "        predicted_labels[\"label_predicted\"].append(preds.item())\n",
        "        predicted_labels[\"existing_label\"].append(label.item())\n",
        "\n",
        "    return predicted_labels"
      ],
      "metadata": {
        "id": "ASudnmb56uIy"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop2(model, scheduler, optimizer, criterion, dataset_size, dataloader1, dataloader2):\n",
        "                     \n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over data for confident data.\n",
        "    for inputs, labels in tqdm(dataloader1):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "    \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over data for relabling.\n",
        "    for inputs, labels, _ in tqdm(dataloader2):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "    \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_loss = running_loss / dataset_size\n",
        "    epoch_acc = running_corrects.double() / dataset_size\n",
        "    return model, epoch_loss, epoch_acc\n",
        "\n",
        "def train_model2(model, *args, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()  # Set model to training mode\n",
        "        model, epoch_loss, epoch_acc = train_loop2(model, *args)\n",
        "        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # deep copy the model\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "tO_cnZt4O-ea"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop3(model, scheduler, optimizer, criterion, dataset_size, dataloader1):\n",
        "                     \n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over data for confident data.\n",
        "    for inputs, labels in tqdm(dataloader1):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "    \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_loss = running_loss / dataset_size\n",
        "    epoch_acc = running_corrects.double() / dataset_size\n",
        "    return model, epoch_loss, epoch_acc\n",
        "\n",
        "def train_model3(model, *args, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()  # Set model to training mode\n",
        "        model, epoch_loss, epoch_acc = train_loop3(model, *args)\n",
        "        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # deep copy the model\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "-0Jf8W03vrbh"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21Vc-VIr_IYk"
      },
      "source": [
        "## Seting up things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JQr89Fhv_IYl"
      },
      "outputs": [],
      "source": [
        "# Set up seeds for reproducibility\n",
        "np.random.seed(experiment_info[\"random_seeds\"][\"numpy_seed\"])\n",
        "torch.manual_seed(experiment_info[\"random_seeds\"][\"torch_seed\"])\n",
        "torch.cuda.manual_seed_all(experiment_info[\"random_seeds\"][\"cuda_seed\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "M51baF3B_IYn"
      },
      "outputs": [],
      "source": [
        "dir_data = 'data/task2/train_data/'\n",
        "# Read the annotations file into a DataFrame\n",
        "df = pd.read_csv(f'{dir_data}annotations.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "n9G-lt1HbzUp",
        "outputId": "2e09f3f4-c997-466e-f7be-d2e6a48731db"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     renamed_path  label_idx\n",
              "0          task2/train_data/images/labeled/0.jpeg          0\n",
              "1          task2/train_data/images/labeled/1.jpeg          0\n",
              "2          task2/train_data/images/labeled/2.jpeg          0\n",
              "3          task2/train_data/images/labeled/3.jpeg          0\n",
              "4          task2/train_data/images/labeled/4.jpeg          0\n",
              "...                                           ...        ...\n",
              "49995  task2/train_data/images/labeled/49995.jpeg          5\n",
              "49996  task2/train_data/images/labeled/49996.jpeg         94\n",
              "49997  task2/train_data/images/labeled/49997.jpeg         24\n",
              "49998  task2/train_data/images/labeled/49998.jpeg         85\n",
              "49999  task2/train_data/images/labeled/49999.jpeg          5\n",
              "\n",
              "[50000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-433c70c7-aa9e-4a7d-aba7-b3add59b5295\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>renamed_path</th>\n",
              "      <th>label_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>task2/train_data/images/labeled/0.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>task2/train_data/images/labeled/1.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>task2/train_data/images/labeled/2.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>task2/train_data/images/labeled/3.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>task2/train_data/images/labeled/4.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>task2/train_data/images/labeled/49995.jpeg</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>task2/train_data/images/labeled/49996.jpeg</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>task2/train_data/images/labeled/49997.jpeg</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>task2/train_data/images/labeled/49998.jpeg</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>task2/train_data/images/labeled/49999.jpeg</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-433c70c7-aa9e-4a7d-aba7-b3add59b5295')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-433c70c7-aa9e-4a7d-aba7-b3add59b5295 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-433c70c7-aa9e-4a7d-aba7-b3add59b5295');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhxmBea3_IYo",
        "outputId": "0d95ee4e-14ee-4880-eecf-f5e95c9078d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/task2/images_by_class does not exist\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:15<00:00, 3158.18it/s]\n"
          ]
        }
      ],
      "source": [
        "# Define the base directory\n",
        "\n",
        "base_dir = 'data/task2/train_data/images'\n",
        "# Check if the directory exists, to recreate it instead of messing it up\n",
        "target_dir = 'data/task2/images_by_class'\n",
        "\n",
        "if os.path.exists(target_dir):\n",
        "    # Use rmtree to delete the directory and all its contents\n",
        "    shutil.rmtree(target_dir)\n",
        "    print(f'{target_dir} has been deleted')\n",
        "else:\n",
        "    print(f'{target_dir} does not exist')\n",
        "\n",
        "# Iterate over the rows in the DataFrame\n",
        "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    # Extract the path and class from the row\n",
        "    path = row['renamed_path']\n",
        "    label = row['label_idx']\n",
        "    \n",
        "    # Create the directory for the class\n",
        "    class_dir = f'{target_dir}/{label}'\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    \n",
        "    # Copy the file to the class directory\n",
        "    shutil.copy(f\"data/{path}\", class_dir) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iteration 1"
      ],
      "metadata": {
        "id": "ZtQ5oHh6XdJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create datset splits for pseudo-labeling"
      ],
      "metadata": {
        "id": "FSoVcX3tdmlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(experiment_info[\"image_processing\"][\"resize\"]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=experiment_info[\"image_processing\"][\"mean\"],\n",
        "                         std=experiment_info[\"image_processing\"][\"std\"]),\n",
        "    ])"
      ],
      "metadata": {
        "id": "pOGkTh1scJR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'data/task2/images_by_class'\n",
        "image_dataset_unshuffled = datasets.ImageFolder(data_dir, preprocess)\n",
        "image_dataset = ShuffledImageFolder(image_dataset_unshuffled)"
      ],
      "metadata": {
        "id": "7iflapmiM4QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = len(image_dataset)\n",
        "\n",
        "# Split the image_dataset into 10 parts\n",
        "part_size = num_samples // 10\n",
        "parts = [list(range(i * part_size, (i + 1) * part_size)) for i in range(10)]\n",
        "\n",
        "batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"]\n",
        "# Use one part for creating new labels and the other 9 parts for training the model\n",
        "loaders = {}\n",
        "for i in range(10):\n",
        "    # Get the indices for the training set, which are not in the labeling set\n",
        "    train_indices = [index for j, part in enumerate(parts) if j != i for index in part]\n",
        "    labeling_indices = parts[i]\n",
        "\n",
        "    # Create a sampler for the training set and the labeling set\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    labeling_sampler = SubsetRandomSampler(labeling_indices)\n",
        "\n",
        "    # Use the samplers to create data loaders for the training set and the labeling set\n",
        "    train_loader = DataLoader(image_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    labeling_loader = DataLoader(image_dataset, batch_size=batch_size, sampler=labeling_sampler)\n",
        "    loaders[i] = {\n",
        "        \"train\":train_loader, \n",
        "        \"labeling\":labeling_loader\n",
        "    }\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data_sizes = {\n",
        "    \"train\" : len(loaders[0][\"train\"]), # this is the number of batches not images\n",
        "    \"labeling\" : len(loaders[0][\"labeling\"]),\n",
        "}"
      ],
      "metadata": {
        "id": "aMpLE-QAvxFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sanity check\n",
        "See how many labels are in each training set\n",
        "Visualize the distribution of labels"
      ],
      "metadata": {
        "id": "J1WLWbz0VBiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(len(loaders[0][\"train\"])*batch_size)\n",
        "print(len(loaders[0][\"labeling\"])*batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrjSwJ29xbgv",
        "outputId": "6736d904-0171-410a-9774-726ea84f64a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45024\n",
            "5024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check, verify the distribution of labels in training and labeling sets\n",
        "labels_for_distribution = [label_batch for _,label_batch,_ in tqdm(loaders[0][\"train\"])]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GIbW5b8TW-B",
        "outputId": "f8d448c9-864d-4df6-d739-070cf41ceec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [01:23<00:00, 16.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_distrib = torch.cat(labels_for_distribution)"
      ],
      "metadata": {
        "id": "VkOog_OqUEON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts, bins, patches = plt.hist(labels_distrib, bins = 100)\n",
        "\n",
        "# Set x-axis label\n",
        "plt.xlabel('Labels')\n",
        "plt.xticks(bins, bins.astype(int))\n",
        "# Set y-axis label\n",
        "plt.ylabel('Number of Observations')\n",
        "\n",
        "# Set plot title\n",
        "plt.title('Distribution of Labels')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "z1Mj6kPMuCPN",
        "outputId": "7eea9428-09c3-49f3-fbfd-6d84b798d4a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Distribution of Labels')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5xddbX//9dKIwkBIsUYUgjtqlwExIAISgsqRQg/r4CoFEG5ihdQQCnSRH8X0AtevKhcMChFRUCvNAtSQpGWECCAtBBISCWkkF4ms75/rLXPOYwzkzNJzpwp7+fjMY/ZZ9e1P7usz2fvffYxd0dERASgR70DEBGRjkNJQURESpQURESkRElBRERKlBRERKRESUFEREqUFKSuzOxqMzt/Pc1ruJktNrOe+XmsmX1lfcw75/dnMztufc2vDcv9gZm9bWaz1uM8R5iZm1mv9pxWOj4lBakZM3vDzJaZ2SIzW2Bmj5rZ18ystN+5+9fc/ftVzuuA1sZx96nuPsDdV6+H2C8ys5uazP8gd79+XefdxjiGA2cAO7j7+5oZvq+ZTWvPmKRrU1KQWjvU3TcCtgIuBc4CxqzvhXThWutwYK67v1XvQKR7UFKQduHu77j7HcBRwHFmtiOAmf3KzH6Q3Zub2V3ZqphnZg+bWQ8zu5E4Od6Zl4e+U3EJ40Qzmwrc38JljW3N7EkzW2hmt5vZprmsf6phF60RMzsQOBc4Kpf3bA4vXY7KuM4zsylm9paZ3WBmm+SwIo7jzGxqXvr5bktlY2ab5PRzcn7n5fwPAP4GbJlx/KotZW5mh5jZ07nub5rZRc2MdoKZzTCzmWZ2ZsW0PczsbDN7zczmmtktRdk1s5zjzWxytghfN7MvtiVO6ViUFKRdufuTwDTgE80MPiOHbQEMIk7M7u7HAFOJVscAd/9hxTT7AB8EPt3CIo8FTgAGAw3AT6qI8S/AfwK/y+Xt3Mxox+fffsA2wADgqibjfBx4PzAKuMDMPtjCIv8H2CTns0/G/GV3vxc4CJiRcRy/ptibWJLzGggcAnzdzA5vMs5+wPbAp4CzKi7RnQIcnvFsCcwHftp0AWa2IVGmB2WLcE/gmTbGKR2IkoLUwwyguVrnKuLkvZW7r3L3h33NL+e6yN2XuPuyFobf6O7Pu/sS4HzgyOJG9Dr6InCFu09298XAOcDnm7RSvufuy9z9WeBZ4J+SS8byeeAcd1/k7m8AlwPHrGuA7j7W3Z9z90Z3nwj8ljjJV/pelt9zwC+Bo7P/14Dvuvs0d18BXAR8roXLdI3AjmbWz91nuvsL6xq71I+SgtTDEGBeM/1/BEwC7snLEWdXMa832zB8CtAb2LyqKFu3Zc6vct69iBZOofJpoaVEa6KpzTOmpvMasq4BmtlHzeyBvCz1DnGib7ruTctny+zeCvi/vJS3AHgRWM27149MtkflvGea2d1m9oF1jV3qR0lB2pWZ7Uac8B5pOixryme4+zbAYcDpZjaqGNzCLNfUkhhW0T2caI28TVxa6V8RV0/islW1851BnDgr590AzF7DdE29nTE1ndf0Ns6nOb8B7gCGufsmwNWANRmnafnMyO43iUtCAyv++rr7P8Xl7n91908SrbyXgGvXQ+xSJ0oK0i7MbGMz+wxwM3BTXq5oOs5nzGw7MzPgHaJm2piDZxPX3NvqS2a2g5n1By4GbstHVl8B+ubN2N7AecAGFdPNBkZUPj7bxG+Bb5nZ1mY2gPI9iIa2BJex3AL8/2a2kZltBZwO3NT6lO9mZn2b/BmwETDP3Zeb2e7AF5qZ9Hwz629m/wp8Gfhd9r86Y9oq57+FmY1uZrmDzGx03ltYASymvM2kE1JSkFq708wWETXP7wJXECef5mwP3EucWB4DfubuD+SwS4Dz8nLGmS1M35wbgV8Rl3L6AqdCPA0FnAz8gqiVLyFuchduzf9zzWxCM/O9Luf9EPA6sJy4Obs2TsnlTyZaUL/J+VdrCLCsyd+2xPpdnOV/AZF8mnqQuGR3H/Bf7n5P9r+SaGXck9M/Dny0mel7EElsBnFJcB/g622IXToY04/siIhIQS0FEREpUVIQEZESJQURESlRUhARkZJO/RKxzTff3EeMGFHvMEREOpWnnnrqbXfforlhnTopjBgxgvHjx9c7DBGRTsXMprQ0TJePRESkRElBRERKlBRERKRESUFEREqUFEREpERJQURESpQURESkRElBRERKlBRERKSkU3+juT2NOPvud31+49JD6hSJiEjtqKUgIiIlSgoiIlKipCAiIiVKCiIiUqKkICIiJUoKIiJSoqQgIiIlSgoiIlKiL6+JiLSTyi/BdtQvwKqlICIiJUoKIiJSoqQgIiIluqfQRXSGa5Ui0vHVtKVgZt8ysxfM7Hkz+62Z9TWzrc3sCTObZGa/M7M+Oe4G+XlSDh9Ry9hEROSf1SwpmNkQ4FRgpLvvCPQEPg9cBvzY3bcD5gMn5iQnAvOz/49zPBERaUe1vqfQC+hnZr2A/sBMYH/gthx+PXB4do/Oz+TwUWZmNY5PREQq1CwpuPt04L+AqUQyeAd4Cljg7g052jRgSHYPAd7MaRty/M2aztfMTjKz8WY2fs6cObUKX0SkW6rZjWYzew9R+98aWADcChy4rvN192uAawBGjhzp6zo/kY5IDw5IvdTy8tEBwOvuPsfdVwF/APYCBublJIChwPTsng4MA8jhmwBzaxifiIg0UcukMBXYw8z6572BUcA/gAeAz+U4xwG3Z/cd+Zkcfr+7qyUgItKOanlP4QnihvEE4Llc1jXAWcDpZjaJuGcwJicZA2yW/U8Hzq5VbCIi0ryafnnN3S8ELmzSezKwezPjLgeOqGU8IiLSOn2jWaQFutkr3ZHefSQiIiVqKXQCqrGKSHtRS0FEREqUFEREpERJQURESpQURESkRElBRERKlBRERKRESUFEREqUFEREpERfXkNfDhMRKailICIiJUoKIiJSoqQgIiIlSgoiIlLSpqRgZu8xs51qFYyIiNTXGpOCmY01s43NbFPipzWvNbMrah+aiIi0t2paCpu4+0Lgs8AN7v5R4IDahiUiIvVQzfcUepnZYOBI4Ls1jqfLqua7EPq+hIjUWzUthYuBvwKT3H2cmW0DvFrbsEREpB7W2FJw91uBWys+Twb+rZZBiYhIfawxKZjZFsBXgRGV47v7CbULS0RE6qGaewq3Aw8D9wKraxuOiIjUUzVJob+7n1XzSEREpO6qudF8l5kdXPNIRESk7qpJCqcRiWG5mS3Kv4W1DkxERNpfNU8fbdQegYiISP1V9SM7ZnYYsHd+HOvud9UuJBGR2tIXRVtWzbuPLiUuIf0j/04zs0tqHZiIiLS/aloKBwO7uHsjgJldDzwNnFPLwEREpP1V++rsgRXdm9QiEBERqb9qWgqXAE+b2QOAEfcWzq5pVCIiUhfVPH30WzMbC+yWvc5y91k1jUpEROqixctHZvaB/L8rMBiYln9bZj8REeliWmspnA6cBFzezDAH9q9JRCIiUjctJgV3Pyk7D3L35ZXDzKxvTaMSaYGeLxeprWqePnq0yn4iItLJtdhSMLP3AUOAfmb2YeLJI4CNgf7VzNzMBgK/AHYkLjmdALwM/I74fYY3gCPdfb6ZGXAl8b2IpcDx7j6h7askIrWk1lrX1to9hU8DxwNDgSsq+i8Czq1y/lcCf3H3z5lZHyKZnAvc5+6XmtnZxOOtZwEHAdvn30eBn+d/ERFpJ63dU7geuN7M/s3df9/WGZvZJsR3Go7P+a0EVprZaGDfHO16YCyRFEYDN7i7A4+b2UAzG+zuM9u6bBGR9tAVW03VfE/h92Z2CPCvQN+K/hevYdKtgTnAL81sZ+Ap4h1KgypO9LOAQdk9BHizYvpp2e9dScHMTiKeimL48OFrCl9ERNqgmhfiXQ0cBZxC3Fc4Atiqinn3AnYFfu7uHwaW0OSb0Nkq8LYE7O7XuPtIdx+5xRZbtGVSERFZg2pec7Gnu+9kZhPd/Xtmdjnw5yqmmwZMc/cn8vNtRFKYXVwWMrPBwFs5fDowrGL6odlPpO664mUCkeZU80jqsvy/1My2BFYR33BuVb4K400ze3/2GkW8evsO4Ljsdxxwe3bfARxrYQ/gHd1PaH8jzr679Cci3U81LYW78tHSHwETiMs911Y5/1OAX+eTR5OBLxOJ6BYzOxGYAhyZ4/6JeBx1EvFI6perXQkREVk/qrnR/P3s/L2Z3QX0dfd3qpm5uz8DjGxm0KhmxnXgG9XMV0REaqOaG80TzexcM9vW3VdUmxBERKTzqeaewqFAA3HJZ5yZnWlmehZURKQLquby0RTgh8APzWx74HzgMqBnjWMTEWkzPSm2bqq50YyZbUV8V+EoYDXwnVoGJVKNznrwd9a4pXtYY1IwsyeA3sAtwBHuPrnmUXUCOrBF1p2Oo46n1aRgZj2AP7j7Ze0Uj4jUkU7S0uqNZndvJF5rISIi3UA1Tx/dm08cDTOzTYu/mkcmIiLtrpobzUfl/8ovljmwzfoPR0RE6qmaR1K3bo9ARESk/qr5RnN/MzvPzK7Jz9ub2WdqH5qIiLS3au4p/BJYCeyZn6cDP6hZRCIiUjfV3FPY1t2PMrOjAdx9qZlZjeOSDkaPKop0D9W0FFaaWT/yF9LMbFtgRU2jEhGRuqimpXAh8BdgmJn9GtgLOL6WQYmISH1U8/TR38xsArAH8RvNp7n72zWPTERE2l01Tx/tBSx397uBgcC5+YI8ERHpYqq5fPRzYGcz2xk4HRgD3ADsU8vApHvrjDe2axVzZywL6byqSQoN7u5mNhr4qbuPyd9Xlg5KJxGRddddj6NqksIiMzsHOAb4RL45tXdtwxIRkXqo9t1HXwBOcPdZ+VOcP6ptWJ1Xd61diEjXUM3TR7PM7DfA7mZ2KDDO3W+ofWgi7a8yqYt0R9U8ffQV4Engs8DngMfN7IRaByYiIu2vmstH3wY+7O5zAcxsM+BR4LpaBiYiIu2vmqQwF1hU8XlR9hORDq673OPSZb/1p8WkYGanZ+ck4Akzu514/9FoYGI7xCYiIu2stZbCRvn/tfwr3F67cEREpJ5aTAru/r2i28wGZL/F7RGUiIjUR6v3FMzs68A5wIb5eTFwmbv/rB1iq4vucg12fWl6LbeyzFSWItVp7Thqb63dUziP+LW1fd19cvbbBrjSzDZ1907962u6MbVm7V1G2ibSFXW2/bq1lsIxwM7uvrzo4e6TzexI4Fn0k5x1odq3rElnOwm1hfb/2mstKXhlQqjouczMGmsYk4h0cjp5d16tJYXpZjbK3e+r7Glm+wMzaxuW1IIO1OZ15Zp1Z6V9tX5aSwqnAreb2SPAU9lvJPFznKNrHZi0n/Y4Keog75q0XbueFt995O4vADsCDwEj8u8hYMccJiIiXUyrj6TmPQW940hEpJtY41tSRUSk+1BSEBGRkhaTgpndl/8va79wRESknlq7pzDYzPYEDjOzmwGrHOjuE6pZgJn1BMYD0939M2a2NXAzsBnxVNMx7r7SzDYAbgA+Qrya+yh3f6OtKyQisrb0eHLrSeEC4HxgKHBFk2EO7F/lMk4DXgQ2zs+XAT9295vN7GrgRODn+X++u29nZp/P8Y6qchnSSekgFOlYWnsk9TZ3Pwj4obvv1+SvqoRgZkOBQ4Bf5GcjksltOcr1wOHZPTo/k8NH5fgiItJO1vjLa+7+fTM7DNg7e41197uqnP9/A9+h/NsMmwEL3L0hP08DhmT3EODNXGaDmb2T479dOUMzOwk4CWD48OFVhtFxqaYs0rl09WN2jUnBzC4Bdgd+nb1OM7M93f3cNUz3GeAtd3/KzPZd50iTu18DXAMwcuRIX1/zlc5N36wVWT+q+Y3mQ4Bd3L0RwMyuB54GWk0KxOswDjOzg4G+xD2FK4GBZtYrWwtDgek5/nRgGDDNzHoBm6Dfgu6SunpNq5aU/KTWqkkKAAOBedm9STUTuPs5xA/0kC2FM939i2Z2K/A54gmk4yj/vOcd+fmxHH6/u6slINJJKNl3DdUkhUuAp83sAeKx1L2Bs9dhmWcBN5vZD4gWx5jsPwa40cwmEQno8+uwDJFW6QQmnUV7tw6rudH8WzMbC+yWvc5y91ltWYi7jwXGZvdk4h5F03GWA0e0Zb4iUj1deur82mMbVnX5yN1nEpd3REQ6JbUOq1PtPQURaYZONNLV6IV4IiJS0mpLId9b9IK7f6Cd4hGRLkb3MjqXNf3Izmoze9nMhrv71PYKSqSSLtGItJ9q7im8B3jBzJ4ElhQ93f2wmkUlVdHJUtaWau+11ZmPzWqSwvk1j6KT64w7QGeMWVqm7SnrSzXfU3jQzLYCtnf3e82sP9Cz9qF1futyoOog71q0PdesrWWkMq2Nal6I91XiraSbAtsSbzO9GhhV29CkM+oOB2o169gdymFtqFw6vmouH32D+AbyEwDu/qqZvbemUcl6090PQl07X7Puvo/Iu1WTFFbkz2UCkG8w1YvqurGOfhJpKb7OGnd31xHKpSPE0F6qSQoPmtm5QD8z+yRwMnBnbcMS6R468n0ntbK6p2qSwtnE7yc/B/w78Cfy5zW7uloddN2p1iFdQ2fdZ3X/p+2qefqoMX9Y5wnistHL+p0DEZF3W59PT9UzUVXz9NEhxNNGrxG/p7C1mf27u/+51sGJiEj7quby0eXAfu4+CcDMtgXuBpQURES6mGrekrqoSAhpMrCoRvGIiEgdtdhSMLPPZud4M/sTcAtxT+EIYFw7xCYiIu2stctHh1Z0zwb2ye45QL+aRSQiInXTYlJw9y+3ZyAiIlJ/1Tx9tDVwCjCicny9OltEpOup5umjPwJjiG8xN9Y2HBERqadqksJyd/9JzSMREZG6qyYpXGlmFwL3ACuKnu4+oWZRiYhIXVSTFD4EHAPsT/nykednERHpQqpJCkcA27j7yloHIyIi9VXNN5qfBwbWOhAREam/aloKA4GXzGwc776noEdSRUS6mGqSwoU1j0JERDqEan5P4cH2CEREROqvmm80L6L8m8x9gN7AEnffuJaBiYhI+6umpbBR0W1mBowG9qhlUCIiUh/VPH1U4uGPwKdrFI+IiNRRNZePPlvxsQcwElhes4hERKRuqnn6qPJ3FRqAN4hLSCIi0sVUc09Bv6sgItJNtPZznBe0Mp27+/drEI+IiNRRay2FJc302xA4EdgMUFIQEeliWnz6yN0vL/6Aa4jfZf4ycDOwzZpmbGbDzOwBM/uHmb1gZqdl/03N7G9m9mr+f0/2NzP7iZlNMrOJZrbrellDERGpWquPpOYJ/AfARKJVsau7n+Xub1Ux7wbgDHffgfhewzfMbAfgbOA+d98euC8/AxwEbJ9/JwE/X5sVEhGRtddiUjCzHwHjgEXAh9z9InefX+2M3X1m8UM87r4IeBEYQjy5dH2Odj1weHaPBm7I70I8Dgw0s8FtXSEREVl7rbUUzgC2BM4DZpjZwvxbZGYL27IQMxsBfBh4Ahjk7jNz0CxgUHYPAd6smGxa9ms6r5PMbLyZjZ8zZ05bwhARkTVo8Uazu7fp284tMbMBwO+Bb7r7wnhTRmkZbmbe4sTNx3UNcY+DkSNHtmlaERFp3Xo58bfEzHoTCeHX7v6H7D27uCyU/4v7E9OBYRWTD81+IiLSTmqWFPLleWOAF939iopBdwDHZfdxwO0V/Y/Np5D2AN6puMwkIiLtoJrXXKytvYBjgOfM7Jnsdy5wKXCLmZ0ITAGOzGF/Ag4GJgFLicdfRUSkHdUsKbj7I4C1MHhUM+M78I1axSMiImtW03sKIiLSuSgpiIhIiZKCiIiUKCmIiEiJkoKIiJQoKYiISImSgoiIlCgpiIhIiZKCiIiUKCmIiEiJkoKIiJQoKYiISImSgoiIlCgpiIhIiZKCiIiUKCmIiEiJkoKIiJQoKYiISImSgoiIlCgpiIhIiZKCiIiUKCmIiEiJkoKIiJQoKYiISImSgoiIlCgpiIhIiZKCiIiUKCmIiEiJkoKIiJQoKYiISImSgoiIlCgpiIhIiZKCiIiUKCmIiEiJkoKIiJQoKYiISImSgoiIlCgpiIhIiZKCiIiUdKikYGYHmtnLZjbJzM6udzwiIt1Nh0kKZtYT+ClwELADcLSZ7VDfqEREupcOkxSA3YFJ7j7Z3VcCNwOj6xyTiEi30qveAVQYArxZ8Xka8NGmI5nZScBJ+XGxmb28lsvbHHh7LbvXdfqu2t1R4uho3R0ljo7W3VHi6GjdVY1nl/3TNG2xVYtD3L1D/AGfA35R8fkY4KoaLm/82nav6/RdtbujxNHRujtKHB2tu6PE0dG613aa9fXXkS4fTQeGVXwemv1ERKSddKSkMA7Y3sy2NrM+wOeBO+ock4hIt9Jh7im4e4OZ/QfwV6AncJ27v1DDRV6zDt3rOn1X7e4ocXS07o4SR0fr7ihxdLTutZ1mvbC8NiUiItKhLh+JiEidKSmIiEhJh7mn0J7M7EDgSuLexUPAXsBgIkm+QXyr+gZgUE6yIbCAKK/bgIuB8cD7gdeA1YBn967AlsBUYCnwAWA+MBd4HzAQaACeAbbJ/hvnslYBK4BNcnkGDMj5L83u5cBdwGfy8zKgd87zDWD7nG4l5e27KufRuyiCHD4nl9sr4zegD7AQ6Ac0Zj9y+GpgNtA/p1udyyXj6lcxr4Ys3565TpbdDTldnxyvMePqmcvtn+P0yGlWZ6xLcrzNs9+qXG5jLrN3zm9lRXm8lNuoAZhCPNHmWeZbZnf/LOveWX7b5TzeIJ6GexyYBPxHlvUrudyhQF/iuzXb5Pq/DuyY67I8x9sg57eM2K6rMuYNcv3mE/tEUUZ9sntVzr/Yb6iYZ58c3jvLaXmWQQ9gUW6HYhuQ87WKMu2dcb8vx1uUwzbM+fYAZmWc2+V8lud0xf++Oe2y7O6VcRfr3JCfN6S8n2yQw9/JsmjIZRTrszJjX0wcE1SUV5+cT4/8W5Xr0ovyPlXsww0V67k6uzcg9q8iBquY3wpi2w0H3pNxeP7NyFg2o7zf9c5xqCjTFTnvFVlGAynvjxvlOvfI9bAcp1inh4AP5/YotjHATOL7W71yuZ7djcCLGUdjjvsG8EV3X8g66nYthSav09gR+CJwCnA4sQMUO/QZ7r4D8QW6lcDRwC7AgcAVxEYB2M/ddwEmAn9x922InWh34FBiI+5DPE3VF7iUOMn0Ar5CnOTuzPGXAA8DTxEH2/+X/RcBf8pYFgGbAs8RO8Rvs/+snH6vjO2o7O857T5Eonotx72UOCGtJHbIm7J7XK7fTCLB/ZrYUXcFLgfem/EvzFh2BU7MctudOAkX084gDvCP5LJXAvtlfIszprsznlnAyUSy3C/LZEZ2/5I40Y6nnPz2y+22FNg3lz8x+y8D3iIOtp65rOtyey0EfkZ86efNLIMXgLE5zltEAvlVxvQB4GNZjv/Ibf1izmcscC2RpB8mKhJOHMDDs9yWEQnkH9k9JJe/BHgUOD/LZhjw8yzTYcB5WV4vEieUBdn/4Oy/jbv3zHUYRjy+/QZwZsa/FDgn41oInEXs9zcRlYFHiBP2rVnmN2T3uTldcXJ9J8vnXOKYWQacl8v+v+w/HfgNcVKbmtNtl8teCWyb67yMSLLnEZWL7XKdlxCVme/m+M/kchdn/0OyTHcgTqRzsv+sLO9/JfaVPsQ+8SqxP3wDuJc4z80Fzs7YTgHuIfaNU4Df5TxezuGe/U/OeZJl9RHimJuY5dsIfIty8hsJTCYSyEdyvv2I/fZnGcdI4JbsvxtwFfBJ4rgskvNuRCVkOLHv7Uokj92yrOcR3+vqDTS4+4dyW3yb9aDbJQUqXqdBnAynAB9x9/uJgt3I3We6+wQAd19EHNBDiI3QH9gT+EUxQzPbBNgbGJPTrHT3BcDHiY35NrHTzSN2PIDbiQ2+MfC9HPY28C/EjrnU3R/I/ktymnnENtsO+H72m5L9NyV2+tkZwx3ZvxfwcK7PUOBZ4qArarVP5Lqdn/PbgDhRP5P97yROKkOIg9qAG4kDZ1b2Hw1MAN7r7pOyewiRQKZk91Di4HHiJFHUZq/KMnDKJ3EHnsxYnGgRDQT+J2Pskf2/RpzEi9aG53yL1sbHMublxImlF3HwHgL8kEjI/YD/IhL2Ibm+A7L7VaJFtCyXgZkNJU7MF1RM89PsPixja8zt0Ui5RTOM8vFWLI8s/0Kx/mScGxD7RqWvEyfMoqZa1BQHZRmNybLuk2X7gVzPq3K5e2d5/ziX8XHihDUku6/K2IcRFRmIb79eBZyan/879/mR2X8Q8GTu88OBBe4+BdgZaMzuJynXqB14LfsvAJZl9045/1OJbTU/+18AvOrurwKjiO0yldi/GonjZnDOdz4wIst6XpZHT+I4fDS75xH7aDFOv+xfHNNF/68T+2+xXy3I8YpjdEkurweRqA7Osl+a3Y/kcjfNbfZy9p9FJICDs5wbicpYTyJ5HUxUtCYDB+TyXsr+gzL2z+ayisrp34B/Y32oxTfiOvIfFd+czu6x5Deniaw/t8n4I4gdcCJRc3mJqAXsmxt/ApE0Xidql08TO9eGRM3ztpxuHnFy3YWoeT2W46+uWM7zxI73OJG4iv4LiSQygzgZXJP9V2dsLxI76WW5/CVEreJI4oQ4lahRriASx1SiNujZvXHOr5Go3R1a0f/eXM9p+Vc07RfmMp8nDoyFRI3ocWKnP5Byzatojv+ZaOF4xvJMxjonx1lMnKyfyXV7jfLlgweyf3H5Z1l2P5zxNRC19ik5bXGpaxVx0hiV81+V2++I7J5BJLWZRMvqlSyzubm+C3P8hpx+QQ6/LmP+O9HanEmcvIoYV2bcC4h9pDHnMYFoCTYSJ4YXM74JOe6KnLa4DDGB8uWRZdm9NMttUU5TxLwq+xfzvim7G7P76Zz3QiL5vUQk1WlZDkvyb1Uu66aKZc+vWPbblE+I9+S4i3M+q4Hf5L7bACzP7uty2rHEvvwf2b84tt7Mefw9+68gjpmJuW5/IxLoTGIf2TuXX6yvEy2lxfl5dXbPpVyrf5vYR4v9YEHFdimW77m84nLUs5RbP/OI47why7CR8jG5gtj/iu1+LVHpLC5BfSHL6VrgwbSrJ2QAAAnaSURBVJz/ohxW7LvFPOdmbKuIfXh8zv9O4tyxIqddAXw1y+t0YFFX+0Zzh2NmA4DfA990952I7L0R5Wunj7r7rsBpxEn1MXf/MLGjnkucbAYBW+f/ScSGHUGcPIpaXqWmzwh/I/8fDuyf3fPy/2qitnZwfj6UuOQ0i2iiHpbjfMvdhxGtgR8QO9uAnOabGcfviYMH4qD7JnAGcUL8ErGDv5c4CTQSNd2vELXxAURNaF/iWrERtZYtgSvcvU+WxwFEjb6oXX2JaLn1IxLDTkQym0QkvqlE0/otoib4pYxxItFaayAuKewOHE+c6ItLZHOJRDCOqEH/KJfZ6O5PEU3y4uQ9ktimX8llzSJquRsDK3P8VUTyv5mo0X2cOKAvzzJdkWXRBxiX63wdUTnol8tZleV6UJbhJKJWPJtIFH/PfgdkeSzK8Wdl/wMryu4golLTL+N6lahdn060Agbk+Ctze6zIfdOJffi2LM8tshwPy3n9X5Zd31ynt3P6P1C+13Q7cWllIFHTHU+c7K8nTnwHmtmELKel+WXUwypi2Qe41cwuII6LTxKt1n7AUzl+L+ATuT/0IVogn8hxjiIS8YDc3oOIS1LDKZ9Qjbicsn9OP4+4tLoVcexdl9vj1FzHgTn+0opybyD2u0XEvvEy0aK6Drgvy3xFbiuIfbAXcfwfRty3bCBaamcSFaajiasGDxHJZ1PiktBhGedjOU1/yueZnsT+MopoHYwlWkBnAZea2VO5TYvW47qpd829Di2FjwF/reh+BTgnP18GzMru3sQX6U6vmPYS4uQ3jzhQlxK1qfcRO+KZOd4niAPlWWBMxfTHEpdengf+k9iRVxA73gii5vYy5ZubxxM1xRdy+k9TrnlPo1zT343YEV+n3OJ4jdjZixttxfoUNwofJg6KYdn/wlz2POKewom5Tmfl+H8nalLFTdOilfJALuv1orzyc1HTLb4LU9xc+3YuYxpxoHyMOLjmEQfP/Rl/D6LmfSFxoK/McYqa1JmUE8CZFfMvarDFdlqS5VTUAovuoqXRWDFO0RooappeMU7RvSTHK26SLm4yTlGbvok4Sc3J+N7M8j0zt/fbGftI4I9ELbc/cBGRvIttN4/ytf3vEbXM+ZT3tXnESatoNZxJ7I+NxGXAKbmd787lFrX5kURlZiXRgnsfUQu/L8vXKbfAVhMtxkfz8905fnEzeSHlff6e/Dw6p386u8fm+j9H7N/HE/v7vbkep2YsK4kk2Zjb9zjKNfTROf/Xcl1Xk8cXUSFooHxfprh39F3KrfQTspwnE63oB3O+i3JeRU19WU5btLBfJPavpZRb6q8Rx/PPKrbzyTn+K9k9OMvylSzvO3PZ/Ynj/7SK7Ty3YjufnbEtAE7O9ZtfsV7/mfM3YGEO/xfiEp5aCmuh9DoNosawFeXayaHAIjMzIhO/CNxoZgNz2ouJWuqxxE79oLt/ifINogU53iiihngHsIeZ9c95HkKc7HsT1wTvIDbycTndQKIWBrHjfIeooRRPDb1M1GTvIGqqDUSNdw7la5wQNY6Nc1mriJrZmOx+Ibs3Iw7023M9iydelhIHyuXEjbI/5PjjiEtOM4H/JWrJjxGJb0LG/iJx03VIxjQ11xngqzn/lzLOXtl9MlETWkXcXBtJJII9iZruU0Stdk6W+7Ish5co30t5iag1Q5xwVhOJ5ljiJvWsXN/5xLXpAcTliOczxl/msI2AfyduzPciWluT3b14wuduonb6dJbHn4inwF7Oed2Y6zY+94tjiP2iuI/0nhzv65SfqPok8Cki+X4o13lclv8C4rLD7Fyfx7LfBsDzZvbRjPl1orbZO4fvleU5k3Iin0nsZ0WSGJ3rOjunLWqarxOJaSlx83gmsa3fyPEas9/GxElpXvY/kdjnX8lYj84yfS275xD7WtFi/g6xv/wty2bPLO87iIrGfGLfPijnsSTnc38uu6gk7G9m/Yn7VOS8F2UZTSJaun1znXckatr/S+wjOxP3qW7P4RcRx8GKnHZ2zmdgbqMFRLJ8iWgRH5XrcALRgrkty2RQlt2p+fktojX/KWJ7jySS2PXAT7Kst83x5gJXU36K7Ddmtk9un2czpi/l/D8LvGpmPYgK5tWsB93yG81mdjDw38TJ6BGitjo4B/cldsgtiFpN0XSdTeyYt7j7xWZ2NLFzvU6cQO6nfPlgCnFgbk3UnI+i/ERGT8qP7y0nDqQiORc3UIvHQCEOwh4V3UWNeCWxY3rFNMXw4mbnCsqPfhaPtRWP8b1D7IDb8O5HAysVj7UWjwCuzHjfyDJp2r/yccu5WV7DKD/mWDwGuIxIekXsxaOVUL7fUMRSzH8pcdB8iHLttXg8sNiJexI14Z7EdpxF1GgHZP8niMRf1ISHUn4ctoFImO/PWF4lTn6r3f1AM1ud6zM1x9uauIn7AnEP6YfEJY1PEpceVuZ8e1BugWxUUba9KD8qCuVHFIubscVjyDOJk1lx7Xs15ZvRxfRzsiw2zHXtRbmFuCjHL8q7KKei3JZQfnR4VcX/53KanYjtVfRfXrEeS/NvBdHKG0AkCCMqBv8LfJC4/v8PIjEP5t3brmiZbUwkiFNyO21A1Lq3z/7Lc9s9n+v7MeIS5S+JikFlBbc4oRblUKwvlG/+F49zFi2DadnvX3JZRUVgcfbfiPLDFcXjwltSfqy1KNt5OW4x/165fOPdj0wXrcoVRNJ1Yt8rHkwobsj35d33SIp7KMUjzsX+8gfiisc6n9C7ZVIQEZHmdcfLRyIi0gIlBRERKVFSEBGREiUFEREpUVIQEZESJQWRFpjZ4jaMe5GZnVmr+Yu0FyUFEREpUVIQaQMzO9TMnjCzp83sXjMbVDF4ZzN7zMxeNbOvVkzzbTMbZ2YTzazpW08xs8Fm9pCZPWNmz5vZJ9plZUSaoaQg0jaPAHvky+VuJl7XUNiJeAHbx4ALzGxLM/sU8a3c3Yk35H7EzPZuMs8vEO/j2oV49cIzNV4HkRZ1y19eE1kHQ4Hfmdlg4rUUr1cMu93dlwHLzOwBIhF8nHjnzdM5zgAiSTxUMd044Doz6w380d2VFKRu1FIQaZv/IX5/40PEC+X6Vgxr+s6Y4p1Ul7j7Lvm3nbuPeddI7g8R7weaDvzKzI6tXfgirVNSEGmbTYiTN5TfblsYbWZ9zWwz4rclxhGvEz8hf5sDMxtiZu+tnMjMtgJmu/u1xMv1dq1h/CKt0uUjkZb1N7NpFZ+vIF6vfKuZzSfejLt1xfCJxGufNwe+7+4zgBlm9kHgsXh7OouJVx+/VTHdvsC3zWxVDldLQepGb0kVEZESXT4SEZESJQURESlRUhARkRIlBRERKVFSEBGREiUFEREpUVIQEZGS/wfU6q4A5BFjCwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on the remaining data "
      ],
      "metadata": {
        "id": "dgSVdENHZedr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a pytorch MobileNet pretrained model\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "# change the Linear output to fit our dataset ( because model initially has 1000 classes)\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "# Setting hyperparameters\n",
        "\n",
        "model = model.to(device)\n",
        "print(model.classifier)\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "eb68d62d599a462f92aaca0ab0b854e3",
            "6eed1376f1f24e8abbe2d4ea0ca3fcb1",
            "07c37bb6b22b4c8abe8d16b61e28e1db",
            "15a23332521441cab29f7acedc079b16",
            "816420dd910b475faa99a8e9ef6eebd4",
            "71a16d1906924e738fc1baa304d94811",
            "a74028e913de43d3b1eae97433d91882",
            "07868acd1960497680b904b81fd0eaf9",
            "a56ed7ebe21c4011a23a63cad606d40c",
            "ba437891973d4629b4881ccb11f044ae",
            "a75d7f60d1c24475a1540bc1cfe94f7b"
          ]
        },
        "id": "C69DiUz3ZkEV",
        "outputId": "2eab0188-0052-48d7-de51-10af01ec0845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb68d62d599a462f92aaca0ab0b854e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=False)\n",
            "  (1): Linear(in_features=1280, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=experiment_info[\"hyperparameters_training\"][\"learning_rate\"],\n",
        "    )\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(\n",
        "    optimizer, \n",
        "    step_size=experiment_info[\"hyperparameters_training\"][\"scheduler_step_size\"], \n",
        "    gamma=experiment_info[\"hyperparameters_training\"][\"scheduler_gamma\"],\n",
        "    )\n",
        "     "
      ],
      "metadata": {
        "id": "8Mprko1yaAI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(model, \n",
        "                    exp_lr_scheduler, \n",
        "                    optimizer, \n",
        "                    criterion,  \n",
        "                    data_sizes[\"train\"], \n",
        "                    loaders[0][\"train\"], \n",
        "                    num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmCqNZ-saHr3",
        "outputId": "badfeed4-5771-4c1f-eca6-47f93af79d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:26<00:00,  5.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 72.5423 Acc: 13.7484\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 45.0769 Acc: 19.0071\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 36.1820 Acc: 20.9026\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 29.3312 Acc: 22.5622\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 23.8855 Acc: 24.1471\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 19.0713 Acc: 25.5572\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 15.4470 Acc: 26.7960\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 8.7784 Acc: 29.4200\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 6.8999 Acc: 30.1770\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 5.9939 Acc: 30.5394\n",
            "\n",
            "Training complete in 42m 54s\n",
            "Best val Acc: 30.539446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repeat process for the other iterations"
      ],
      "metadata": {
        "id": "q8xaVIRwm8PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# because unlike the missing labels example, the iterations are independent from eachother\n",
        "# We can train all the models at once, than make predictions for all the labels at once\n",
        "# this makes the things simpler"
      ],
      "metadata": {
        "id": "jIZWJhNplVOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), '/gdrive/MyDrive/checkpoints/noisy_labels/model_it_0.pt')"
      ],
      "metadata": {
        "id": "L5s4zlU1j_Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,10): \n",
        "    model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "    model.classifier[1] = nn.Linear(1280, 100)\n",
        "    model = model.to(device) \n",
        "    model = train_model(model, \n",
        "                    exp_lr_scheduler, \n",
        "                    optimizer, \n",
        "                    criterion,  \n",
        "                    data_sizes[\"train\"], \n",
        "                    loaders[i][\"train\"], \n",
        "                    num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])\n",
        "    torch.save(model.state_dict(), f'/gdrive/MyDrive/checkpoints/noisy_labels/model_it_{i}.pt')"
      ],
      "metadata": {
        "id": "Zq3Ax7_Pm6By",
        "outputId": "8328961f-1389-437e-f643-02c9b3034311",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.3708 Acc: 0.3042\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.4221 Acc: 0.3113\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.4296 Acc: 0.3149\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.4003 Acc: 0.3369\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.4245 Acc: 0.3028\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.4086 Acc: 0.3220\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.4538 Acc: 0.3298\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.3897 Acc: 0.3390\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.4531 Acc: 0.3177\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.4248 Acc: 0.3383\n",
            "\n",
            "Training complete in 43m 0s\n",
            "Best val Acc: 0.339019\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5652 Acc: 0.2687\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5455 Acc: 0.2694\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5926 Acc: 0.2608\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5716 Acc: 0.2580\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5509 Acc: 0.2523\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5032 Acc: 0.2587\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5528 Acc: 0.2601\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5674 Acc: 0.2715\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5859 Acc: 0.2694\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5112 Acc: 0.2559\n",
            "\n",
            "Training complete in 42m 32s\n",
            "Best val Acc: 0.271500\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1574 Acc: 0.2992\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2140 Acc: 0.3021\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2206 Acc: 0.2822\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1759 Acc: 0.2878\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1663 Acc: 0.2999\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2137 Acc: 0.2978\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2461 Acc: 0.2701\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1719 Acc: 0.3205\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2607 Acc: 0.2857\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1828 Acc: 0.2935\n",
            "\n",
            "Training complete in 42m 33s\n",
            "Best val Acc: 0.320540\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5430 Acc: 0.3326\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6434 Acc: 0.3603\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5310 Acc: 0.3497\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6065 Acc: 0.3319\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5749 Acc: 0.3539\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5546 Acc: 0.3490\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5303 Acc: 0.3632\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6104 Acc: 0.3340\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5767 Acc: 0.3653\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.5474 Acc: 0.3547\n",
            "\n",
            "Training complete in 42m 58s\n",
            "Best val Acc: 0.365316\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.9253 Acc: 0.4030\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.8417 Acc: 0.4108\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.9271 Acc: 0.4200\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.9143 Acc: 0.4335\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.9639 Acc: 0.4172\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.8806 Acc: 0.4193\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.9342 Acc: 0.4151\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.9048 Acc: 0.4080\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.9162 Acc: 0.3980\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 149.8818 Acc: 0.4271\n",
            "\n",
            "Training complete in 42m 57s\n",
            "Best val Acc: 0.433547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.7060 Acc: 0.3689\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6360 Acc: 0.3234\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6773 Acc: 0.3369\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6889 Acc: 0.3383\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6507 Acc: 0.3525\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6708 Acc: 0.3333\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6872 Acc: 0.3184\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6516 Acc: 0.3198\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6673 Acc: 0.3276\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.6903 Acc: 0.3611\n",
            "\n",
            "Training complete in 42m 51s\n",
            "Best val Acc: 0.368870\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2038 Acc: 0.3362\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1416 Acc: 0.3326\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1803 Acc: 0.3362\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1441 Acc: 0.3170\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1624 Acc: 0.3227\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1674 Acc: 0.3291\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1685 Acc: 0.3468\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1829 Acc: 0.3255\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1622 Acc: 0.3390\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1886 Acc: 0.3177\n",
            "\n",
            "Training complete in 43m 9s\n",
            "Best val Acc: 0.346837\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8979 Acc: 0.2402\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.9184 Acc: 0.2082\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8989 Acc: 0.2701\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8834 Acc: 0.2274\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8732 Acc: 0.2459\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8074 Acc: 0.2530\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8743 Acc: 0.2672\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8564 Acc: 0.2367\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8534 Acc: 0.2360\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.8483 Acc: 0.2523\n",
            "\n",
            "Training complete in 43m 8s\n",
            "Best val Acc: 0.270078\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2236 Acc: 0.2345\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2041 Acc: 0.2537\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2436 Acc: 0.2687\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2801 Acc: 0.2495\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2507 Acc: 0.2900\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2444 Acc: 0.2509\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1874 Acc: 0.2580\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.1854 Acc: 0.2786\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2250 Acc: 0.2679\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.2056 Acc: 0.2694\n",
            "\n",
            "Training complete in 42m 57s\n",
            "Best val Acc: 0.289979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the labels that will be used for relabling in a file, such that you can leave the training of the model alone, and later when you return, even if the session has finished, you have the models trained and you know which are the images that need new labels. And you can use the checkpoints to predict them"
      ],
      "metadata": {
        "id": "ZsTeJsCocwyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paths_labeling = {}\n",
        "for i in tqdm(range(10)):\n",
        "    paths_labeling[i] = []\n",
        "    for images, labels, paths in loaders[i]['labeling']:\n",
        "        paths_labeling[i].append(paths)"
      ],
      "metadata": {
        "id": "iXT_CNZKny20",
        "outputId": "6af4be55-4996-48f9-9617-4d58fa769aa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:23<00:00,  8.37s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the experiment information to a JSON file\n",
        "with open(f'/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_images_for_models.json', 'w') as f:\n",
        "    json.dump(paths_labeling, f)"
      ],
      "metadata": {
        "id": "-Z4e4GA3o37v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict new labels"
      ],
      "metadata": {
        "id": "3K66q0bu_NGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the json of the labels that need to be predicted\n",
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_images_for_models.json', 'r') as f:\n",
        "    paths_labeling = json.load(f)\n"
      ],
      "metadata": {
        "id": "2qtPuBm_OwUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create new dataset for pseudo-labeling\n",
        "all_predicted_labels = {}\n",
        "for iteration, paths in paths_labeling.items(): \n",
        "    paths_list = batches_to_list(paths)\n",
        "    dataset = ImageDataset(paths_list)    \n",
        "    labeling_loader = DataLoader(dataset, batch_size=1)\n",
        "    model = load_model(iteration, 'cuda')\n",
        "    all_predicted_labels[iteration] = generate_new_labels(model, labeling_loader, 'cuda')  \n",
        "    # 00:33<00:00, 149.59it/s on standard GPU\n",
        "    # [00:18<02:44, 27.31it/s] on standard TPU\n"
      ],
      "metadata": {
        "id": "n4o4h4r5bseB",
        "outputId": "fce9abfa-4ba3-49c8-ccfd-e823bbdd40e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 164.74it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:31<00:00, 157.75it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 163.49it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 163.92it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:31<00:00, 156.30it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:31<00:00, 161.24it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 161.34it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 161.48it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:31<00:00, 160.84it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 162.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "for i in all_predicted_labels.items():\n",
        "    print(i[1]['path'][0])\n",
        "    print(i[1]['confidence'][0])\n",
        "    print(i[1]['label_predicted'][0])\n",
        "    print(i[1]['existing_label'][0])\n",
        "    break"
      ],
      "metadata": {
        "id": "Os7BEdDFiI_0",
        "outputId": "d85f068c-fc7a-427e-e510-51676b1c8d22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('data/task2/images_by_class/3/1383.jpeg',)\n",
            "0.3645797073841095\n",
            "19\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the new predictions and the old predictions in a file for future \n",
        "with open(f'/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_first_iteration.json', 'w') as f:\n",
        "    json.dump(all_predicted_labels, f)"
      ],
      "metadata": {
        "id": "KuNeoaTNhFEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze results pseudo-labeling"
      ],
      "metadata": {
        "id": "Y5GTYn7k4WzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have created new predictions for the labels. But I have only 1 label extra label for each prediction. In order to figure out which labels are correct and which ones are not. I want to use the confidence too. Not only rely on more sessions of pseudo-labeling. Because 1 session costs me about 6 hours anyway. Using the confidence for labeling would be to use a rule similar to these ones: If the labels have the same label and there is a high confidence, I will consider the previous label as valid. If the labels are similar and the confidence is very low, I will try to run this label in another session of relabeling. If the label is different but the confidence is very low, it is probably my mistake. But if the label is different and the confidence is very high. I should probably try to create a new label for that image in another session. "
      ],
      "metadata": {
        "id": "lgF62cD4h2JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_first_iteration.json', 'r') as f:\n",
        "    all_predicted_labels = json.load(f)"
      ],
      "metadata": {
        "id": "fwLsMoYz4pCh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " There are 10 splits of the data, each one has a corresponding key in this dictionary"
      ],
      "metadata": {
        "id": "RmzJre9veAf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_predicted_labels.keys()"
      ],
      "metadata": {
        "id": "AoKd2zR1tXc3",
        "outputId": "22d7d35d-a32d-42a6-961b-1560cceaa957",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the predicted labels and confidence scores\n",
        "iteration = '0'\n",
        "predictions = pd.DataFrame.from_dict(all_predicted_labels[iteration],orient='index').transpose()\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "6zbom3Zmtf_4",
        "outputId": "e8b7be2e-96d8-4c1a-f5f8-dcd998379320",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            path confidence existing_label  \\\n",
            "0       [data/task2/images_by_class/3/1383.jpeg]    0.36458              3   \n",
            "1     [data/task2/images_by_class/76/29313.jpeg]   0.098717             76   \n",
            "2     [data/task2/images_by_class/30/12277.jpeg]   0.203538             30   \n",
            "3     [data/task2/images_by_class/59/22781.jpeg]    0.13695             59   \n",
            "4     [data/task2/images_by_class/88/34072.jpeg]   0.259119             88   \n",
            "...                                          ...        ...            ...   \n",
            "4995  [data/task2/images_by_class/56/21497.jpeg]   0.544975             56   \n",
            "4996  [data/task2/images_by_class/32/12788.jpeg]   0.115707             32   \n",
            "4997   [data/task2/images_by_class/22/8574.jpeg]   0.101662             22   \n",
            "4998  [data/task2/images_by_class/38/14964.jpeg]   0.125019             38   \n",
            "4999  [data/task2/images_by_class/82/48602.jpeg]   0.089067             82   \n",
            "\n",
            "     label_predicted  \n",
            "0                 19  \n",
            "1                 59  \n",
            "2                 34  \n",
            "3                 61  \n",
            "4                 34  \n",
            "...              ...  \n",
            "4995              34  \n",
            "4996              87  \n",
            "4997               7  \n",
            "4998              44  \n",
            "4999              19  \n",
            "\n",
            "[5000 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"How many labels have the same value after relabling ?\")\n",
        "print((predictions['existing_label'] == predictions['label_predicted']).value_counts())"
      ],
      "metadata": {
        "id": "TyeK6Ll-t09m",
        "outputId": "27533cb1-f1ac-4032-a0b1-7b5de250915b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How many labels have the same value after relabling ?\n",
            "False    4939\n",
            "True       61\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"What is the confidence of those ?\")\n",
        "print(predictions[predictions['existing_label'] == predictions['label_predicted']]['confidence'])\n",
        "predictions[predictions['existing_label'] == predictions['label_predicted']]['confidence'].plot.hist(bins = 20)"
      ],
      "metadata": {
        "id": "X-9AbJrHugPA",
        "outputId": "48821f09-ad15-48c2-b28d-d24b31d542aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the confidence of those ?\n",
            "191     1.041596\n",
            "220     0.176706\n",
            "226     0.309039\n",
            "272     1.208265\n",
            "378     1.241412\n",
            "          ...   \n",
            "4771    0.202634\n",
            "4789    0.619976\n",
            "4853    0.182252\n",
            "4931    0.596758\n",
            "4982    0.264925\n",
            "Name: confidence, Length: 61, dtype: object\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f04bec745e0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_same_label = predictions[predictions['existing_label'] == predictions['label_predicted']]\n",
        "predictions_correct_label = predictions_same_label[predictions_same_label['confidence']>0.5]\n",
        "display(predictions_correct_label)"
      ],
      "metadata": {
        "id": "GZqnYaMM58Nf",
        "outputId": "95130f2f-03c2-46c7-bbd7-cc65a22bd2f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                            path confidence existing_label  \\\n",
              "191    [data/task2/images_by_class/19/7468.jpeg]   1.041596             19   \n",
              "272   [data/task2/images_by_class/30/12107.jpeg]   1.208265             30   \n",
              "378   [data/task2/images_by_class/34/13499.jpeg]   1.241412             34   \n",
              "945   [data/task2/images_by_class/92/35180.jpeg]  11.464233             92   \n",
              "1176  [data/task2/images_by_class/63/23960.jpeg]   1.529731             63   \n",
              "1253   [data/task2/images_by_class/11/4422.jpeg]   1.074163             11   \n",
              "1356  [data/task2/images_by_class/34/13544.jpeg]   0.647533             34   \n",
              "1409  [data/task2/images_by_class/17/42317.jpeg]   1.304711             17   \n",
              "1506  [data/task2/images_by_class/38/15092.jpeg]   0.958325             38   \n",
              "1591   [data/task2/images_by_class/19/7451.jpeg]   0.862999             19   \n",
              "1908    [data/task2/images_by_class/7/3188.jpeg]   0.658398              7   \n",
              "2020  [data/task2/images_by_class/34/13546.jpeg]   0.983313             34   \n",
              "2417  [data/task2/images_by_class/34/13435.jpeg]   1.450222             34   \n",
              "2649   [data/task2/images_by_class/22/8810.jpeg]   0.520593             22   \n",
              "2836  [data/task2/images_by_class/34/13624.jpeg]   0.652207             34   \n",
              "3122  [data/task2/images_by_class/34/13490.jpeg]   1.297747             34   \n",
              "3125  [data/task2/images_by_class/21/44836.jpeg]   0.701892             21   \n",
              "3262  [data/task2/images_by_class/44/17250.jpeg]   0.576789             44   \n",
              "3371   [data/task2/images_by_class/19/7640.jpeg]   1.699485             19   \n",
              "3980   [data/task2/images_by_class/11/4320.jpeg]   1.343193             11   \n",
              "4299  [data/task2/images_by_class/34/13596.jpeg]   0.848232             34   \n",
              "4789  [data/task2/images_by_class/34/48202.jpeg]   0.619976             34   \n",
              "4931  [data/task2/images_by_class/34/13542.jpeg]   0.596758             34   \n",
              "\n",
              "     label_predicted  \n",
              "191               19  \n",
              "272               30  \n",
              "378               34  \n",
              "945               92  \n",
              "1176              63  \n",
              "1253              11  \n",
              "1356              34  \n",
              "1409              17  \n",
              "1506              38  \n",
              "1591              19  \n",
              "1908               7  \n",
              "2020              34  \n",
              "2417              34  \n",
              "2649              22  \n",
              "2836              34  \n",
              "3122              34  \n",
              "3125              21  \n",
              "3262              44  \n",
              "3371              19  \n",
              "3980              11  \n",
              "4299              34  \n",
              "4789              34  \n",
              "4931              34  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3975d65-8304-46e5-9042-58ab15e4b130\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>confidence</th>\n",
              "      <th>existing_label</th>\n",
              "      <th>label_predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>[data/task2/images_by_class/19/7468.jpeg]</td>\n",
              "      <td>1.041596</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>[data/task2/images_by_class/30/12107.jpeg]</td>\n",
              "      <td>1.208265</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>[data/task2/images_by_class/34/13499.jpeg]</td>\n",
              "      <td>1.241412</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>945</th>\n",
              "      <td>[data/task2/images_by_class/92/35180.jpeg]</td>\n",
              "      <td>11.464233</td>\n",
              "      <td>92</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1176</th>\n",
              "      <td>[data/task2/images_by_class/63/23960.jpeg]</td>\n",
              "      <td>1.529731</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1253</th>\n",
              "      <td>[data/task2/images_by_class/11/4422.jpeg]</td>\n",
              "      <td>1.074163</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1356</th>\n",
              "      <td>[data/task2/images_by_class/34/13544.jpeg]</td>\n",
              "      <td>0.647533</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1409</th>\n",
              "      <td>[data/task2/images_by_class/17/42317.jpeg]</td>\n",
              "      <td>1.304711</td>\n",
              "      <td>17</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1506</th>\n",
              "      <td>[data/task2/images_by_class/38/15092.jpeg]</td>\n",
              "      <td>0.958325</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1591</th>\n",
              "      <td>[data/task2/images_by_class/19/7451.jpeg]</td>\n",
              "      <td>0.862999</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1908</th>\n",
              "      <td>[data/task2/images_by_class/7/3188.jpeg]</td>\n",
              "      <td>0.658398</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020</th>\n",
              "      <td>[data/task2/images_by_class/34/13546.jpeg]</td>\n",
              "      <td>0.983313</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2417</th>\n",
              "      <td>[data/task2/images_by_class/34/13435.jpeg]</td>\n",
              "      <td>1.450222</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2649</th>\n",
              "      <td>[data/task2/images_by_class/22/8810.jpeg]</td>\n",
              "      <td>0.520593</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2836</th>\n",
              "      <td>[data/task2/images_by_class/34/13624.jpeg]</td>\n",
              "      <td>0.652207</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3122</th>\n",
              "      <td>[data/task2/images_by_class/34/13490.jpeg]</td>\n",
              "      <td>1.297747</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3125</th>\n",
              "      <td>[data/task2/images_by_class/21/44836.jpeg]</td>\n",
              "      <td>0.701892</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3262</th>\n",
              "      <td>[data/task2/images_by_class/44/17250.jpeg]</td>\n",
              "      <td>0.576789</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3371</th>\n",
              "      <td>[data/task2/images_by_class/19/7640.jpeg]</td>\n",
              "      <td>1.699485</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3980</th>\n",
              "      <td>[data/task2/images_by_class/11/4320.jpeg]</td>\n",
              "      <td>1.343193</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4299</th>\n",
              "      <td>[data/task2/images_by_class/34/13596.jpeg]</td>\n",
              "      <td>0.848232</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4789</th>\n",
              "      <td>[data/task2/images_by_class/34/48202.jpeg]</td>\n",
              "      <td>0.619976</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4931</th>\n",
              "      <td>[data/task2/images_by_class/34/13542.jpeg]</td>\n",
              "      <td>0.596758</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3975d65-8304-46e5-9042-58ab15e4b130')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c3975d65-8304-46e5-9042-58ab15e4b130 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c3975d65-8304-46e5-9042-58ab15e4b130');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[predictions['existing_label'] != predictions['label_predicted']]['confidence'].plot.hist(bins = 20)"
      ],
      "metadata": {
        "id": "BFIOtoxFvLjz",
        "outputId": "31b80c1c-bce4-4ad0-f457-dc74e7c95b62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f04beb3ef10>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the predictions have very low score, even if they are right or wrong. \n",
        "But a very small number of images have a high confidence of being correct or incorrect. \n",
        "We would take those images with a very high confidence of being right from each iteration and put them aside as true. Than we will repeat the process of forgetting some images and trying to train on the remaining images. But now we have a little bit better model, since it has some images for which we have a high confidence that they are correct. "
      ],
      "metadata": {
        "id": "lZy2StjD5PgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repeat thresholding pseudo-labels for the following iterations. \n"
      ],
      "metadata": {
        "id": "9R2wBaePAfZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_first_iteration.json', 'r') as f:\n",
        "    all_predicted_labels = json.load(f)\n",
        "# Load the predicted labels and confidence scores\n",
        "correct_labels_df = []\n",
        "for i in range(10):\n",
        "    predictions=pd.DataFrame.from_dict(all_predicted_labels[str(i)],orient='index').transpose() \n",
        "    predictions_same_label = predictions[predictions['existing_label'] == predictions['label_predicted']]\n",
        "    predictions_correct_label = predictions_same_label[predictions_same_label['confidence']>0.5] \n",
        "    correct_labels_df.append(predictions_correct_label)\n",
        "df_correct = pd.concat(correct_labels_df)"
      ],
      "metadata": {
        "id": "O5ghSTjFAkIW"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_correct)"
      ],
      "metadata": {
        "id": "GlOHKGEyCOle",
        "outputId": "f1eee80c-e764-44ec-9a3b-48a2670b8453",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "431"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: After the first iteration of the algorithm we found 431 images to have correct labels with high confidence. This means we should repeat this process 100 times. Which would take a very large amount of time. But hopefully the futher iterations will become faster as we find that more images are correctly labeled. \n",
        "\n",
        "NOTE: Also, because we still have the previous results from the first iteration. If we get the same labels on the second or third iteration, we are much more likely to believe that the respective labels are true, and we can include them in the \"pseudo ground truth dataset\" even if they don't have a high confidence, but they have the same label on multiple runs. \n",
        "\n",
        "NOTE: Finally. In the end we can try again the whole experiment from the beginning. The expectation would be that we would find much fewer wrongly labeled images. "
      ],
      "metadata": {
        "id": "Wvwdmh-BC2_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iteration 2\n",
        "considering previously found labels as true, and not reconsidering to label them anymore. "
      ],
      "metadata": {
        "id": "q_emV0k5Dv9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_dir = 'data/task2/images_by_class'\n",
        "\n",
        "if os.path.exists(target_dir):\n",
        "    # Use rmtree to delete the directory and all its contents\n",
        "    shutil.rmtree(target_dir)\n",
        "    print(f'{target_dir} has been deleted')\n",
        "else:\n",
        "    print(f'{target_dir} does not exist')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYWytvbjLeJi",
        "outputId": "f149e39e-34c8-4ed5-b1b6-e9df5164da2f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/task2/images_by_class does not exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the directory exists, to recreate it instead of messing it up\n",
        "target_dir = 'data/task2/training_confident'\n",
        "\n",
        "if os.path.exists(target_dir):\n",
        "    # Use rmtree to delete the directory and all its contents\n",
        "    shutil.rmtree(target_dir)\n",
        "    print(f'{target_dir} has been deleted') \n",
        "else:\n",
        "    print(f'{target_dir} does not exist')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNK5li4VMQae",
        "outputId": "c6c1f104-4f1a-4302-fc86-71c21db5abee"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/task2/training_confident does not exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_dir = 'data/task2/images_by_class'\n",
        "dir_data = 'data/task2/train_data/'\n",
        "# Read the annotations file into a DataFrame\n",
        "df = pd.read_csv(f'{dir_data}annotations.csv')\n",
        "\n",
        "# Iterate over the rows in the DataFrame\n",
        "for _, row in tqdm(df.iterrows(), total= df.shape[0]):\n",
        "    # Extract the path and class from the row\n",
        "    path = row['renamed_path']\n",
        "    label = row['label_idx']\n",
        "    \n",
        "    # Create the directory for the class\n",
        "    class_dir = f'{target_dir}/{label}'\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    \n",
        "    # Copy the file to the class directory\n",
        "    shutil.copy(f\"data/{path}\", class_dir) \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ri4BEas4HE8u",
        "outputId": "cd7303fb-5761-4c60-d165-c70757317159"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:11<00:00, 4519.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on 2 datasets at the same time, one with high confidence labels\n",
        "# this dataset will only be used during training, never forgetting labels\n",
        "# the second dataset will iteratively forget labels and train on the remaining labels.\n",
        "# Define the base directory\n",
        "target_dir = 'data/task2/training_confident'\n",
        "# Iterate over the rows in the DataFrame\n",
        "for _, row in tqdm(df_correct.iterrows(), total=df_correct.shape[0]):\n",
        "    # Extract the path and class from the row\n",
        "    path = row['path'][0]\n",
        "    label = row['existing_label'] \n",
        "\n",
        "    # Create the directory for the class\n",
        "    class_dir = f'{target_dir}/{label}'\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    \n",
        "    # move the file to the class directory\n",
        "    shutil.move(f\"{path}\", class_dir)\n",
        "    \n",
        "\n",
        "# we move the images because we don't want them to remain in the dataset for relabeling.  \n"
      ],
      "metadata": {
        "id": "ZKxIxGUz5a8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4508c7a-e283-4a50-fa8a-52f148cab6af"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 431/431 [00:00<00:00, 7773.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: create the 2 datasets\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(experiment_info[\"image_processing\"][\"resize\"]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=experiment_info[\"image_processing\"][\"mean\"],\n",
        "                         std=experiment_info[\"image_processing\"][\"std\"]),\n",
        "    ])\n",
        "\n",
        "data_dir_train = 'data/task2/training_confident'\n",
        "image_dataset_train_only = datasets.ImageFolder(data_dir_train, preprocess) \n",
        "dataloader_train_only  = DataLoader(\n",
        "    image_dataset_train_only, \n",
        "    batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"],\n",
        "    shuffle = experiment_info[\"hyperparameters_data\"][\"shuffle_dataloader\"], \n",
        "    num_workers = experiment_info[\"hyperparameters_data\"][\"num_workers\"]\n",
        "    )\n",
        "\n",
        "class_names = image_dataset_train_only.classes\n",
        "dataset_size = len(image_dataset_train_only)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "K7KktFggEt_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd30b0fe-b32f-4c46-b37c-f848881e9ce2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i8BvAlyH110",
        "outputId": "df09c0ec-5d9e-4621-b525-1073b3e4d16a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create datasets splits for pseudo-labeling round 2"
      ],
      "metadata": {
        "id": "iG8vZn7mFmRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'data/task2/images_by_class'\n",
        "image_dataset_unshuffled = datasets.ImageFolder(data_dir, preprocess)\n",
        "image_dataset = ShuffledImageFolder(image_dataset_unshuffled)"
      ],
      "metadata": {
        "id": "XfnzLXF_FqMa"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = len(image_dataset)\n",
        "\n",
        "# Split the image_dataset into 10 parts\n",
        "splits = 2\n",
        "part_size = num_samples // splits\n",
        "parts = [list(range(i * part_size, (i + 1) * part_size)) for i in range(splits)]\n",
        "\n",
        "batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"]\n",
        "# Use one part for creating new labels and the other 9 parts for training the model\n",
        "loaders2 = {}\n",
        "for i in range(splits):\n",
        "    # Get the indices for the training set, which are not in the labeling set\n",
        "    train_indices = [index for j, part in enumerate(parts) if j != i for index in part]\n",
        "    labeling_indices = parts[i]\n",
        "\n",
        "    # Create a sampler for the training set and the labeling set\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    labeling_sampler = SubsetRandomSampler(labeling_indices)\n",
        "\n",
        "    # Use the samplers to create data loaders2 for the training set and the labeling set\n",
        "    train_loader = DataLoader(image_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    labeling_loader = DataLoader(image_dataset, batch_size=batch_size, sampler=labeling_sampler)\n",
        "    loaders2[i] = {\n",
        "        \"train\":train_loader, \n",
        "        \"labeling\":labeling_loader\n",
        "    }\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data_sizes = {\n",
        "    \"train\" : len(loaders2[0][\"train\"]), # this is the number of batches not images\n",
        "    \"labeling\" : len(loaders2[0][\"labeling\"]),\n",
        "}"
      ],
      "metadata": {
        "id": "gEMNSHMBFtLa"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_sizes)\n",
        "print(\"num samples train\", data_sizes['train']*batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a95SXdQCH8GL",
        "outputId": "eb79f48c-f2ea-4100-e828-1b8059bf5820"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': 775, 'labeling': 775}\n",
            "num samples train 24800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on remaining data"
      ],
      "metadata": {
        "id": "vohgejEvNNLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a pytorch MobileNet pretrained model\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "# change the Linear output to fit our dataset ( because model initially has 1000 classes)\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "# Setting hyperparameters\n",
        "\n",
        "model = model.to(device)\n",
        "print(model.classifier)\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "0b6b0668f3764a7296945547803e2688",
            "c39c58da42ce4c2ca76546673443f0fd",
            "b02ccf11c1714786a67bfca3446e9b9d",
            "284de89238b54efd8d834cfc5cdef224",
            "4d1e4cd757a749bda0f6fbcede536076",
            "dd637c7b9a5645dfb198f017092d098a",
            "afee6006e2324bac8bbc9c291e638617",
            "8e3263acdec647e594b2140e749bd110",
            "4dd784f55bb14f6ba9274ec956ec8061",
            "7adf26ae5d2e430e946d5576135d893a",
            "69db9e55793a4d87bd7b0c96efc53170"
          ]
        },
        "id": "bk_xDk2MNPk7",
        "outputId": "04ce156f-2d44-4405-bc1e-b2d78fea20ce"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b6b0668f3764a7296945547803e2688"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=False)\n",
            "  (1): Linear(in_features=1280, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=experiment_info[\"hyperparameters_training\"][\"learning_rate\"],\n",
        "    )\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(\n",
        "    optimizer, \n",
        "    step_size=experiment_info[\"hyperparameters_training\"][\"scheduler_step_size\"], \n",
        "    gamma=experiment_info[\"hyperparameters_training\"][\"scheduler_gamma\"],\n",
        "    )"
      ],
      "metadata": {
        "id": "LRzyP-FuNhhh"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model2(model, \n",
        "                    exp_lr_scheduler, \n",
        "                    optimizer, \n",
        "                    criterion,  \n",
        "                    data_sizes[\"train\"], \n",
        "                    dataloader_train_only,\n",
        "                    loaders2[0][\"train\"], \n",
        "                    num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmT0N9O9NUIX",
        "outputId": "1fa0bc4c-7baf-4cee-c213-846f55d6dcf5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:09<00:00,  1.42it/s]\n",
            "100%|██████████| 775/775 [02:21<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 86.6548 Acc: 11.1110\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.00it/s]\n",
            "100%|██████████| 775/775 [02:20<00:00,  5.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 51.9183 Acc: 17.8194\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  4.72it/s]\n",
            "100%|██████████| 775/775 [02:23<00:00,  5.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 39.8597 Acc: 20.5716\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.05it/s]\n",
            "100%|██████████| 775/775 [02:18<00:00,  5.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 31.2823 Acc: 22.5729\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.90it/s]\n",
            "100%|██████████| 775/775 [02:15<00:00,  5.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 24.2291 Acc: 24.5897\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.96it/s]\n",
            "100%|██████████| 775/775 [02:16<00:00,  5.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 18.5088 Acc: 26.2800\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.97it/s]\n",
            "100%|██████████| 775/775 [02:15<00:00,  5.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 13.6184 Acc: 27.8787\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.91it/s]\n",
            "100%|██████████| 775/775 [02:15<00:00,  5.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 7.1740 Acc: 30.4065\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.04it/s]\n",
            "100%|██████████| 775/775 [02:17<00:00,  5.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 5.7742 Acc: 30.9032\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.01it/s]\n",
            "100%|██████████| 775/775 [02:15<00:00,  5.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 5.1758 Acc: 31.1458\n",
            "\n",
            "Training complete in 23m 33s\n",
            "Best val Acc: 31.145806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), '/gdrive/MyDrive/checkpoints/noisy_labels/model_2_it_0.pt')"
      ],
      "metadata": {
        "id": "1XhOuuNyNvtU"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Repeat for the other half"
      ],
      "metadata": {
        "id": "3S_tI-1ZNstT"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "model = model.to(device) \n",
        "model = train_model2(model, \n",
        "                exp_lr_scheduler, \n",
        "                optimizer, \n",
        "                criterion,  \n",
        "                data_sizes[\"train\"], \n",
        "                dataloader_train_only,\n",
        "                loaders2[1][\"train\"], \n",
        "                num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])\n",
        "torch.save(model.state_dict(), f'/gdrive/MyDrive/checkpoints/noisy_labels/model_2_it_{1}.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wMaIPVTN-KN",
        "outputId": "d388f3b4-f6e4-4f21-e092-7f0f79e9af11"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.00it/s]\n",
            "100%|██████████| 775/775 [02:15<00:00,  5.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 151.0671 Acc: 0.2813\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.14it/s]\n",
            "100%|██████████| 775/775 [02:16<00:00,  5.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 151.0593 Acc: 0.2606\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.15it/s]\n",
            "100%|██████████| 775/775 [02:17<00:00,  5.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 151.0380 Acc: 0.2606\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.04it/s]\n",
            "100%|██████████| 775/775 [02:26<00:00,  5.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.9949 Acc: 0.2839\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  4.90it/s]\n",
            "100%|██████████| 775/775 [02:16<00:00,  5.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 151.0060 Acc: 0.2813\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.79it/s]\n",
            "100%|██████████| 775/775 [02:23<00:00,  5.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 151.0038 Acc: 0.2826\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.73it/s]\n",
            "100%|██████████| 775/775 [02:20<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 150.9881 Acc: 0.2710\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.03it/s]\n",
            "100%|██████████| 775/775 [02:16<00:00,  5.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 151.0254 Acc: 0.2594\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.00it/s]\n",
            "100%|██████████| 775/775 [02:16<00:00,  5.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 151.0795 Acc: 0.2619\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.96it/s]\n",
            "100%|██████████| 775/775 [02:20<00:00,  5.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 151.0896 Acc: 0.2865\n",
            "\n",
            "Training complete in 23m 34s\n",
            "Best val Acc: 0.286452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save labels for later predicting"
      ],
      "metadata": {
        "id": "4r4x6giiTYqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paths_labeling = {}\n",
        "for i in range(splits):\n",
        "    paths_labeling[i] = []\n",
        "    for images, labels, paths in tqdm(loaders2[i]['labeling']):\n",
        "        paths_labeling[i].append(paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwmcZrQyWISg",
        "outputId": "d6b882c5-ae21-4f56-fb2f-b60589d24e79"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 775/775 [00:39<00:00, 19.54it/s]\n",
            "100%|██████████| 775/775 [00:38<00:00, 19.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the experiment information to a JSON file\n",
        "with open(f'/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_images_for_models_2.json', 'w') as f:\n",
        "    json.dump(paths_labeling, f)"
      ],
      "metadata": {
        "id": "mi2U16qVWKJY"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict new labels 2"
      ],
      "metadata": {
        "id": "_9TaBnOkWX-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the json of the labels that need to be predicted\n",
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_images_for_models_2.json', 'r') as f:\n",
        "    paths_labeling = json.load(f)"
      ],
      "metadata": {
        "id": "cM3GCPEaWZ0V"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create new dataset for pseudo-labeling\n",
        "all_predicted_labels = {}\n",
        "for iteration, paths in paths_labeling.items(): \n",
        "    paths_list = batches_to_list(paths)\n",
        "    dataset = ImageDataset(paths_list)    \n",
        "    labeling_loader = DataLoader(dataset, batch_size=1)\n",
        "    model = load_model(iteration, 'cuda')\n",
        "    all_predicted_labels[iteration] = generate_new_labels(model, labeling_loader, 'cuda')   "
      ],
      "metadata": {
        "id": "FfqmTKpzdn3n",
        "outputId": "882668ad-7545-44f0-b650-0046ec31e8e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 24784/24784 [02:38<00:00, 156.00it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 24784/24784 [02:36<00:00, 158.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "for i in all_predicted_labels.items():\n",
        "    print(i[1]['path'][0])\n",
        "    print(i[1]['confidence'][0])\n",
        "    print(i[1]['label_predicted'][0])\n",
        "    print(i[1]['existing_label'][0])\n",
        "    break"
      ],
      "metadata": {
        "id": "ljiMdeEFdsM1",
        "outputId": "261b7fee-a520-4786-9560-82219135b83f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('data/task2/images_by_class/81/31055.jpeg',)\n",
            "0.038609541952610016\n",
            "85\n",
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the new predictions and the old predictions in a file for future \n",
        "with open(f'/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_second_iteration.json', 'w') as f:\n",
        "    json.dump(all_predicted_labels, f)"
      ],
      "metadata": {
        "id": "EakLd6JddzKw"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze pseudo lableing from second iteration"
      ],
      "metadata": {
        "id": "wm7WbpH7d2Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_second_iteration.json', 'r') as f:\n",
        "    all_predicted_labels = json.load(f)"
      ],
      "metadata": {
        "id": "uny5rmNHd2AF"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_predicted_labels.keys()"
      ],
      "metadata": {
        "id": "JDDWpwUGd-0b",
        "outputId": "937323a5-1003-4f46-d7c6-715730e51c65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['0', '1'])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the predicted labels and confidence scores\n",
        "iteration = '0'\n",
        "predictions = pd.DataFrame.from_dict(all_predicted_labels[iteration],orient='index').transpose()\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "F8dYkXFHePtL",
        "outputId": "03cce4e5-ad9c-42e4-cfe6-62e8c0e3ef86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             path confidence existing_label  \\\n",
            "0      [data/task2/images_by_class/81/31055.jpeg]    0.03861             81   \n",
            "1      [data/task2/images_by_class/86/33112.jpeg]   0.122728             86   \n",
            "2      [data/task2/images_by_class/42/43263.jpeg]   0.151605             42   \n",
            "3      [data/task2/images_by_class/80/30852.jpeg]   0.188782             80   \n",
            "4      [data/task2/images_by_class/94/36087.jpeg]   0.088798             94   \n",
            "...                                           ...        ...            ...   \n",
            "24779  [data/task2/images_by_class/27/10476.jpeg]    0.05421             27   \n",
            "24780  [data/task2/images_by_class/69/26650.jpeg]   0.104292             69   \n",
            "24781  [data/task2/images_by_class/67/25549.jpeg]   0.182475             67   \n",
            "24782   [data/task2/images_by_class/14/5501.jpeg]   0.504089             14   \n",
            "24783    [data/task2/images_by_class/5/2113.jpeg]   2.366076              5   \n",
            "\n",
            "      label_predicted  \n",
            "0                  85  \n",
            "1                  40  \n",
            "2                  44  \n",
            "3                  38  \n",
            "4                  61  \n",
            "...               ...  \n",
            "24779              61  \n",
            "24780              34  \n",
            "24781              56  \n",
            "24782               7  \n",
            "24783              19  \n",
            "\n",
            "[24784 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"How many labels have the same value after relabling ?\")\n",
        "print((predictions['existing_label'] == predictions['label_predicted']).value_counts())"
      ],
      "metadata": {
        "id": "06u9mblBeRUt",
        "outputId": "c2e5454f-ead7-42ed-bc8a-305ccaeb2f07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How many labels have the same value after relabling ?\n",
            "False    24460\n",
            "True       324\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"What is the confidence of those ?\")\n",
        "print(predictions[predictions['existing_label'] == predictions['label_predicted']]['confidence'])\n",
        "predictions[predictions['existing_label'] == predictions['label_predicted']]['confidence'].plot.hist(bins = 20)"
      ],
      "metadata": {
        "id": "EbY2OOOceTpg",
        "outputId": "b45def99-b03a-48e9-c353-65203b335163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the confidence of those ?\n",
            "7        0.891132\n",
            "195       0.28196\n",
            "209      1.789522\n",
            "320      0.079717\n",
            "415      0.094428\n",
            "           ...   \n",
            "24496    0.232504\n",
            "24504     0.54723\n",
            "24578    0.111499\n",
            "24602    0.193937\n",
            "24677    0.453228\n",
            "Name: confidence, Length: 324, dtype: object\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f04b697b8b0>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_same_label = predictions[predictions['existing_label'] == predictions['label_predicted']]\n",
        "predictions_correct_label = predictions_same_label[predictions_same_label['confidence']>0.5]\n",
        "display(predictions_correct_label)"
      ],
      "metadata": {
        "id": "uV0JhS1teWQW",
        "outputId": "755e4d67-c9cd-47ba-9dab-0b2eff67e7c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                             path confidence existing_label  \\\n",
              "7         [data/task2/images_by_class/1/607.jpeg]   0.891132              1   \n",
              "209    [data/task2/images_by_class/34/13389.jpeg]   1.789522             34   \n",
              "560    [data/task2/images_by_class/34/13392.jpeg]   0.721701             34   \n",
              "684     [data/task2/images_by_class/19/7674.jpeg]   1.754833             19   \n",
              "1242   [data/task2/images_by_class/34/13629.jpeg]   0.866545             34   \n",
              "...                                           ...        ...            ...   \n",
              "23578  [data/task2/images_by_class/34/13413.jpeg]   0.796553             34   \n",
              "23683  [data/task2/images_by_class/38/15165.jpeg]   0.571368             38   \n",
              "24351   [data/task2/images_by_class/19/7350.jpeg]   5.060008             19   \n",
              "24458  [data/task2/images_by_class/34/13401.jpeg]   0.548119             34   \n",
              "24504  [data/task2/images_by_class/34/13642.jpeg]    0.54723             34   \n",
              "\n",
              "      label_predicted  \n",
              "7                   1  \n",
              "209                34  \n",
              "560                34  \n",
              "684                19  \n",
              "1242               34  \n",
              "...               ...  \n",
              "23578              34  \n",
              "23683              38  \n",
              "24351              19  \n",
              "24458              34  \n",
              "24504              34  \n",
              "\n",
              "[102 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f2415f14-78cc-4e3e-bee5-4af4ec8f5898\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>confidence</th>\n",
              "      <th>existing_label</th>\n",
              "      <th>label_predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[data/task2/images_by_class/1/607.jpeg]</td>\n",
              "      <td>0.891132</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>[data/task2/images_by_class/34/13389.jpeg]</td>\n",
              "      <td>1.789522</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>560</th>\n",
              "      <td>[data/task2/images_by_class/34/13392.jpeg]</td>\n",
              "      <td>0.721701</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684</th>\n",
              "      <td>[data/task2/images_by_class/19/7674.jpeg]</td>\n",
              "      <td>1.754833</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1242</th>\n",
              "      <td>[data/task2/images_by_class/34/13629.jpeg]</td>\n",
              "      <td>0.866545</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23578</th>\n",
              "      <td>[data/task2/images_by_class/34/13413.jpeg]</td>\n",
              "      <td>0.796553</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23683</th>\n",
              "      <td>[data/task2/images_by_class/38/15165.jpeg]</td>\n",
              "      <td>0.571368</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24351</th>\n",
              "      <td>[data/task2/images_by_class/19/7350.jpeg]</td>\n",
              "      <td>5.060008</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24458</th>\n",
              "      <td>[data/task2/images_by_class/34/13401.jpeg]</td>\n",
              "      <td>0.548119</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24504</th>\n",
              "      <td>[data/task2/images_by_class/34/13642.jpeg]</td>\n",
              "      <td>0.54723</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>102 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2415f14-78cc-4e3e-bee5-4af4ec8f5898')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f2415f14-78cc-4e3e-bee5-4af4ec8f5898 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f2415f14-78cc-4e3e-bee5-4af4ec8f5898');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"the distribution of labels different from one dataset to another\")\n",
        "predictions[predictions['existing_label'] != predictions['label_predicted']]['confidence'].plot.hist(bins = 20)"
      ],
      "metadata": {
        "id": "iLJx33B8eYYg",
        "outputId": "782a576a-a3f2-45a3-b327-4427ad850d60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f047bf95880>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repeat thresholding pseudo-labels for the following iterations."
      ],
      "metadata": {
        "id": "xHLNQeikefi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second iteration has a different process for analyzing the labels, we are not only interested if the new labels have the the same label as the one provided by the dataset. We are also interested to see if the first iteration gives labels equal to the second iteration. And also how high is the confidence, even if the labels are \"wrong\" ( different from dataset), but for which both models provide the same label"
      ],
      "metadata": {
        "id": "Veirr9GWgKm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_second_iteration.json', 'r') as f:\n",
        "    all_predicted_labels2 = json.load(f)\n",
        "# Load the predicted labels and confidence scores\n",
        "correct_labels_df = []\n",
        "for i in range(splits):\n",
        "    predictions=pd.DataFrame.from_dict(all_predicted_labels2[str(i)],orient='index').transpose() \n",
        "    predictions_same_label = predictions[predictions['existing_label'] == predictions['label_predicted']]\n",
        "    predictions_correct_label = predictions_same_label[predictions_same_label['confidence']>0.5] \n",
        "    correct_labels_df.append(predictions_correct_label)\n",
        "df_correct2 = pd.concat(correct_labels_df)\n",
        "df_correct2['path'] = df_correct2['path'].apply(lambda x: x[0])"
      ],
      "metadata": {
        "id": "kC2CIK45ekNA"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_correct2)"
      ],
      "metadata": {
        "id": "wmNsoNo2eo6E",
        "outputId": "faef7944-9163-4eb8-b3ee-11b1fd034fd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "331"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_first_iteration.json', 'r') as f:\n",
        "    all_predicted_labels1 = json.load(f)\n",
        "# Load the predicted labels and confidence scores\n",
        "correct_labels_df_1 = []\n",
        "for i in range(10):\n",
        "    predictions=pd.DataFrame.from_dict(all_predicted_labels1[str(i)],orient='index').transpose() \n",
        "    predictions_same_label = predictions[predictions['existing_label'] == predictions['label_predicted']]\n",
        "    predictions_correct_label = predictions_same_label[predictions_same_label['confidence']>0.5] \n",
        "    correct_labels_df_1.append(predictions_correct_label)\n",
        "\n",
        "df_correct_1 = pd.concat(correct_labels_df_1)\n",
        "df_correct_1['path'] = df_correct_1['path'].apply(lambda x: x[0])"
      ],
      "metadata": {
        "id": "3em0OOnpjFds"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_correct_1"
      ],
      "metadata": {
        "id": "x-yiskagje-i",
        "outputId": "a482032c-ba01-492f-a71d-95ddf0aee543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            path confidence existing_label  \\\n",
              "191    [data/task2/images_by_class/19/7468.jpeg]   1.041596             19   \n",
              "272   [data/task2/images_by_class/30/12107.jpeg]   1.208265             30   \n",
              "378   [data/task2/images_by_class/34/13499.jpeg]   1.241412             34   \n",
              "945   [data/task2/images_by_class/92/35180.jpeg]  11.464233             92   \n",
              "1176  [data/task2/images_by_class/63/23960.jpeg]   1.529731             63   \n",
              "...                                          ...        ...            ...   \n",
              "4310  [data/task2/images_by_class/50/19476.jpeg]   1.982109             50   \n",
              "4365  [data/task2/images_by_class/52/19993.jpeg]   2.803925             52   \n",
              "4545  [data/task2/images_by_class/38/48931.jpeg]    3.05493             38   \n",
              "4587  [data/task2/images_by_class/38/15138.jpeg]   1.706635             38   \n",
              "4938  [data/task2/images_by_class/93/35654.jpeg]   2.009159             93   \n",
              "\n",
              "     label_predicted  \n",
              "191               19  \n",
              "272               30  \n",
              "378               34  \n",
              "945               92  \n",
              "1176              63  \n",
              "...              ...  \n",
              "4310              50  \n",
              "4365              52  \n",
              "4545              38  \n",
              "4587              38  \n",
              "4938              93  \n",
              "\n",
              "[431 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-78c10859-38a2-470d-b907-1f3c322e8f3f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>confidence</th>\n",
              "      <th>existing_label</th>\n",
              "      <th>label_predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>[data/task2/images_by_class/19/7468.jpeg]</td>\n",
              "      <td>1.041596</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>[data/task2/images_by_class/30/12107.jpeg]</td>\n",
              "      <td>1.208265</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>[data/task2/images_by_class/34/13499.jpeg]</td>\n",
              "      <td>1.241412</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>945</th>\n",
              "      <td>[data/task2/images_by_class/92/35180.jpeg]</td>\n",
              "      <td>11.464233</td>\n",
              "      <td>92</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1176</th>\n",
              "      <td>[data/task2/images_by_class/63/23960.jpeg]</td>\n",
              "      <td>1.529731</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4310</th>\n",
              "      <td>[data/task2/images_by_class/50/19476.jpeg]</td>\n",
              "      <td>1.982109</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4365</th>\n",
              "      <td>[data/task2/images_by_class/52/19993.jpeg]</td>\n",
              "      <td>2.803925</td>\n",
              "      <td>52</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4545</th>\n",
              "      <td>[data/task2/images_by_class/38/48931.jpeg]</td>\n",
              "      <td>3.05493</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4587</th>\n",
              "      <td>[data/task2/images_by_class/38/15138.jpeg]</td>\n",
              "      <td>1.706635</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4938</th>\n",
              "      <td>[data/task2/images_by_class/93/35654.jpeg]</td>\n",
              "      <td>2.009159</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>431 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78c10859-38a2-470d-b907-1f3c322e8f3f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-78c10859-38a2-470d-b907-1f3c322e8f3f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-78c10859-38a2-470d-b907-1f3c322e8f3f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_correct2"
      ],
      "metadata": {
        "id": "s5LJ1aX1jhRZ",
        "outputId": "b9bb16a1-b47b-4d6b-aeff-2759be1cf65e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             path confidence existing_label  \\\n",
              "7         [data/task2/images_by_class/1/607.jpeg]   0.891132              1   \n",
              "209    [data/task2/images_by_class/34/13389.jpeg]   1.789522             34   \n",
              "560    [data/task2/images_by_class/34/13392.jpeg]   0.721701             34   \n",
              "684     [data/task2/images_by_class/19/7674.jpeg]   1.754833             19   \n",
              "1242   [data/task2/images_by_class/34/13629.jpeg]   0.866545             34   \n",
              "...                                           ...        ...            ...   \n",
              "23971  [data/task2/images_by_class/99/37829.jpeg]   3.654842             99   \n",
              "24292  [data/task2/images_by_class/99/38055.jpeg]   1.987471             99   \n",
              "24424  [data/task2/images_by_class/99/37854.jpeg]   1.884956             99   \n",
              "24585  [data/task2/images_by_class/91/35101.jpeg]   1.706358             91   \n",
              "24623  [data/task2/images_by_class/84/32243.jpeg]   2.178458             84   \n",
              "\n",
              "      label_predicted  \n",
              "7                   1  \n",
              "209                34  \n",
              "560                34  \n",
              "684                19  \n",
              "1242               34  \n",
              "...               ...  \n",
              "23971              99  \n",
              "24292              99  \n",
              "24424              99  \n",
              "24585              91  \n",
              "24623              84  \n",
              "\n",
              "[331 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2bc33eb1-373f-4645-be40-907c1e8488f5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>confidence</th>\n",
              "      <th>existing_label</th>\n",
              "      <th>label_predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[data/task2/images_by_class/1/607.jpeg]</td>\n",
              "      <td>0.891132</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>[data/task2/images_by_class/34/13389.jpeg]</td>\n",
              "      <td>1.789522</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>560</th>\n",
              "      <td>[data/task2/images_by_class/34/13392.jpeg]</td>\n",
              "      <td>0.721701</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684</th>\n",
              "      <td>[data/task2/images_by_class/19/7674.jpeg]</td>\n",
              "      <td>1.754833</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1242</th>\n",
              "      <td>[data/task2/images_by_class/34/13629.jpeg]</td>\n",
              "      <td>0.866545</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23971</th>\n",
              "      <td>[data/task2/images_by_class/99/37829.jpeg]</td>\n",
              "      <td>3.654842</td>\n",
              "      <td>99</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24292</th>\n",
              "      <td>[data/task2/images_by_class/99/38055.jpeg]</td>\n",
              "      <td>1.987471</td>\n",
              "      <td>99</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24424</th>\n",
              "      <td>[data/task2/images_by_class/99/37854.jpeg]</td>\n",
              "      <td>1.884956</td>\n",
              "      <td>99</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24585</th>\n",
              "      <td>[data/task2/images_by_class/91/35101.jpeg]</td>\n",
              "      <td>1.706358</td>\n",
              "      <td>91</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24623</th>\n",
              "      <td>[data/task2/images_by_class/84/32243.jpeg]</td>\n",
              "      <td>2.178458</td>\n",
              "      <td>84</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>331 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2bc33eb1-373f-4645-be40-907c1e8488f5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2bc33eb1-373f-4645-be40-907c1e8488f5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2bc33eb1-373f-4645-be40-907c1e8488f5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = []\n",
        "for i in range(10):\n",
        "    predictions1=pd.DataFrame.from_dict(all_predicted_labels1[str(i)],orient='index').transpose() \n",
        "    df1.append(predictions1)\n",
        "\n",
        "df_all1 = pd.concat(df1)\n",
        "\n",
        "df2 = []\n",
        "for i in range(2):\n",
        "    predictions1=pd.DataFrame.from_dict(all_predicted_labels2[str(i)],orient='index').transpose() \n",
        "    df2.append(predictions1)\n",
        "df_all2 = pd.concat(df2)"
      ],
      "metadata": {
        "id": "YId3WwTlkXvf"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_all1))\n",
        "print(len(df_all2))"
      ],
      "metadata": {
        "id": "t7cf4f6_lCkt",
        "outputId": "0760dc6a-877f-420a-a490-90e90980e451",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n",
            "49568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all1['path'] = df_all1['path'].apply(lambda x: x[0])\n",
        "df_all2['path'] = df_all2['path'].apply(lambda x: x[0])"
      ],
      "metadata": {
        "id": "4Drmf-I7lF02"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all2.rename({\"confidence\":\"confidence2\", \"label_predicted\":\"label_predicted2\"}, inplace=True, axis = 1)"
      ],
      "metadata": {
        "id": "elypq76GlJlq"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all2.drop(\"existing_label\", inplace=True,axis=1)"
      ],
      "metadata": {
        "id": "I2fPv9mflacx"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_df = pd.merge(df_all1, df_all2, on=\"path\", how=\"left\")\n",
        "# we don't have all rows for confidence 2 so we use \"left\" merging"
      ],
      "metadata": {
        "id": "OoWG-96cltAM"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_df"
      ],
      "metadata": {
        "id": "82ZFbAYTnzVj",
        "outputId": "be983c2d-e092-4131-902f-3f55219b84ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           path confidence existing_label  \\\n",
              "0        data/task2/images_by_class/3/1383.jpeg    0.36458              3   \n",
              "1      data/task2/images_by_class/76/29313.jpeg   0.098717             76   \n",
              "2      data/task2/images_by_class/30/12277.jpeg   0.203538             30   \n",
              "3      data/task2/images_by_class/59/22781.jpeg    0.13695             59   \n",
              "4      data/task2/images_by_class/88/34072.jpeg   0.259119             88   \n",
              "...                                         ...        ...            ...   \n",
              "49995  data/task2/images_by_class/74/28286.jpeg   1.359533             74   \n",
              "49996  data/task2/images_by_class/15/45448.jpeg   3.355858             15   \n",
              "49997   data/task2/images_by_class/8/44196.jpeg   1.073675              8   \n",
              "49998  data/task2/images_by_class/75/28828.jpeg   1.837749             75   \n",
              "49999  data/task2/images_by_class/99/40549.jpeg    3.33866             99   \n",
              "\n",
              "      label_predicted confidence2 label_predicted2  \n",
              "0                  19     0.36458               19  \n",
              "1                  59    1.701364               99  \n",
              "2                  34    0.203538               34  \n",
              "3                  61    2.192473               99  \n",
              "4                  34    1.853496               99  \n",
              "...               ...         ...              ...  \n",
              "49995              50    0.081949               85  \n",
              "49996              61    2.509316               99  \n",
              "49997               0    0.157974               34  \n",
              "49998              39    1.737912               99  \n",
              "49999              67     0.72167               44  \n",
              "\n",
              "[50000 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-89cf3a39-da67-44a8-b686-33324a8f9fe4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>confidence</th>\n",
              "      <th>existing_label</th>\n",
              "      <th>label_predicted</th>\n",
              "      <th>confidence2</th>\n",
              "      <th>label_predicted2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/task2/images_by_class/3/1383.jpeg</td>\n",
              "      <td>0.36458</td>\n",
              "      <td>3</td>\n",
              "      <td>19</td>\n",
              "      <td>0.36458</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/task2/images_by_class/76/29313.jpeg</td>\n",
              "      <td>0.098717</td>\n",
              "      <td>76</td>\n",
              "      <td>59</td>\n",
              "      <td>1.701364</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/task2/images_by_class/30/12277.jpeg</td>\n",
              "      <td>0.203538</td>\n",
              "      <td>30</td>\n",
              "      <td>34</td>\n",
              "      <td>0.203538</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/task2/images_by_class/59/22781.jpeg</td>\n",
              "      <td>0.13695</td>\n",
              "      <td>59</td>\n",
              "      <td>61</td>\n",
              "      <td>2.192473</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/task2/images_by_class/88/34072.jpeg</td>\n",
              "      <td>0.259119</td>\n",
              "      <td>88</td>\n",
              "      <td>34</td>\n",
              "      <td>1.853496</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>data/task2/images_by_class/74/28286.jpeg</td>\n",
              "      <td>1.359533</td>\n",
              "      <td>74</td>\n",
              "      <td>50</td>\n",
              "      <td>0.081949</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>data/task2/images_by_class/15/45448.jpeg</td>\n",
              "      <td>3.355858</td>\n",
              "      <td>15</td>\n",
              "      <td>61</td>\n",
              "      <td>2.509316</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>data/task2/images_by_class/8/44196.jpeg</td>\n",
              "      <td>1.073675</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0.157974</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>data/task2/images_by_class/75/28828.jpeg</td>\n",
              "      <td>1.837749</td>\n",
              "      <td>75</td>\n",
              "      <td>39</td>\n",
              "      <td>1.737912</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>data/task2/images_by_class/99/40549.jpeg</td>\n",
              "      <td>3.33866</td>\n",
              "      <td>99</td>\n",
              "      <td>67</td>\n",
              "      <td>0.72167</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89cf3a39-da67-44a8-b686-33324a8f9fe4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-89cf3a39-da67-44a8-b686-33324a8f9fe4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-89cf3a39-da67-44a8-b686-33324a8f9fe4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"How many rows have the label_predicted2 = label_predicted 2\")\n",
        "print((merge_df['label_predicted'] == merge_df['label_predicted2']).value_counts())"
      ],
      "metadata": {
        "id": "ZumSZ5hCm8ns",
        "outputId": "b9b166d6-bc48-4ea1-d429-e3384af09545",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How many rows have the label_predicted2 = label_predicted 2\n",
            "False    44578\n",
            "True      5422\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wow!** There are much more labels that have the same label prediction from 2 models compared to label predictions from only 1 model and the dataset. We can consider them also as correct and try again the process of training. "
      ],
      "metadata": {
        "id": "wMg4Og_xoanG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"What is the confidence distribution of those with same label\")\n",
        "merge_df[merge_df['label_predicted2'] == merge_df['label_predicted']]['confidence'].plot.hist(bins = 20)"
      ],
      "metadata": {
        "id": "nir6eDGuo42r",
        "outputId": "ddedc9d0-2ff0-4eee-85ea-f25c5cc79d18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the confidence distribution of those with same label\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0478688640>"
            ]
          },
          "metadata": {},
          "execution_count": 111
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"We can also notice that the confidence is very similar\")\n",
        "merge_df[merge_df['label_predicted2'] == merge_df['label_predicted']]['confidence2'].plot.hist(bins = 20)"
      ],
      "metadata": {
        "id": "x5m_1cbzqMnY",
        "outputId": "8f1f9657-178f-4bdc-d35b-3d493f5e5184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the confidence distribution of those with same label\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f04785cdcd0>"
            ]
          },
          "metadata": {},
          "execution_count": 112
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merge_same_label = merge_df[merge_df['label_predicted2'] == merge_df['label_predicted']]\n",
        "df_merge_correct = merge_same_label[merge_same_label['confidence']>0.5][merge_same_label['confidence2']>0.5]\n",
        "display(df_merge_correct)"
      ],
      "metadata": {
        "id": "noY7G4e9p5HV",
        "outputId": "f9f1f546-84fd-4006-a6fd-a64789053622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-115-7dfd195fb6bf>:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df_merge_correct = merge_same_label[merge_same_label['confidence']>0.5][merge_same_label['confidence2']>0.5]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                           path confidence existing_label  \\\n",
              "16     data/task2/images_by_class/26/10161.jpeg   1.163544             26   \n",
              "18      data/task2/images_by_class/3/41165.jpeg   0.593057              3   \n",
              "24     data/task2/images_by_class/90/43274.jpeg   0.833597             90   \n",
              "40      data/task2/images_by_class/15/5984.jpeg   0.595178             15   \n",
              "65      data/task2/images_by_class/13/4939.jpeg   0.571823             13   \n",
              "...                                         ...        ...            ...   \n",
              "48689  data/task2/images_by_class/70/47017.jpeg   1.977154             70   \n",
              "48704  data/task2/images_by_class/78/30148.jpeg   1.566837             78   \n",
              "48937   data/task2/images_by_class/11/4351.jpeg   3.494004             11   \n",
              "49445  data/task2/images_by_class/97/37227.jpeg   2.371952             97   \n",
              "49721  data/task2/images_by_class/33/49813.jpeg   3.107179             33   \n",
              "\n",
              "      label_predicted confidence2 label_predicted2  \n",
              "16                  7    1.163544                7  \n",
              "18                 34    0.593057               34  \n",
              "24                 68    0.833597               68  \n",
              "40                 84    0.595178               84  \n",
              "65                 34    0.571823               34  \n",
              "...               ...         ...              ...  \n",
              "48689              99    2.983143               99  \n",
              "48704              67    1.100655               67  \n",
              "48937              38    1.395029               38  \n",
              "49445              38    0.759175               38  \n",
              "49721              52    2.001565               52  \n",
              "\n",
              "[3208 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-69e36f3e-921e-4678-bb5c-a3f593828367\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>confidence</th>\n",
              "      <th>existing_label</th>\n",
              "      <th>label_predicted</th>\n",
              "      <th>confidence2</th>\n",
              "      <th>label_predicted2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>data/task2/images_by_class/26/10161.jpeg</td>\n",
              "      <td>1.163544</td>\n",
              "      <td>26</td>\n",
              "      <td>7</td>\n",
              "      <td>1.163544</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>data/task2/images_by_class/3/41165.jpeg</td>\n",
              "      <td>0.593057</td>\n",
              "      <td>3</td>\n",
              "      <td>34</td>\n",
              "      <td>0.593057</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>data/task2/images_by_class/90/43274.jpeg</td>\n",
              "      <td>0.833597</td>\n",
              "      <td>90</td>\n",
              "      <td>68</td>\n",
              "      <td>0.833597</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>data/task2/images_by_class/15/5984.jpeg</td>\n",
              "      <td>0.595178</td>\n",
              "      <td>15</td>\n",
              "      <td>84</td>\n",
              "      <td>0.595178</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>data/task2/images_by_class/13/4939.jpeg</td>\n",
              "      <td>0.571823</td>\n",
              "      <td>13</td>\n",
              "      <td>34</td>\n",
              "      <td>0.571823</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48689</th>\n",
              "      <td>data/task2/images_by_class/70/47017.jpeg</td>\n",
              "      <td>1.977154</td>\n",
              "      <td>70</td>\n",
              "      <td>99</td>\n",
              "      <td>2.983143</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48704</th>\n",
              "      <td>data/task2/images_by_class/78/30148.jpeg</td>\n",
              "      <td>1.566837</td>\n",
              "      <td>78</td>\n",
              "      <td>67</td>\n",
              "      <td>1.100655</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48937</th>\n",
              "      <td>data/task2/images_by_class/11/4351.jpeg</td>\n",
              "      <td>3.494004</td>\n",
              "      <td>11</td>\n",
              "      <td>38</td>\n",
              "      <td>1.395029</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49445</th>\n",
              "      <td>data/task2/images_by_class/97/37227.jpeg</td>\n",
              "      <td>2.371952</td>\n",
              "      <td>97</td>\n",
              "      <td>38</td>\n",
              "      <td>0.759175</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49721</th>\n",
              "      <td>data/task2/images_by_class/33/49813.jpeg</td>\n",
              "      <td>3.107179</td>\n",
              "      <td>33</td>\n",
              "      <td>52</td>\n",
              "      <td>2.001565</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3208 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69e36f3e-921e-4678-bb5c-a3f593828367')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-69e36f3e-921e-4678-bb5c-a3f593828367 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-69e36f3e-921e-4678-bb5c-a3f593828367');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iteration 3"
      ],
      "metadata": {
        "id": "FvT-3Id7Fs6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct_df_all = pd.concat([\n",
        "    df_merge_correct[['path','existing_label']], \n",
        "    df_correct_1[['path','existing_label']], \n",
        "    df_correct2[['path','existing_label']]])"
      ],
      "metadata": {
        "id": "pkJt2zyOr82N"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_df_all"
      ],
      "metadata": {
        "id": "yu52lu-NsRDY",
        "outputId": "ce97eec1-e933-49f2-eb38-b2df4ca15c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           path existing_label\n",
              "16     data/task2/images_by_class/26/10161.jpeg             26\n",
              "18      data/task2/images_by_class/3/41165.jpeg              3\n",
              "24     data/task2/images_by_class/90/43274.jpeg             90\n",
              "40      data/task2/images_by_class/15/5984.jpeg             15\n",
              "65      data/task2/images_by_class/13/4939.jpeg             13\n",
              "...                                         ...            ...\n",
              "23971  data/task2/images_by_class/99/37829.jpeg             99\n",
              "24292  data/task2/images_by_class/99/38055.jpeg             99\n",
              "24424  data/task2/images_by_class/99/37854.jpeg             99\n",
              "24585  data/task2/images_by_class/91/35101.jpeg             91\n",
              "24623  data/task2/images_by_class/84/32243.jpeg             84\n",
              "\n",
              "[3970 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e0e8eedd-45a1-4a9c-90a8-41778edbc80a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>existing_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>data/task2/images_by_class/26/10161.jpeg</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>data/task2/images_by_class/3/41165.jpeg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>data/task2/images_by_class/90/43274.jpeg</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>data/task2/images_by_class/15/5984.jpeg</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>data/task2/images_by_class/13/4939.jpeg</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23971</th>\n",
              "      <td>data/task2/images_by_class/99/37829.jpeg</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24292</th>\n",
              "      <td>data/task2/images_by_class/99/38055.jpeg</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24424</th>\n",
              "      <td>data/task2/images_by_class/99/37854.jpeg</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24585</th>\n",
              "      <td>data/task2/images_by_class/91/35101.jpeg</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24623</th>\n",
              "      <td>data/task2/images_by_class/84/32243.jpeg</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3970 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0e8eedd-45a1-4a9c-90a8-41778edbc80a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e0e8eedd-45a1-4a9c-90a8-41778edbc80a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e0e8eedd-45a1-4a9c-90a8-41778edbc80a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move all images considered correct into the training folder"
      ],
      "metadata": {
        "id": "ocsGDv3yr3U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "zfGR6AoGzpbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare unlabeled dataset"
      ],
      "metadata": {
        "id": "lnTlyuE9wLF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir_data = 'data/task2/val_data'\n",
        "# copy data to allow dataloader to read it easier\n",
        "dest_dir = 'data/task2/val_data/0'\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "# Iterate over all the files in the source directory\n",
        "for file in tqdm(os.listdir(dir_data)):\n",
        "    if file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".png\"):\n",
        "        # Construct the source and destination paths\n",
        "        src_path = os.path.join(dir_data, file)\n",
        "        dst_path = os.path.join(dest_dir, file)\n",
        "        # Move the file\n",
        "        shutil.move(src_path, dst_path) "
      ],
      "metadata": {
        "id": "1oL3eDGtwQNU",
        "outputId": "6319bd46-3712-4f27-f0f2-82b5838dba46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5001/5001 [00:00<00:00, 31199.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import DatasetFolder\n",
        "\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(experiment_info[\"image_processing\"][\"resize\"]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=experiment_info[\"image_processing\"][\"mean\"],\n",
        "                         std=experiment_info[\"image_processing\"][\"std\"]),\n",
        "    ])\n",
        "\n",
        "dir_data = 'data/task2/val_data'\n",
        "image_dataset_unlabeled = datasets.ImageFolder(root=dir_data, transform=preprocess)\n",
        "\n",
        "dataloader_unlabeled  = torch.utils.data.DataLoader(\n",
        "    image_dataset_unlabeled, \n",
        "    batch_size = 1, \n",
        "    )\n",
        "\n",
        "dataset_size = len(image_dataset_unlabeled)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "     "
      ],
      "metadata": {
        "id": "a1RFD1utwWGL"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_csv(predicted_labels, name=\"file.csv\"):\n",
        "    pred = df.from_dict(predicted_labels)\n",
        "    to_csv_paths = []\n",
        "    for it in predicted_labels['path']:\n",
        "        to_csv_paths.append(it.split('/')[-1])\n",
        "    order = [int(i[:-5]) for i in to_csv_paths]\n",
        "    # Drop that column\n",
        "    # Put whatever series you want in its place\n",
        "    pred['path'] = to_csv_paths\n",
        "    pred['order'] = order\n",
        "    pred.sort_values('order', inplace=True)\n",
        "    pred.drop([\"order\",\"confidence\"], axis=1, inplace = True)\n",
        "    pred.rename(columns={\"path\": \"sample\"}, inplace = True)\n",
        "    pred.to_csv(name, index=False)"
      ],
      "metadata": {
        "id": "oEGkAY3Iw7Am"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_eval_predictions(model, image_dataset_unlabeled, dataloader_unlabeled):\n",
        "    predicted_labels = {\n",
        "    \"path\":[],\n",
        "    \"confidence\":[],\n",
        "    \"label\":[]\n",
        "    }\n",
        "\n",
        "    # Each epoch has a training and validation phase \n",
        "    model.eval()   # Set model to evaluate mode \n",
        "\n",
        "    # Iterate over data.\n",
        "    img_path_generator = ((image, path) for (path,_) , (image, _) in zip(image_dataset_unlabeled.samples, dataloader_unlabeled))\n",
        "\n",
        "    for inputs, path in tqdm(img_path_generator, total=dataset_size):\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(False): # we don't want to train\n",
        "            outputs = model(inputs)\n",
        "            confidence, preds = torch.max(outputs, 1) \n",
        "        predicted_labels[\"path\"].append(path) \n",
        "        predicted_labels[\"confidence\"].append(confidence.item())\n",
        "        predicted_labels[\"label\"].append(preds.item())\n",
        "    return predicted_labels "
      ],
      "metadata": {
        "id": "FLxGkNPcytku"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a single model on all the correct labels only for evaluation\n",
        "Also compare this with training on the whole dataset "
      ],
      "metadata": {
        "id": "a0eBLrahrkvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_dir = 'data/task2/images_by_class'\n",
        "if os.path.exists(target_dir):\n",
        "    # Use rmtree to delete the directory and all its contents\n",
        "    shutil.rmtree(target_dir)\n",
        "    print(f'{target_dir} has been deleted')\n",
        "else:\n",
        "    print(f'{target_dir} does not exist')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66a69569-f398-46af-f4f7-6b4a1313cd53",
        "id": "AVuYWTr-tcJN"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/task2/images_by_class has been deleted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the directory exists, to recreate it instead of messing it up\n",
        "target_dir = 'data/task2/training_confident'\n",
        "\n",
        "if os.path.exists(target_dir):\n",
        "    # Use rmtree to delete the directory and all its contents\n",
        "    shutil.rmtree(target_dir)\n",
        "    print(f'{target_dir} has been deleted') \n",
        "else:\n",
        "    print(f'{target_dir} does not exist')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9b82987-5dc6-4420-e90e-d8b5352f9e6a",
        "id": "_quCksYWtcJO"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/task2/training_confident has been deleted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_dir = 'data/task2/images_by_class'\n",
        "dir_data = 'data/task2/train_data/'\n",
        "# Read the annotations file into a DataFrame\n",
        "df = pd.read_csv(f'{dir_data}annotations.csv')\n",
        "\n",
        "# Iterate over the rows in the DataFrame\n",
        "for _, row in tqdm(df.iterrows(), total= df.shape[0]):\n",
        "    # Extract the path and class from the row\n",
        "    path = row['renamed_path']\n",
        "    label = row['label_idx']\n",
        "    \n",
        "    # Create the directory for the class\n",
        "    class_dir = f'{target_dir}/{label}'\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    \n",
        "    # Copy the file to the class directory\n",
        "    shutil.copy(f\"data/{path}\", class_dir) \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c02300a-41d4-4398-e8b8-833c60d42339",
        "id": "jt7k_xhHtcJO"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:10<00:00, 4568.15it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on 2 datasets at the same time, one with high confidence labels\n",
        "# this dataset will only be used during training, never forgetting labels\n",
        "# the second dataset will iteratively forget labels and train on the remaining labels.\n",
        "# Define the base directory\n",
        "target_dir = 'data/task2/training_confident'\n",
        "# Iterate over the rows in the DataFrame\n",
        "for _, row in tqdm(correct_df_all.iterrows(), total=correct_df_all.shape[0]):\n",
        "    # Extract the path and class from the row\n",
        "    path = row['path']\n",
        "    label = row['existing_label'] \n",
        "\n",
        "    # Create the directory for the class\n",
        "    class_dir = f'{target_dir}/{label}'\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    \n",
        "    # move the file to the class directory\n",
        "    shutil.move(f\"{path}\", class_dir)\n",
        "    \n",
        "\n",
        "# we move the images because we don't want them to remain in the dataset for relabeling.  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46a03a9b-b7c2-4078-a44c-e4f113dfe75f",
        "id": "a-m3ANMVtcJP"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3970/3970 [00:01<00:00, 3182.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: create the 2 datasets\n",
        "\n",
        "data_dir_train = 'data/task2/training_confident'\n",
        "image_dataset_train_only = datasets.ImageFolder(data_dir_train, preprocess) \n",
        "dataloader_train_only  = DataLoader(\n",
        "    image_dataset_train_only, \n",
        "    batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"],\n",
        "    shuffle = experiment_info[\"hyperparameters_data\"][\"shuffle_dataloader\"], \n",
        "    num_workers = experiment_info[\"hyperparameters_data\"][\"num_workers\"]\n",
        "    )\n",
        "\n",
        "class_names = image_dataset_train_only.classes\n",
        "dataset_size = len(image_dataset_train_only)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "O266GLXcrqyR",
        "outputId": "db3070bd-c090-4124-a9d8-dbac52ba25b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_size)"
      ],
      "metadata": {
        "id": "5dSTW4C4vK2u",
        "outputId": "fc73a05a-de9a-4370-8989-29c4ddd886a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3970\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a pytorch MobileNet pretrained model\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "# change the Linear output to fit our dataset ( because model initially has 1000 classes)\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "# Setting hyperparameters\n",
        "\n",
        "model = model.to(device)\n",
        "print(model.classifier)"
      ],
      "metadata": {
        "id": "mngC6ZIsvV2U",
        "outputId": "c5871efc-7a27-45a4-f7c7-1e710550353a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=False)\n",
            "  (1): Linear(in_features=1280, out_features=100, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=experiment_info[\"hyperparameters_training\"][\"learning_rate\"],\n",
        "    )\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(\n",
        "    optimizer, \n",
        "    step_size=experiment_info[\"hyperparameters_training\"][\"scheduler_step_size\"], \n",
        "    gamma=experiment_info[\"hyperparameters_training\"][\"scheduler_gamma\"],\n",
        "    )"
      ],
      "metadata": {
        "id": "Y3AgFetjvYcY"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model3(model, \n",
        "                    exp_lr_scheduler, \n",
        "                    optimizer, \n",
        "                    criterion,  \n",
        "                    data_sizes[\"train\"], \n",
        "                    dataloader_train_only,\n",
        "                    num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])"
      ],
      "metadata": {
        "id": "LZ47VxvRvZxu",
        "outputId": "4137fcd5-96f7-4cfe-ddea-9a24501102c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/125 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|██████████| 125/125 [00:17<00:00,  7.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 20.2717 Acc: 0.7445\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:17<00:00,  7.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 14.2092 Acc: 1.8645\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:18<00:00,  6.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 10.5710 Acc: 2.7677\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:17<00:00,  7.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 8.0411 Acc: 3.3316\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:20<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 5.8973 Acc: 3.8697\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:18<00:00,  6.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 4.3195 Acc: 4.2968\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:18<00:00,  6.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.9363 Acc: 4.7097\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:17<00:00,  7.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.8878 Acc: 4.9613\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:19<00:00,  6.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.7099 Acc: 4.9897\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:17<00:00,  7.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.5976 Acc: 5.0271\n",
            "\n",
            "Training complete in 3m 1s\n",
            "Best val Acc: 5.027097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_m = f'/gdrive/MyDrive/checkpoints/noisy_labels/eval_model1.pt'\n",
        "torch.save(model.state_dict(), path_m)"
      ],
      "metadata": {
        "id": "D-nRCGD1we3J"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = make_eval_predictions(model, image_dataset_unlabeled, dataloader_unlabeled)"
      ],
      "metadata": {
        "id": "Ubt6EFWwwE8q",
        "outputId": "98ab8f11-4ece-4732-d152-54a08230f05c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [00:51<00:00, 97.96it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_csv(predicted_labels, \"eval1.csv\")"
      ],
      "metadata": {
        "id": "KA3wrD_myNpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train model directly on all images "
      ],
      "metadata": {
        "id": "Hj5efjupxV74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_dir = 'data/task2/images_by_class'\n",
        "if os.path.exists(target_dir):\n",
        "    # Use rmtree to delete the directory and all its contents\n",
        "    shutil.rmtree(target_dir)\n",
        "    print(f'{target_dir} has been deleted')\n",
        "else:\n",
        "    print(f'{target_dir} does not exist')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "053538a4-7654-445a-a8b4-880b85416ea4",
        "id": "-1bMpEmExikL"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/task2/images_by_class has been deleted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the directory exists, to recreate it instead of messing it up\n",
        "target_dir = 'data/task2/training_confident'\n",
        "\n",
        "if os.path.exists(target_dir):\n",
        "    # Use rmtree to delete the directory and all its contents\n",
        "    shutil.rmtree(target_dir)\n",
        "    print(f'{target_dir} has been deleted') \n",
        "else:\n",
        "    print(f'{target_dir} does not exist')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "410eeb9b-e709-4754-a15c-042acf8b1b2f",
        "id": "bYSCm9_JxikL"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/task2/training_confident has been deleted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_dir = 'data/task2/images_by_class'\n",
        "dir_data = 'data/task2/train_data/'\n",
        "# Read the annotations file into a DataFrame\n",
        "df = pd.read_csv(f'{dir_data}annotations.csv')\n",
        "\n",
        "# Iterate over the rows in the DataFrame\n",
        "for _, row in tqdm(df.iterrows(), total= df.shape[0]):\n",
        "    # Extract the path and class from the row\n",
        "    path = row['renamed_path']\n",
        "    label = row['label_idx']\n",
        "    \n",
        "    # Create the directory for the class\n",
        "    class_dir = f'{target_dir}/{label}'\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    \n",
        "    # Copy the file to the class directory\n",
        "    shutil.copy(f\"data/{path}\", class_dir) \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70970ef9-2ee1-4da6-fb85-1ff52fdc06af",
        "id": "2j4k5oqJxikM"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:11<00:00, 4461.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: create the 2 datasets\n",
        "\n",
        "data_dir_train = 'data/task2/images_by_class'\n",
        "image_dataset_train_only = datasets.ImageFolder(data_dir_train, preprocess) \n",
        "dataloader_train_only  = DataLoader(\n",
        "    image_dataset_train_only, \n",
        "    batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"],\n",
        "    shuffle = experiment_info[\"hyperparameters_data\"][\"shuffle_dataloader\"], \n",
        "    num_workers = experiment_info[\"hyperparameters_data\"][\"num_workers\"]\n",
        "    )\n",
        "\n",
        "class_names = image_dataset_train_only.classes\n",
        "dataset_size = len(image_dataset_train_only)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "blbcqNulxZ-Y",
        "outputId": "7637a39d-be98-488e-c1a1-9e673ab066b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a pytorch MobileNet pretrained model\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "# change the Linear output to fit our dataset ( because model initially has 1000 classes)\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "# Setting hyperparameters\n",
        "\n",
        "model = model.to(device)\n",
        "print(model.classifier)"
      ],
      "metadata": {
        "id": "V5zGYVgsxt6N",
        "outputId": "eddb38ba-b1d6-4669-ab68-146776eff0fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=False)\n",
            "  (1): Linear(in_features=1280, out_features=100, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model3(model, \n",
        "                    exp_lr_scheduler, \n",
        "                    optimizer, \n",
        "                    criterion,  \n",
        "                    data_sizes[\"train\"], \n",
        "                    dataloader_train_only,\n",
        "                    num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])"
      ],
      "metadata": {
        "id": "vsXVQbxgx2Kh",
        "outputId": "59fe2e18-58cb-45e2-c70c-4556bd0d5560",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 167,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:47<00:00,  6.86it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.8403 Acc: 0.7884\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:46<00:00,  6.91it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.8761 Acc: 0.7665\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:36<00:00,  7.22it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.7891 Acc: 0.7600\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:31<00:00,  7.39it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.8233 Acc: 0.8052\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:31<00:00,  7.39it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.6771 Acc: 0.7613\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:31<00:00,  7.37it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.8680 Acc: 0.7316\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:31<00:00,  7.38it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.7473 Acc: 0.8039\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:31<00:00,  7.38it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.8817 Acc: 0.7613\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:32<00:00,  7.36it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.7556 Acc: 0.7290\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [03:31<00:00,  7.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 301.7398 Acc: 0.7561\n",
            "\n",
            "Training complete in 35m 53s\n",
            "Best val Acc: 0.805161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_m = f'/gdrive/MyDrive/checkpoints/noisy_labels/just_train.pt'\n",
        "torch.save(model.state_dict(), path_m)"
      ],
      "metadata": {
        "id": "Gd4z81Y6x5YZ"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = make_eval_predictions(model, image_dataset_unlabeled, dataloader_unlabeled)"
      ],
      "metadata": {
        "id": "I-dKq24XzDI2",
        "outputId": "91b34ffd-27d6-4fa4-8eac-f0b9ebddf673",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [00:42<00:00, 116.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_csv(predicted_labels, \"just_train.csv\")"
      ],
      "metadata": {
        "id": "E8S6x_8pyVTJ"
      },
      "execution_count": 177,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb68d62d599a462f92aaca0ab0b854e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6eed1376f1f24e8abbe2d4ea0ca3fcb1",
              "IPY_MODEL_07c37bb6b22b4c8abe8d16b61e28e1db",
              "IPY_MODEL_15a23332521441cab29f7acedc079b16"
            ],
            "layout": "IPY_MODEL_816420dd910b475faa99a8e9ef6eebd4"
          }
        },
        "6eed1376f1f24e8abbe2d4ea0ca3fcb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71a16d1906924e738fc1baa304d94811",
            "placeholder": "​",
            "style": "IPY_MODEL_a74028e913de43d3b1eae97433d91882",
            "value": "100%"
          }
        },
        "07c37bb6b22b4c8abe8d16b61e28e1db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07868acd1960497680b904b81fd0eaf9",
            "max": 14212972,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a56ed7ebe21c4011a23a63cad606d40c",
            "value": 14212972
          }
        },
        "15a23332521441cab29f7acedc079b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba437891973d4629b4881ccb11f044ae",
            "placeholder": "​",
            "style": "IPY_MODEL_a75d7f60d1c24475a1540bc1cfe94f7b",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 62.9MB/s]"
          }
        },
        "816420dd910b475faa99a8e9ef6eebd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71a16d1906924e738fc1baa304d94811": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a74028e913de43d3b1eae97433d91882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07868acd1960497680b904b81fd0eaf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a56ed7ebe21c4011a23a63cad606d40c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba437891973d4629b4881ccb11f044ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a75d7f60d1c24475a1540bc1cfe94f7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b6b0668f3764a7296945547803e2688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c39c58da42ce4c2ca76546673443f0fd",
              "IPY_MODEL_b02ccf11c1714786a67bfca3446e9b9d",
              "IPY_MODEL_284de89238b54efd8d834cfc5cdef224"
            ],
            "layout": "IPY_MODEL_4d1e4cd757a749bda0f6fbcede536076"
          }
        },
        "c39c58da42ce4c2ca76546673443f0fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd637c7b9a5645dfb198f017092d098a",
            "placeholder": "​",
            "style": "IPY_MODEL_afee6006e2324bac8bbc9c291e638617",
            "value": "100%"
          }
        },
        "b02ccf11c1714786a67bfca3446e9b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e3263acdec647e594b2140e749bd110",
            "max": 14212972,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4dd784f55bb14f6ba9274ec956ec8061",
            "value": 14212972
          }
        },
        "284de89238b54efd8d834cfc5cdef224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7adf26ae5d2e430e946d5576135d893a",
            "placeholder": "​",
            "style": "IPY_MODEL_69db9e55793a4d87bd7b0c96efc53170",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 46.0MB/s]"
          }
        },
        "4d1e4cd757a749bda0f6fbcede536076": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd637c7b9a5645dfb198f017092d098a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afee6006e2324bac8bbc9c291e638617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e3263acdec647e594b2140e749bd110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dd784f55bb14f6ba9274ec956ec8061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7adf26ae5d2e430e946d5576135d893a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69db9e55793a4d87bd7b0c96efc53170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}