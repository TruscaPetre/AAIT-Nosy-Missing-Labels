{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TruscaPetre/AAIT-Nosy-Missing-Labels/blob/main/tutorial%20colab%20noisy%20labels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaRT8CyQPpcv"
      },
      "source": [
        "# Noisy labels problem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOThg22n_IX7"
      },
      "source": [
        "## Introduction theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJxksI3i_IX-"
      },
      "source": [
        "Before starting to implement anything we should think about the problem first. We don't know which of the training samples are wrong, and we cannot try to correct them by hand because there are too many of them and the images are not very clear anyway.\n",
        "\n",
        "We need some Noise-aware training techniques. Some ideas are:\n",
        "- training using a loss function that is designed to be robust to label noise. Such as:\n",
        "  - focal loss, it down-weighs the contribution of easy examples and puts more ephasis on difficult examples, which can make it resistant to noise. `torch.nn.FocalLoss`\n",
        "  - Generalized Cross-Entropy loss, which includes additional hyperparameters that allow the model to learn the noise rate and label corruption matrix. `torch.nn.GCE`\n",
        "- bootstrapping\n",
        "- self-ensembling to learn multiple models that are more robust to noise\n",
        "- Confidence based method, because predicting the confidence may allow humans to discard the least confidence examples. ( but we are not creating a real world model, we are getting tested automatically with a test set, so this method cannot work.)\n",
        "- Pseudo-labeling, using the model's own predictions to re-label a portion of the training data. Similar to self-training, we eliminate completely labels of some samples and we try to predict them. In case the labels are the same after relabling, there is a higher chance they are correct. We would gradually relable the images into a better dataset. \n",
        "    - Or Computing a probability that the labels are misslabeled, than using that probability to weigh heavier on the labels that have a higher chance of being correct during training. `torch.nn.CrossEntropyLoss`\n",
        "- Noise tolerant algorithms: complex deep learning models are more tolerant to noise. (this is not a very solid technique but should be used together with others.)\n",
        "\n",
        "References:\n",
        "- Focal loss: This loss function was first introduced in the paper \"Focal Loss for Dense Object Detection\" by Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár (https://arxiv.org/abs/1708.02002).\n",
        "- Generalized cross-entropy loss: This loss function was proposed in the paper \"Learning with Noisy Labels\" by Mengye Ren, Elad Hazan, and Yoram Singer (https://arxiv.org/abs/1609.03683).\n",
        "- Bootstrapping: This technique involves training multiple models on different subsets of the training data, and then combining their predictions to make a final prediction. It was first introduced in the paper \"Bagging Predictors\" by Leo Breiman (https://link.springer.com/article/10.1023/A:1018054314350).\n",
        "- Self-ensembling: This technique involves training multiple models on the same data, and then using their predictions to create a final prediction. It was first introduced in the paper \"Self-Ensembling for Visual Domain Adaptation\" by Sergey Zagoruyko and Nikos Komodakis (https://arxiv.org/abs/1706.05208).\n",
        "- Pseudo-labeling: This technique involves using the model's own predictions to label a portion of the training data. It was first introduced in the paper \"Semi-Supervised Learning with Deep Generative Models\" by Diederik Kingma, Danilo Jimenez Rezende, Shakir Mohamed, and Max Welling (https://arxiv.org/abs/1406.5298).\n",
        "- Noise-aware training: This refers to techniques that are specifically designed to handle label noise in the training data. One example of such a technique is the generalized cross-entropy loss function, which was introduced in the paper \"Learning with Noisy Labels\" by Mengye Ren, Elad Hazan, and Yoram Singer (https://arxiv.org/abs/1609.03683).\n",
        "- Confidence based methods: This refers to techniques that involve training a model to predict the confidence or probability of each class label, rather than just the class label itself. This can allow the model to identify examples that are less confident and potentially more prone to noise. One example of a confidence-based method is the method of \"bootstrapping,\" which was introduced in the paper \"Bagging Predictors\" by Leo Breiman (https://link.springer.com/article/10.1023/A:1018054314350)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeBHHsZt_IYU"
      },
      "source": [
        "## Pseudo Labeling solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnkncVA__IYW"
      },
      "source": [
        "\n",
        "This technique is barely described above\n",
        "\n",
        "The first question is what portion of the dataset should I \"forget\" the labels and try predict new ones(by training on the remaining ones)? \n",
        "\n",
        "Making the worse assumption that half of the labels are wrong. Than we should leave out a smaller portion of the labels to be re-labeled.\n",
        "But because we have a rather large dataset of 500 images per class and that the model we are using is a rather complex one which is already pretrained, we may increase the number of labels for re-labeling. \n",
        "\n",
        "According to previous argument I chose to separate the dataset in 10 parts. \n",
        "I will leave 10% of the dataset out and only use the rest of 90%. \n",
        "Similar to a 10-fold cross validation, but instead of validating we are re-labeling the remaining 10% of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmzRZhui_IYY"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x_I1fk99_IYZ"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "import shutil\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import SubsetRandomSampler, Dataset, DataLoader\n",
        "from torchvision import transforms, datasets \n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import itertools\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yyOZc59J_IYc"
      },
      "outputs": [],
      "source": [
        "task2_id = \"1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCcacF4f_IYe",
        "outputId": "c19a4eb9-5373-489c-b1d2-b2b932ea3a28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-01-19 18:37:33--  https://docs.google.com/uc?export=download&confirm=t&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.10.138, 142.251.10.101, 142.251.10.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.10.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-8k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/l6i8f33bttsueieqhgf7ckour82p22me/1674153450000/08997952672865575084/*/1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL?e=download&uuid=a5475b9c-b253-45e2-bc63-9e72e3af5048 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-01-19 18:37:33--  https://doc-04-8k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/l6i8f33bttsueieqhgf7ckour82p22me/1674153450000/08997952672865575084/*/1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL?e=download&uuid=a5475b9c-b253-45e2-bc63-9e72e3af5048\n",
            "Resolving doc-04-8k-docs.googleusercontent.com (doc-04-8k-docs.googleusercontent.com)... 142.251.12.132, 2404:6800:4003:c11::84\n",
            "Connecting to doc-04-8k-docs.googleusercontent.com (doc-04-8k-docs.googleusercontent.com)|142.251.12.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75324736 (72M) [application/x-gzip]\n",
            "Saving to: ‘task2.tar.gz’\n",
            "\n",
            "task2.tar.gz        100%[===================>]  71.83M  28.8MB/s    in 2.5s    \n",
            "\n",
            "2023-01-19 18:37:36 (28.8 MB/s) - ‘task2.tar.gz’ saved [75324736/75324736]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# replace here your ide &id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\"\n",
        "# replace here your id 'https://docs.google.com/uc?export=download&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL'\n",
        "# replace here your target name -O task1.tar.gz &&\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\" -O task2.tar.gz && rm -rf /tmp/cookies.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1bkbvqeF_IYf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!mkdir data\n",
        "!mv task2.tar.gz ./data\n",
        "!tar -xzvf \"/content/data/task2.tar.gz\" -C \"/content/data/\"     #[run this cell to extract tar.gz files]\n",
        "# this may take 12 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DLqmpUlecPCx"
      },
      "outputs": [],
      "source": [
        "experiment_info = {\n",
        "    \"iteration\": 1,\n",
        "    \"image_processing\":{\n",
        "        \"resize\":224,\n",
        "        \"mean\":[0.485, 0.456, 0.406],\n",
        "        \"std\":[0.229, 0.224, 0.225],\n",
        "    },\n",
        "    \"hyperparameters_data\": {\n",
        "        \"batch_size\":32,\n",
        "        \"shuffle_dataloader\":True,\n",
        "        \"num_workers\":4\n",
        "    },\n",
        "    \"random_seeds\":{\n",
        "        \"torch_seed\":42,\n",
        "        \"numpy_seed\":42,\n",
        "        \"cuda_seed\":42,\n",
        "\n",
        "    },\n",
        "    \"hyperparameters_training\":{\n",
        "        \"learning_rate\": 0.0001,\n",
        "        \"scheduler_step_size\":7,\n",
        "        \"scheduler_gamma\":0.1,\n",
        "        \"num_epochs\":10, \n",
        "    },\n",
        "    \"total_unlabeled\":26445,\n",
        "}\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euHUUz0w_IYg"
      },
      "source": [
        "## Mount drive \n",
        "( in order to save each iteration of leave out labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R4ZCnSB_IYh",
        "outputId": "7f647fa6-850a-4b74-fed6-72c40c8c58fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j0gmw68_IYi",
        "outputId": "3ccc82e0-c591-4dab-c42d-fde67184ef77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello Google Drive!"
          ]
        }
      ],
      "source": [
        "with open('/gdrive/MyDrive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat '/gdrive/MyDrive/foo.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0w0JOuB5_IYj"
      },
      "outputs": [],
      "source": [
        "!rm '/gdrive/MyDrive/foo.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g7qmzT4Wkbf"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Ci_KHZYMWqgD"
      },
      "outputs": [],
      "source": [
        "class ShuffledImageFolder(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.indices = torch.randperm(len(dataset))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path, _ = self.dataset.samples[self.indices[index]]\n",
        "        image, label = self.dataset[self.indices[index]]\n",
        "        return image, label, path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "toHy-rW4_IYr"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, scheduler, optimizer, criterion, dataset_size, dataloader):\n",
        "                     \n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over data.\n",
        "    for inputs, labels, _ in tqdm(dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "    \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_loss = running_loss / dataset_size\n",
        "    epoch_acc = running_corrects.double() / dataset_size\n",
        "    return model, epoch_loss, epoch_acc\n",
        "\n",
        "def train_model(model, *args, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()  # Set model to training mode\n",
        "        model, epoch_loss, epoch_acc = train_loop(model, *args)\n",
        "        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # deep copy the model\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "-gh3Y2c5PFYo"
      },
      "outputs": [],
      "source": [
        "def load_model(PATH, device='cuda'):\n",
        "    # Download a pytorch MobileNet pretrained model\n",
        "    model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "    # change the Linear output to fit our dataset ( because model initially has 1000 classes)\n",
        "    model.classifier[1] = nn.Linear(1280, 100)\n",
        "    # Read the models from drive that need to be used for the predictions\n",
        "    model.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))\n",
        "    model = model.to(device) \n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Z7xQs8c7bkQq"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_paths):\n",
        "        self.image_paths = image_paths\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # Load the image and return it as a tensor\n",
        "        image_path = self.image_paths[index]\n",
        "        image = Image.open(image_path)\n",
        "        image = image.convert('RGB')\n",
        "        image = transforms.ToTensor()(image)\n",
        "        label = int(image_path.split(\"/\")[-2])\n",
        "        return image, label, image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ASudnmb56uIy"
      },
      "outputs": [],
      "source": [
        "def batches_to_list(batches):\n",
        "    paths_list = []\n",
        "    for batch in batches:\n",
        "        paths_list += batch\n",
        "    return paths_list\n",
        "\n",
        "def generate_new_labels(model, loader, device='cuda'):\n",
        "    # create new predictions\n",
        "    \n",
        "    predicted_labels = {\n",
        "        \"path\":[],\n",
        "        \"confidence\":[],\n",
        "        \"existing_label\":[],\n",
        "        \"label_predicted\":[]\n",
        "    }\n",
        "\n",
        "    # Each epoch has a training and validation phase \n",
        "    model.eval()   # Set model to evaluate mode \n",
        "\n",
        "    # Iterate over data. \n",
        "\n",
        "    for image, label, image_path in tqdm(loader):\n",
        "        image = image.to(device)\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(False): # we don't want to train\n",
        "            outputs = model(image)\n",
        "            confidence, preds = torch.max(outputs, 1) \n",
        "\n",
        "        predicted_labels[\"path\"].append(image_path) \n",
        "        predicted_labels[\"confidence\"].append(confidence.item())\n",
        "        predicted_labels[\"label_predicted\"].append(preds.item())\n",
        "        predicted_labels[\"existing_label\"].append(label.item())\n",
        "\n",
        "    return predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "tO_cnZt4O-ea"
      },
      "outputs": [],
      "source": [
        "def train_loop2(model, scheduler, optimizer, criterion, dataset_size, dataloader1, dataloader2):\n",
        "                     \n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over data for confident data.\n",
        "    for inputs, labels in tqdm(dataloader1):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "    \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over data for relabling.\n",
        "    for inputs, labels, _ in tqdm(dataloader2):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "    \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_loss = running_loss / dataset_size\n",
        "    epoch_acc = running_corrects.double() / dataset_size\n",
        "    return model, epoch_loss, epoch_acc\n",
        "\n",
        "def train_model2(model, *args, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()  # Set model to training mode\n",
        "        model, epoch_loss, epoch_acc = train_loop2(model, *args)\n",
        "        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # deep copy the model\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "-0Jf8W03vrbh"
      },
      "outputs": [],
      "source": [
        "def train_loop3(model, scheduler, optimizer, criterion, dataset_size, dataloader1):\n",
        "                     \n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over data for confident data.\n",
        "    for inputs, labels in tqdm(dataloader1):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "    \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_loss = running_loss / dataset_size\n",
        "    epoch_acc = running_corrects.double() / dataset_size\n",
        "    return model, epoch_loss, epoch_acc\n",
        "\n",
        "def train_model3(model, *args, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()  # Set model to training mode\n",
        "        model, epoch_loss, epoch_acc = train_loop3(model, *args)\n",
        "        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # deep copy the model\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "AqiqE-lxGyKJ"
      },
      "outputs": [],
      "source": [
        "def del_dir_if_exist(target_dir):\n",
        "    if os.path.exists(target_dir):\n",
        "        # Use rmtree to delete the directory and all its contents\n",
        "        shutil.rmtree(target_dir)\n",
        "        print(f'{target_dir} has been deleted')\n",
        "    else:\n",
        "        print(f'{target_dir} does not exist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "hccRF8_QId_G"
      },
      "outputs": [],
      "source": [
        "def make_dataset_folder():\n",
        "    target_dir = 'data/task2/images_by_class'\n",
        "    df = pd.read_csv('data/task2/train_data/annotations.csv')\n",
        "    # Iterate over the rows in the DataFrame\n",
        "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "        # Extract the path and class from the row\n",
        "        path = row['renamed_path']\n",
        "        label = row['label_idx']\n",
        "        \n",
        "        # Create the directory for the class\n",
        "        class_dir = f'{target_dir}/{label}'\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "        \n",
        "        # Copy the file to the class directory\n",
        "        shutil.copy(f\"data/{path}\", class_dir) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "oEGkAY3Iw7Am"
      },
      "outputs": [],
      "source": [
        "def to_csv(predicted_labels, name=\"file.csv\"):\n",
        "    pred = pd.DataFrame.from_dict(predicted_labels)\n",
        "    to_csv_paths = []\n",
        "    for it in predicted_labels['path']:\n",
        "        to_csv_paths.append(it.split('/')[-1])\n",
        "    order = [int(i[:-5]) for i in to_csv_paths]\n",
        "    # Drop that column\n",
        "    # Put whatever series you want in its place\n",
        "    pred['path'] = to_csv_paths\n",
        "    pred['order'] = order\n",
        "    pred.sort_values('order', inplace=True)\n",
        "    pred.drop([\"order\",\"confidence\"], axis=1, inplace = True)\n",
        "    pred.rename(columns={\"path\": \"sample\"}, inplace = True)\n",
        "    pred.to_csv(name, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "OgOmpkf9LR-8"
      },
      "outputs": [],
      "source": [
        "def create_dataset_splits(splits, image_dataset):\n",
        "\n",
        "    num_samples = len(image_dataset)\n",
        "\n",
        "    # Split the image_dataset into 10 parts\n",
        "    part_size = num_samples // splits\n",
        "    parts = [list(range(i * part_size, (i + 1) * part_size)) for i in range(splits)]\n",
        "\n",
        "    batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"]\n",
        "    # Use one part for creating new labels and the other 9 parts for training the model\n",
        "    loaders = {}\n",
        "    for i in range(splits):\n",
        "        # Get the indices for the training set, which are not in the labeling set\n",
        "        train_indices = [index for j, part in enumerate(parts) if j != i for index in part]\n",
        "        labeling_indices = parts[i]\n",
        "\n",
        "        # Create a sampler for the training set and the labeling set\n",
        "        train_sampler = SubsetRandomSampler(train_indices)\n",
        "        labeling_sampler = SubsetRandomSampler(labeling_indices)\n",
        "\n",
        "        # Use the samplers to create data loaders for the training set and the labeling set\n",
        "        train_loader = DataLoader(image_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "        labeling_loader = DataLoader(image_dataset, batch_size=batch_size, sampler=labeling_sampler)\n",
        "        loaders[i] = {\n",
        "            \"train\":train_loader, \n",
        "            \"labeling\":labeling_loader\n",
        "        }\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    data_sizes = {\n",
        "        \"train\" : len(loaders[0][\"train\"]), # this is the number of batches not images\n",
        "        \"labeling\" : len(loaders[0][\"labeling\"]),\n",
        "    }\n",
        "\n",
        "    return data_sizes, loaders, device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "nzlyTvnQMt7z"
      },
      "outputs": [],
      "source": [
        "def get_training_parameters():\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=experiment_info[\"hyperparameters_training\"][\"learning_rate\"],\n",
        "        )\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(\n",
        "        optimizer, \n",
        "        step_size=experiment_info[\"hyperparameters_training\"][\"scheduler_step_size\"], \n",
        "        gamma=experiment_info[\"hyperparameters_training\"][\"scheduler_gamma\"],\n",
        "        )\n",
        "    return criterion, optimizer, exp_lr_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {
        "id": "IHy4ifv6s1yK"
      },
      "outputs": [],
      "source": [
        "def prep_to_merge(labels_predicted, splits):\n",
        "    df = []\n",
        "    for i in range(splits):\n",
        "        predictions1=pd.DataFrame.from_dict(labels_predicted[str(i)],orient='index').transpose() \n",
        "        df.append(predictions1)\n",
        "\n",
        "    df_all = pd.concat(df)\n",
        "    df_all['path'] = df_all['path'].apply(lambda x: x[0])\n",
        "    return df_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21Vc-VIr_IYk"
      },
      "source": [
        "## Seting up things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JQr89Fhv_IYl"
      },
      "outputs": [],
      "source": [
        "# Set up seeds for reproducibility\n",
        "np.random.seed(experiment_info[\"random_seeds\"][\"numpy_seed\"])\n",
        "torch.manual_seed(experiment_info[\"random_seeds\"][\"torch_seed\"])\n",
        "torch.cuda.manual_seed_all(experiment_info[\"random_seeds\"][\"cuda_seed\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "M51baF3B_IYn"
      },
      "outputs": [],
      "source": [
        "dir_data = 'data/task2/train_data/'\n",
        "# Read the annotations file into a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhxmBea3_IYo",
        "outputId": "0d95ee4e-14ee-4880-eecf-f5e95c9078d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/task2/images_by_class does not exist\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [00:15<00:00, 3158.18it/s]\n"
          ]
        }
      ],
      "source": [
        "# Check if the directory exists, to recreate it instead of messing it up\n",
        "target_dir = 'data/task2/images_by_class'\n",
        "\n",
        "del_dir_if_exist(target_dir)\n",
        "\n",
        "make_dataset_folder()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtQ5oHh6XdJA"
      },
      "source": [
        "# Iteration 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSoVcX3tdmlU"
      },
      "source": [
        "## Create datset splits for pseudo-labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOGkTh1scJR_"
      },
      "outputs": [],
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(experiment_info[\"image_processing\"][\"resize\"]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=experiment_info[\"image_processing\"][\"mean\"],\n",
        "                         std=experiment_info[\"image_processing\"][\"std\"]),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iflapmiM4QO"
      },
      "outputs": [],
      "source": [
        "data_dir = 'data/task2/images_by_class'\n",
        "image_dataset_unshuffled = datasets.ImageFolder(data_dir, preprocess)\n",
        "image_dataset = ShuffledImageFolder(image_dataset_unshuffled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r26i0ApmLjLB"
      },
      "outputs": [],
      "source": [
        "splits = 10\n",
        "data_sizes, loaders, device = create_dataset_splits(splits, image_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1WLWbz0VBiP"
      },
      "source": [
        "### sanity check\n",
        "See how many labels are in each training set\n",
        "Visualize the distribution of labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrjSwJ29xbgv",
        "outputId": "6736d904-0171-410a-9774-726ea84f64a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45024\n",
            "5024\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(len(loaders[0][\"train\"])*batch_size)\n",
        "print(len(loaders[0][\"labeling\"])*batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GIbW5b8TW-B",
        "outputId": "f8d448c9-864d-4df6-d739-070cf41ceec3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [01:23<00:00, 16.77it/s]\n"
          ]
        }
      ],
      "source": [
        "# sanity check, verify the distribution of labels in training and labeling sets\n",
        "labels_for_distribution = [label_batch for _,label_batch,_ in tqdm(loaders[0][\"train\"])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkOog_OqUEON"
      },
      "outputs": [],
      "source": [
        "labels_distrib = torch.cat(labels_for_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "z1Mj6kPMuCPN",
        "outputId": "7eea9428-09c3-49f3-fbfd-6d84b798d4a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Distribution of Labels')"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "counts, bins, patches = plt.hist(labels_distrib, bins = 100)\n",
        "\n",
        "# Set x-axis label\n",
        "plt.xlabel('Labels')\n",
        "plt.xticks(bins, bins.astype(int))\n",
        "# Set y-axis label\n",
        "plt.ylabel('Number of Observations')\n",
        "\n",
        "# Set plot title\n",
        "plt.title('Distribution of Labels')  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgSVdENHZedr"
      },
      "source": [
        "## Train on the remaining data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "eb68d62d599a462f92aaca0ab0b854e3",
            "6eed1376f1f24e8abbe2d4ea0ca3fcb1",
            "07c37bb6b22b4c8abe8d16b61e28e1db",
            "15a23332521441cab29f7acedc079b16",
            "816420dd910b475faa99a8e9ef6eebd4",
            "71a16d1906924e738fc1baa304d94811",
            "a74028e913de43d3b1eae97433d91882",
            "07868acd1960497680b904b81fd0eaf9",
            "a56ed7ebe21c4011a23a63cad606d40c",
            "ba437891973d4629b4881ccb11f044ae",
            "a75d7f60d1c24475a1540bc1cfe94f7b"
          ]
        },
        "id": "C69DiUz3ZkEV",
        "outputId": "2eab0188-0052-48d7-de51-10af01ec0845"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb68d62d599a462f92aaca0ab0b854e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=False)\n",
            "  (1): Linear(in_features=1280, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Download a pytorch MobileNet pretrained model\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "# change the Linear output to fit our dataset ( because model initially has 1000 classes)\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "# Setting hyperparameters\n",
        "\n",
        "model = model.to(device)\n",
        "print(model.classifier)\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Mprko1yaAI-"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=experiment_info[\"hyperparameters_training\"][\"learning_rate\"],\n",
        "    )\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(\n",
        "    optimizer, \n",
        "    step_size=experiment_info[\"hyperparameters_training\"][\"scheduler_step_size\"], \n",
        "    gamma=experiment_info[\"hyperparameters_training\"][\"scheduler_gamma\"],\n",
        "    )\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmCqNZ-saHr3",
        "outputId": "badfeed4-5771-4c1f-eca6-47f93af79d12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:26<00:00,  5.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 72.5423 Acc: 13.7484\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 45.0769 Acc: 19.0071\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 36.1820 Acc: 20.9026\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 29.3312 Acc: 22.5622\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 23.8855 Acc: 24.1471\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 19.0713 Acc: 25.5572\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 15.4470 Acc: 26.7960\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 8.7784 Acc: 29.4200\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 6.8999 Acc: 30.1770\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 5.9939 Acc: 30.5394\n",
            "\n",
            "Training complete in 42m 54s\n",
            "Best val Acc: 30.539446\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = train_model(model, \n",
        "                    exp_lr_scheduler, \n",
        "                    optimizer, \n",
        "                    criterion,  \n",
        "                    data_sizes[\"train\"], \n",
        "                    loaders[0][\"train\"], \n",
        "                    num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8xaVIRwm8PG"
      },
      "source": [
        "## Repeat process for the other iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIZWJhNplVOl"
      },
      "outputs": [],
      "source": [
        "# because unlike the missing labels example, the iterations are independent from eachother\n",
        "# We can train all the models at once, than make predictions for all the labels at once\n",
        "# this makes the things simpler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5s4zlU1j_Py"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), '/gdrive/MyDrive/checkpoints/noisy_labels/model_it_0.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq3Ax7_Pm6By",
        "outputId": "8328961f-1389-437e-f643-02c9b3034311"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.3708 Acc: 0.3042\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.4221 Acc: 0.3113\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.4296 Acc: 0.3149\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.4003 Acc: 0.3369\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.4245 Acc: 0.3028\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.4086 Acc: 0.3220\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.4538 Acc: 0.3298\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.3897 Acc: 0.3390\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.4531 Acc: 0.3177\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.4248 Acc: 0.3383\n",
            "\n",
            "Training complete in 43m 0s\n",
            "Best val Acc: 0.339019\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5652 Acc: 0.2687\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5455 Acc: 0.2694\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5926 Acc: 0.2608\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5716 Acc: 0.2580\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5509 Acc: 0.2523\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5032 Acc: 0.2587\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5528 Acc: 0.2601\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5674 Acc: 0.2715\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:15<00:00,  5.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5859 Acc: 0.2694\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5112 Acc: 0.2559\n",
            "\n",
            "Training complete in 42m 32s\n",
            "Best val Acc: 0.271500\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.1574 Acc: 0.2992\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.2140 Acc: 0.3021\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.2206 Acc: 0.2822\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.1759 Acc: 0.2878\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.1663 Acc: 0.2999\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.2137 Acc: 0.2978\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.2461 Acc: 0.2701\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:14<00:00,  5.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.1719 Acc: 0.3205\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.2607 Acc: 0.2857\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.1828 Acc: 0.2935\n",
            "\n",
            "Training complete in 42m 33s\n",
            "Best val Acc: 0.320540\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5430 Acc: 0.3326\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.6434 Acc: 0.3603\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5310 Acc: 0.3497\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.6065 Acc: 0.3319\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5749 Acc: 0.3539\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5546 Acc: 0.3490\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5303 Acc: 0.3632\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.6104 Acc: 0.3340\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5767 Acc: 0.3653\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.5474 Acc: 0.3547\n",
            "\n",
            "Training complete in 42m 58s\n",
            "Best val Acc: 0.365316\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 149.9253 Acc: 0.4030\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 149.8417 Acc: 0.4108\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 149.9271 Acc: 0.4200\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 149.9143 Acc: 0.4335\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 149.9639 Acc: 0.4172\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 149.8806 Acc: 0.4193\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 149.9342 Acc: 0.4151\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 149.9048 Acc: 0.4080\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 149.9162 Acc: 0.3980\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 149.8818 Acc: 0.4271\n",
            "\n",
            "Training complete in 42m 57s\n",
            "Best val Acc: 0.433547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.7060 Acc: 0.3689\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.6360 Acc: 0.3234\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.6773 Acc: 0.3369\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.6889 Acc: 0.3383\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.6507 Acc: 0.3525\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.6708 Acc: 0.3333\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.6872 Acc: 0.3184\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.6516 Acc: 0.3198\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.6673 Acc: 0.3276\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.6903 Acc: 0.3611\n",
            "\n",
            "Training complete in 42m 51s\n",
            "Best val Acc: 0.368870\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.2038 Acc: 0.3362\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.1416 Acc: 0.3326\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.1803 Acc: 0.3362\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.1441 Acc: 0.3170\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.1624 Acc: 0.3227\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.1674 Acc: 0.3291\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.1685 Acc: 0.3468\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.1829 Acc: 0.3255\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.1622 Acc: 0.3390\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.1886 Acc: 0.3177\n",
            "\n",
            "Training complete in 43m 9s\n",
            "Best val Acc: 0.346837\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.8979 Acc: 0.2402\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.9184 Acc: 0.2082\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:19<00:00,  5.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.8989 Acc: 0.2701\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.8834 Acc: 0.2274\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.8732 Acc: 0.2459\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.8074 Acc: 0.2530\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.8743 Acc: 0.2672\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.8564 Acc: 0.2367\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.8534 Acc: 0.2360\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.44it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.8483 Acc: 0.2523\n",
            "\n",
            "Training complete in 43m 8s\n",
            "Best val Acc: 0.270078\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.2236 Acc: 0.2345\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.2041 Acc: 0.2537\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.2436 Acc: 0.2687\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.2801 Acc: 0.2495\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.2507 Acc: 0.2900\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.2444 Acc: 0.2509\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.1874 Acc: 0.2580\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.1854 Acc: 0.2786\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.2250 Acc: 0.2679\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1407/1407 [04:17<00:00,  5.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.2056 Acc: 0.2694\n",
            "\n",
            "Training complete in 42m 57s\n",
            "Best val Acc: 0.289979\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(1,10): \n",
        "    model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "    model.classifier[1] = nn.Linear(1280, 100)\n",
        "    model = model.to(device) \n",
        "    model = train_model(model, \n",
        "                    exp_lr_scheduler, \n",
        "                    optimizer, \n",
        "                    criterion,  \n",
        "                    data_sizes[\"train\"], \n",
        "                    loaders[i][\"train\"], \n",
        "                    num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])\n",
        "    torch.save(model.state_dict(), f'/gdrive/MyDrive/checkpoints/noisy_labels/model_it_{i}.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsTeJsCocwyt"
      },
      "source": [
        "Save the labels that will be used for relabling in a file, such that you can leave the training of the model alone, and later when you return, even if the session has finished, you have the models trained and you know which are the images that need new labels. And you can use the checkpoints to predict them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXT_CNZKny20",
        "outputId": "6af4be55-4996-48f9-9617-4d58fa769aa2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:23<00:00,  8.37s/it]\n"
          ]
        }
      ],
      "source": [
        "paths_labeling = {}\n",
        "for i in tqdm(range(10)):\n",
        "    paths_labeling[i] = []\n",
        "    for images, labels, paths in loaders[i]['labeling']:\n",
        "        paths_labeling[i].append(paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z4e4GA3o37v"
      },
      "outputs": [],
      "source": [
        "# Save the experiment information to a JSON file\n",
        "with open(f'/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_images_for_models.json', 'w') as f:\n",
        "    json.dump(paths_labeling, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K66q0bu_NGt"
      },
      "source": [
        "## Predict new labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qtPuBm_OwUV"
      },
      "outputs": [],
      "source": [
        "# Read the json of the labels that need to be predicted\n",
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_images_for_models.json', 'r') as f:\n",
        "    paths_labeling = json.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4o4h4r5bseB",
        "outputId": "fce9abfa-4ba3-49c8-ccfd-e823bbdd40e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 164.74it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:31<00:00, 157.75it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 163.49it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 163.92it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:31<00:00, 156.30it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:31<00:00, 161.24it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 161.34it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 161.48it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:31<00:00, 160.84it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 5000/5000 [00:30<00:00, 162.56it/s]\n"
          ]
        }
      ],
      "source": [
        "# create new dataset for pseudo-labeling\n",
        "all_predicted_labels = {}\n",
        "for iteration, paths in paths_labeling.items(): \n",
        "    paths_list = batches_to_list(paths)\n",
        "    dataset = ImageDataset(paths_list)    \n",
        "    labeling_loader = DataLoader(dataset, batch_size=1)\n",
        "    PATH = f'/gdrive/MyDrive/checkpoints/noisy_labels/model_it_{iteration}.pt'\n",
        "    model = load_model(PATH, 'cuda')\n",
        "    all_predicted_labels[iteration] = generate_new_labels(model, labeling_loader, 'cuda')  \n",
        "    # 00:33<00:00, 149.59it/s on standard GPU\n",
        "    # [00:18<02:44, 27.31it/s] on standard TPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os7BEdDFiI_0",
        "outputId": "d85f068c-fc7a-427e-e510-51676b1c8d22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('data/task2/images_by_class/3/1383.jpeg',)\n",
            "0.3645797073841095\n",
            "19\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "# Sanity check\n",
        "for i in all_predicted_labels.items():\n",
        "    print(i[1]['path'][0])\n",
        "    print(i[1]['confidence'][0])\n",
        "    print(i[1]['label_predicted'][0])\n",
        "    print(i[1]['existing_label'][0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuNeoaTNhFEZ"
      },
      "outputs": [],
      "source": [
        "# Save the new predictions and the old predictions in a file for future \n",
        "with open(f'/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_first_iteration.json', 'w') as f:\n",
        "    json.dump(all_predicted_labels, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5GTYn7k4WzO"
      },
      "source": [
        "## Analyze results pseudo-labeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgF62cD4h2JY"
      },
      "source": [
        "I have created new predictions for the labels. But I have only 1 label extra label for each prediction. In order to figure out which labels are correct and which ones are not. I want to use the confidence too. Not only rely on more sessions of pseudo-labeling. Because 1 session costs me about 6 hours anyway. Using the confidence for labeling would be to use a rule similar to these ones: If the labels have the same label and there is a high confidence, I will consider the previous label as valid. If the labels are similar and the confidence is very low, I will try to run this label in another session of relabeling. If the label is different but the confidence is very low, it is probably my mistake. But if the label is different and the confidence is very high. I should probably try to create a new label for that image in another session. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fwLsMoYz4pCh"
      },
      "outputs": [],
      "source": [
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_first_iteration.json', 'r') as f:\n",
        "    all_predicted_labels = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmzJre9veAf5"
      },
      "source": [
        " There are 10 splits of the data, each one has a corresponding key in this dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoKd2zR1tXc3",
        "outputId": "22d7d35d-a32d-42a6-961b-1560cceaa957"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_predicted_labels.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zbom3Zmtf_4",
        "outputId": "e8b7be2e-96d8-4c1a-f5f8-dcd998379320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                            path confidence existing_label  \\\n",
            "0       [data/task2/images_by_class/3/1383.jpeg]    0.36458              3   \n",
            "1     [data/task2/images_by_class/76/29313.jpeg]   0.098717             76   \n",
            "2     [data/task2/images_by_class/30/12277.jpeg]   0.203538             30   \n",
            "3     [data/task2/images_by_class/59/22781.jpeg]    0.13695             59   \n",
            "4     [data/task2/images_by_class/88/34072.jpeg]   0.259119             88   \n",
            "...                                          ...        ...            ...   \n",
            "4995  [data/task2/images_by_class/56/21497.jpeg]   0.544975             56   \n",
            "4996  [data/task2/images_by_class/32/12788.jpeg]   0.115707             32   \n",
            "4997   [data/task2/images_by_class/22/8574.jpeg]   0.101662             22   \n",
            "4998  [data/task2/images_by_class/38/14964.jpeg]   0.125019             38   \n",
            "4999  [data/task2/images_by_class/82/48602.jpeg]   0.089067             82   \n",
            "\n",
            "     label_predicted  \n",
            "0                 19  \n",
            "1                 59  \n",
            "2                 34  \n",
            "3                 61  \n",
            "4                 34  \n",
            "...              ...  \n",
            "4995              34  \n",
            "4996              87  \n",
            "4997               7  \n",
            "4998              44  \n",
            "4999              19  \n",
            "\n",
            "[5000 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "# Load the predicted labels and confidence scores\n",
        "iteration = '0'\n",
        "predictions = pd.DataFrame.from_dict(all_predicted_labels[iteration],orient='index').transpose()\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyeK6Ll-t09m",
        "outputId": "27533cb1-f1ac-4032-a0b1-7b5de250915b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How many labels have the same value after relabling ?\n",
            "False    4939\n",
            "True       61\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"How many labels have the same value after relabling ?\")\n",
        "print((predictions['existing_label'] == predictions['label_predicted']).value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "X-9AbJrHugPA",
        "outputId": "48821f09-ad15-48c2-b28d-d24b31d542aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the confidence of those ?\n",
            "191     1.041596\n",
            "220     0.176706\n",
            "226     0.309039\n",
            "272     1.208265\n",
            "378     1.241412\n",
            "          ...   \n",
            "4771    0.202634\n",
            "4789    0.619976\n",
            "4853    0.182252\n",
            "4931    0.596758\n",
            "4982    0.264925\n",
            "Name: confidence, Length: 61, dtype: object\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f04bec745e0>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"What is the confidence of those ?\")\n",
        "print(predictions[predictions['existing_label'] == predictions['label_predicted']]['confidence'])\n",
        "predictions[predictions['existing_label'] == predictions['label_predicted']]['confidence'].plot.hist(bins = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "GZqnYaMM58Nf",
        "outputId": "95130f2f-03c2-46c7-bbd7-cc65a22bd2f2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c3975d65-8304-46e5-9042-58ab15e4b130\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>confidence</th>\n",
              "      <th>existing_label</th>\n",
              "      <th>label_predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>[data/task2/images_by_class/19/7468.jpeg]</td>\n",
              "      <td>1.041596</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>[data/task2/images_by_class/30/12107.jpeg]</td>\n",
              "      <td>1.208265</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>[data/task2/images_by_class/34/13499.jpeg]</td>\n",
              "      <td>1.241412</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>945</th>\n",
              "      <td>[data/task2/images_by_class/92/35180.jpeg]</td>\n",
              "      <td>11.464233</td>\n",
              "      <td>92</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1176</th>\n",
              "      <td>[data/task2/images_by_class/63/23960.jpeg]</td>\n",
              "      <td>1.529731</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1253</th>\n",
              "      <td>[data/task2/images_by_class/11/4422.jpeg]</td>\n",
              "      <td>1.074163</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1356</th>\n",
              "      <td>[data/task2/images_by_class/34/13544.jpeg]</td>\n",
              "      <td>0.647533</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1409</th>\n",
              "      <td>[data/task2/images_by_class/17/42317.jpeg]</td>\n",
              "      <td>1.304711</td>\n",
              "      <td>17</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1506</th>\n",
              "      <td>[data/task2/images_by_class/38/15092.jpeg]</td>\n",
              "      <td>0.958325</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1591</th>\n",
              "      <td>[data/task2/images_by_class/19/7451.jpeg]</td>\n",
              "      <td>0.862999</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1908</th>\n",
              "      <td>[data/task2/images_by_class/7/3188.jpeg]</td>\n",
              "      <td>0.658398</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020</th>\n",
              "      <td>[data/task2/images_by_class/34/13546.jpeg]</td>\n",
              "      <td>0.983313</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2417</th>\n",
              "      <td>[data/task2/images_by_class/34/13435.jpeg]</td>\n",
              "      <td>1.450222</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2649</th>\n",
              "      <td>[data/task2/images_by_class/22/8810.jpeg]</td>\n",
              "      <td>0.520593</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2836</th>\n",
              "      <td>[data/task2/images_by_class/34/13624.jpeg]</td>\n",
              "      <td>0.652207</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3122</th>\n",
              "      <td>[data/task2/images_by_class/34/13490.jpeg]</td>\n",
              "      <td>1.297747</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3125</th>\n",
              "      <td>[data/task2/images_by_class/21/44836.jpeg]</td>\n",
              "      <td>0.701892</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3262</th>\n",
              "      <td>[data/task2/images_by_class/44/17250.jpeg]</td>\n",
              "      <td>0.576789</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3371</th>\n",
              "      <td>[data/task2/images_by_class/19/7640.jpeg]</td>\n",
              "      <td>1.699485</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3980</th>\n",
              "      <td>[data/task2/images_by_class/11/4320.jpeg]</td>\n",
              "      <td>1.343193</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4299</th>\n",
              "      <td>[data/task2/images_by_class/34/13596.jpeg]</td>\n",
              "      <td>0.848232</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4789</th>\n",
              "      <td>[data/task2/images_by_class/34/48202.jpeg]</td>\n",
              "      <td>0.619976</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4931</th>\n",
              "      <td>[data/task2/images_by_class/34/13542.jpeg]</td>\n",
              "      <td>0.596758</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3975d65-8304-46e5-9042-58ab15e4b130')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c3975d65-8304-46e5-9042-58ab15e4b130 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c3975d65-8304-46e5-9042-58ab15e4b130');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            path confidence existing_label  \\\n",
              "191    [data/task2/images_by_class/19/7468.jpeg]   1.041596             19   \n",
              "272   [data/task2/images_by_class/30/12107.jpeg]   1.208265             30   \n",
              "378   [data/task2/images_by_class/34/13499.jpeg]   1.241412             34   \n",
              "945   [data/task2/images_by_class/92/35180.jpeg]  11.464233             92   \n",
              "1176  [data/task2/images_by_class/63/23960.jpeg]   1.529731             63   \n",
              "1253   [data/task2/images_by_class/11/4422.jpeg]   1.074163             11   \n",
              "1356  [data/task2/images_by_class/34/13544.jpeg]   0.647533             34   \n",
              "1409  [data/task2/images_by_class/17/42317.jpeg]   1.304711             17   \n",
              "1506  [data/task2/images_by_class/38/15092.jpeg]   0.958325             38   \n",
              "1591   [data/task2/images_by_class/19/7451.jpeg]   0.862999             19   \n",
              "1908    [data/task2/images_by_class/7/3188.jpeg]   0.658398              7   \n",
              "2020  [data/task2/images_by_class/34/13546.jpeg]   0.983313             34   \n",
              "2417  [data/task2/images_by_class/34/13435.jpeg]   1.450222             34   \n",
              "2649   [data/task2/images_by_class/22/8810.jpeg]   0.520593             22   \n",
              "2836  [data/task2/images_by_class/34/13624.jpeg]   0.652207             34   \n",
              "3122  [data/task2/images_by_class/34/13490.jpeg]   1.297747             34   \n",
              "3125  [data/task2/images_by_class/21/44836.jpeg]   0.701892             21   \n",
              "3262  [data/task2/images_by_class/44/17250.jpeg]   0.576789             44   \n",
              "3371   [data/task2/images_by_class/19/7640.jpeg]   1.699485             19   \n",
              "3980   [data/task2/images_by_class/11/4320.jpeg]   1.343193             11   \n",
              "4299  [data/task2/images_by_class/34/13596.jpeg]   0.848232             34   \n",
              "4789  [data/task2/images_by_class/34/48202.jpeg]   0.619976             34   \n",
              "4931  [data/task2/images_by_class/34/13542.jpeg]   0.596758             34   \n",
              "\n",
              "     label_predicted  \n",
              "191               19  \n",
              "272               30  \n",
              "378               34  \n",
              "945               92  \n",
              "1176              63  \n",
              "1253              11  \n",
              "1356              34  \n",
              "1409              17  \n",
              "1506              38  \n",
              "1591              19  \n",
              "1908               7  \n",
              "2020              34  \n",
              "2417              34  \n",
              "2649              22  \n",
              "2836              34  \n",
              "3122              34  \n",
              "3125              21  \n",
              "3262              44  \n",
              "3371              19  \n",
              "3980              11  \n",
              "4299              34  \n",
              "4789              34  \n",
              "4931              34  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "predictions_same_label = predictions[predictions['existing_label'] == predictions['label_predicted']]\n",
        "predictions_correct_label = predictions_same_label[predictions_same_label['confidence']>0.5]\n",
        "display(predictions_correct_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "BFIOtoxFvLjz",
        "outputId": "31b80c1c-bce4-4ad0-f457-dc74e7c95b62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f04beb3ef10>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "predictions[predictions['existing_label'] != predictions['label_predicted']]['confidence'].plot.hist(bins = 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZy2StjD5PgP"
      },
      "source": [
        "Most of the predictions have very low score, even if they are right or wrong. \n",
        "But a very small number of images have a high confidence of being correct or incorrect. \n",
        "We would take those images with a very high confidence of being right from each iteration and put them aside as true. Than we will repeat the process of forgetting some images and trying to train on the remaining images. But now we have a little bit better model, since it has some images for which we have a high confidence that they are correct. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R2wBaePAfZj"
      },
      "source": [
        "## Repeat thresholding pseudo-labels for the following iterations. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "O5ghSTjFAkIW"
      },
      "outputs": [],
      "source": [
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_first_iteration.json', 'r') as f:\n",
        "    all_predicted_labels = json.load(f)\n",
        "# Load the predicted labels and confidence scores\n",
        "correct_labels_df = []\n",
        "for i in range(10):\n",
        "    predictions=pd.DataFrame.from_dict(all_predicted_labels[str(i)],orient='index').transpose() \n",
        "    predictions_same_label = predictions[predictions['existing_label'] == predictions['label_predicted']]\n",
        "    predictions_correct_label = predictions_same_label[predictions_same_label['confidence']>0.5] \n",
        "    correct_labels_df.append(predictions_correct_label)\n",
        "df_correct = pd.concat(correct_labels_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlOHKGEyCOle",
        "outputId": "f1eee80c-e764-44ec-9a3b-48a2670b8453"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "431"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df_correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvwdmh-BC2_5"
      },
      "source": [
        "NOTE: After the first iteration of the algorithm we found 431 images to have correct labels with high confidence. This means we should repeat this process 100 times. Which would take a very large amount of time. But hopefully the futher iterations will become faster as we find that more images are correctly labeled. \n",
        "\n",
        "NOTE: Also, because we still have the previous results from the first iteration. If we get the same labels on the second or third iteration, we are much more likely to believe that the respective labels are true, and we can include them in the \"pseudo ground truth dataset\" even if they don't have a high confidence, but they have the same label on multiple runs. \n",
        "\n",
        "NOTE: Finally. In the end we can try again the whole experiment from the beginning. The expectation would be that we would find much fewer wrongly labeled images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_emV0k5Dv9P"
      },
      "source": [
        "# Iteration 2\n",
        "considering previously found labels as true, and not reconsidering to label them anymore. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYWytvbjLeJi",
        "outputId": "be313eb5-bf78-40ed-ad9f-f4b91208f8db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/task2/images_by_class has been deleted\n"
          ]
        }
      ],
      "source": [
        "target_dir = 'data/task2/images_by_class'\n",
        "\n",
        "del_dir_if_exist(target_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNK5li4VMQae",
        "outputId": "c6c1f104-4f1a-4302-fc86-71c21db5abee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/task2/training_confident does not exist\n"
          ]
        }
      ],
      "source": [
        "# Check if the directory exists, to recreate it instead of messing it up\n",
        "target_dir = 'data/task2/training_confident'\n",
        "\n",
        "del_dir_if_exist(target_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ri4BEas4HE8u",
        "outputId": "7804a162-7fdf-4073-cbf1-95944c72323e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [00:10<00:00, 4666.24it/s]\n"
          ]
        }
      ],
      "source": [
        "make_dataset_folder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKxIxGUz5a8X",
        "outputId": "c4508c7a-e283-4a50-fa8a-52f148cab6af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 431/431 [00:00<00:00, 7773.98it/s]\n"
          ]
        }
      ],
      "source": [
        "# Train on 2 datasets at the same time, one with high confidence labels\n",
        "# this dataset will only be used during training, never forgetting labels\n",
        "# the second dataset will iteratively forget labels and train on the remaining labels.\n",
        "# Define the base directory\n",
        "target_dir = 'data/task2/training_confident'\n",
        "# Iterate over the rows in the DataFrame\n",
        "for _, row in tqdm(df_correct.iterrows(), total=df_correct.shape[0]):\n",
        "    # Extract the path and class from the row\n",
        "    path = row['path'][0]\n",
        "    label = row['existing_label'] \n",
        "\n",
        "    # Create the directory for the class\n",
        "    class_dir = f'{target_dir}/{label}'\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    \n",
        "    # move the file to the class directory\n",
        "    shutil.move(f\"{path}\", class_dir)\n",
        "    \n",
        "\n",
        "# we move the images because we don't want them to remain in the dataset for relabeling.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7KktFggEt_p",
        "outputId": "bd30b0fe-b32f-4c46-b37c-f848881e9ce2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# TODO: create the 2 datasets\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(experiment_info[\"image_processing\"][\"resize\"]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=experiment_info[\"image_processing\"][\"mean\"],\n",
        "                         std=experiment_info[\"image_processing\"][\"std\"]),\n",
        "    ])\n",
        "\n",
        "data_dir_train = 'data/task2/training_confident'\n",
        "image_dataset_train_only = datasets.ImageFolder(data_dir_train, preprocess) \n",
        "dataloader_train_only  = DataLoader(\n",
        "    image_dataset_train_only, \n",
        "    batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"],\n",
        "    shuffle = experiment_info[\"hyperparameters_data\"][\"shuffle_dataloader\"], \n",
        "    num_workers = experiment_info[\"hyperparameters_data\"][\"num_workers\"]\n",
        "    )\n",
        "\n",
        "class_names = image_dataset_train_only.classes\n",
        "dataset_size = len(image_dataset_train_only)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i8BvAlyH110",
        "outputId": "df09c0ec-5d9e-4621-b525-1073b3e4d16a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "431\n"
          ]
        }
      ],
      "source": [
        "print(dataset_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG8vZn7mFmRd"
      },
      "source": [
        "## Create datasets splits for pseudo-labeling round 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHZnbNcrLwtu"
      },
      "outputs": [],
      "source": [
        "data_dir = 'data/task2/images_by_class'\n",
        "image_dataset_unshuffled = datasets.ImageFolder(data_dir, preprocess)\n",
        "image_dataset = ShuffledImageFolder(image_dataset_unshuffled)\n",
        "splits = 2\n",
        "data_sizes, loaders, device = create_dataset_splits(splits, image_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a95SXdQCH8GL",
        "outputId": "eb79f48c-f2ea-4100-e828-1b8059bf5820"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train': 775, 'labeling': 775}\n",
            "num samples train 24800\n"
          ]
        }
      ],
      "source": [
        "print(data_sizes)\n",
        "print(\"num samples train\", data_sizes['train']*batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vohgejEvNNLp"
      },
      "source": [
        "## Train on remaining data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "0b6b0668f3764a7296945547803e2688",
            "c39c58da42ce4c2ca76546673443f0fd",
            "b02ccf11c1714786a67bfca3446e9b9d",
            "284de89238b54efd8d834cfc5cdef224",
            "4d1e4cd757a749bda0f6fbcede536076",
            "dd637c7b9a5645dfb198f017092d098a",
            "afee6006e2324bac8bbc9c291e638617",
            "8e3263acdec647e594b2140e749bd110",
            "4dd784f55bb14f6ba9274ec956ec8061",
            "7adf26ae5d2e430e946d5576135d893a",
            "69db9e55793a4d87bd7b0c96efc53170"
          ]
        },
        "id": "bk_xDk2MNPk7",
        "outputId": "04ce156f-2d44-4405-bc1e-b2d78fea20ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b6b0668f3764a7296945547803e2688",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=False)\n",
            "  (1): Linear(in_features=1280, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Download a pytorch MobileNet pretrained model\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "# change the Linear output to fit our dataset ( because model initially has 1000 classes)\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "# Setting hyperparameters\n",
        "\n",
        "model = model.to(device)\n",
        "print(model.classifier)\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "LRzyP-FuNhhh"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=experiment_info[\"hyperparameters_training\"][\"learning_rate\"],\n",
        "    )\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(\n",
        "    optimizer, \n",
        "    step_size=experiment_info[\"hyperparameters_training\"][\"scheduler_step_size\"], \n",
        "    gamma=experiment_info[\"hyperparameters_training\"][\"scheduler_gamma\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmT0N9O9NUIX",
        "outputId": "1fa0bc4c-7baf-4cee-c213-846f55d6dcf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:09<00:00,  1.42it/s]\n",
            "100%|██████████| 775/775 [02:21<00:00,  5.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 86.6548 Acc: 11.1110\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.00it/s]\n",
            "100%|██████████| 775/775 [02:20<00:00,  5.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 51.9183 Acc: 17.8194\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  4.72it/s]\n",
            "100%|██████████| 775/775 [02:23<00:00,  5.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 39.8597 Acc: 20.5716\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.05it/s]\n",
            "100%|██████████| 775/775 [02:18<00:00,  5.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 31.2823 Acc: 22.5729\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.90it/s]\n",
            "100%|██████████| 775/775 [02:15<00:00,  5.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 24.2291 Acc: 24.5897\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.96it/s]\n",
            "100%|██████████| 775/775 [02:16<00:00,  5.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 18.5088 Acc: 26.2800\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.97it/s]\n",
            "100%|██████████| 775/775 [02:15<00:00,  5.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 13.6184 Acc: 27.8787\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.91it/s]\n",
            "100%|██████████| 775/775 [02:15<00:00,  5.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 7.1740 Acc: 30.4065\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.04it/s]\n",
            "100%|██████████| 775/775 [02:17<00:00,  5.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 5.7742 Acc: 30.9032\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.01it/s]\n",
            "100%|██████████| 775/775 [02:15<00:00,  5.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 5.1758 Acc: 31.1458\n",
            "\n",
            "Training complete in 23m 33s\n",
            "Best val Acc: 31.145806\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = train_model2(model, \n",
        "                    exp_lr_scheduler, \n",
        "                    optimizer, \n",
        "                    criterion,  \n",
        "                    data_sizes[\"train\"], \n",
        "                    dataloader_train_only,\n",
        "                    loaders2[0][\"train\"], \n",
        "                    num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1XhOuuNyNvtU"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), '/gdrive/MyDrive/checkpoints/noisy_labels/model_2_it_0.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3S_tI-1ZNstT"
      },
      "outputs": [],
      "source": [
        "## Repeat for the other half"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wMaIPVTN-KN",
        "outputId": "d388f3b4-f6e4-4f21-e092-7f0f79e9af11"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.00it/s]\n",
            "100%|██████████| 775/775 [02:15<00:00,  5.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 151.0671 Acc: 0.2813\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.14it/s]\n",
            "100%|██████████| 775/775 [02:16<00:00,  5.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 151.0593 Acc: 0.2606\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.15it/s]\n",
            "100%|██████████| 775/775 [02:17<00:00,  5.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 151.0380 Acc: 0.2606\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.04it/s]\n",
            "100%|██████████| 775/775 [02:26<00:00,  5.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.9949 Acc: 0.2839\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  4.90it/s]\n",
            "100%|██████████| 775/775 [02:16<00:00,  5.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 151.0060 Acc: 0.2813\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.79it/s]\n",
            "100%|██████████| 775/775 [02:23<00:00,  5.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 151.0038 Acc: 0.2826\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.73it/s]\n",
            "100%|██████████| 775/775 [02:20<00:00,  5.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.9881 Acc: 0.2710\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.03it/s]\n",
            "100%|██████████| 775/775 [02:16<00:00,  5.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 151.0254 Acc: 0.2594\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  6.00it/s]\n",
            "100%|██████████| 775/775 [02:16<00:00,  5.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 151.0795 Acc: 0.2619\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.96it/s]\n",
            "100%|██████████| 775/775 [02:20<00:00,  5.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 151.0896 Acc: 0.2865\n",
            "\n",
            "Training complete in 23m 34s\n",
            "Best val Acc: 0.286452\n"
          ]
        }
      ],
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "model = model.to(device) \n",
        "model = train_model2(model, \n",
        "                exp_lr_scheduler, \n",
        "                optimizer, \n",
        "                criterion,  \n",
        "                data_sizes[\"train\"], \n",
        "                dataloader_train_only,\n",
        "                loaders2[1][\"train\"], \n",
        "                num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])\n",
        "torch.save(model.state_dict(), f'/gdrive/MyDrive/checkpoints/noisy_labels/model_2_it_{1}.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r4x6giiTYqn"
      },
      "source": [
        "## save labels for later predicting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwmcZrQyWISg",
        "outputId": "d6b882c5-ae21-4f56-fb2f-b60589d24e79"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 775/775 [00:39<00:00, 19.54it/s]\n",
            "100%|██████████| 775/775 [00:38<00:00, 19.96it/s]\n"
          ]
        }
      ],
      "source": [
        "paths_labeling = {}\n",
        "for i in range(splits):\n",
        "    paths_labeling[i] = []\n",
        "    for images, labels, paths in tqdm(loaders2[i]['labeling']):\n",
        "        paths_labeling[i].append(paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "mi2U16qVWKJY"
      },
      "outputs": [],
      "source": [
        "# Save the experiment information to a JSON file\n",
        "with open(f'/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_images_for_models_2.json', 'w') as f:\n",
        "    json.dump(paths_labeling, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9TaBnOkWX-s"
      },
      "source": [
        "## Predict new labels 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "cM3GCPEaWZ0V"
      },
      "outputs": [],
      "source": [
        "# Read the json of the labels that need to be predicted\n",
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_images_for_models_2.json', 'r') as f:\n",
        "    paths_labeling = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfqmTKpzdn3n",
        "outputId": "dd3d7e24-d4b6-4bc0-bdf1-37925be9208a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 24784/24784 [02:41<00:00, 153.12it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 24784/24784 [02:41<00:00, 153.32it/s]\n"
          ]
        }
      ],
      "source": [
        "# create new dataset for pseudo-labeling\n",
        "all_predicted_labels = {}\n",
        "for iteration, paths in paths_labeling.items(): \n",
        "    paths_list = batches_to_list(paths)\n",
        "    dataset = ImageDataset(paths_list)    \n",
        "    labeling_loader = DataLoader(dataset, batch_size=1)\n",
        "    PATH = f'/gdrive/MyDrive/checkpoints/noisy_labels/model_2_it_{iteration}.pt'\n",
        "    model = load_model(PATH, 'cuda')\n",
        "    all_predicted_labels[iteration] = generate_new_labels(model, labeling_loader, 'cuda')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljiMdeEFdsM1",
        "outputId": "665b9ca1-c80f-44f0-db20-8cb3d8cc9cfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('data/task2/images_by_class/81/31055.jpeg',)\n",
            "0.14650920033454895\n",
            "90\n",
            "81\n"
          ]
        }
      ],
      "source": [
        "# Sanity check\n",
        "for i in all_predicted_labels.items():\n",
        "    print(i[1]['path'][0])\n",
        "    print(i[1]['confidence'][0])\n",
        "    print(i[1]['label_predicted'][0])\n",
        "    print(i[1]['existing_label'][0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "EakLd6JddzKw"
      },
      "outputs": [],
      "source": [
        "# Save the new predictions and the old predictions in a file for future \n",
        "with open(f'/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_second_iteration.json', 'w') as f:\n",
        "    json.dump(all_predicted_labels, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm7WbpH7d2Y7"
      },
      "source": [
        "## Analyze pseudo lableing from second iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "uny5rmNHd2AF"
      },
      "outputs": [],
      "source": [
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_second_iteration.json', 'r') as f:\n",
        "    all_predicted_labels = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDDWpwUGd-0b",
        "outputId": "937323a5-1003-4f46-d7c6-715730e51c65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['0', '1'])"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_predicted_labels.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8dYkXFHePtL",
        "outputId": "03cce4e5-ad9c-42e4-cfe6-62e8c0e3ef86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             path confidence existing_label  \\\n",
            "0      [data/task2/images_by_class/81/31055.jpeg]    0.03861             81   \n",
            "1      [data/task2/images_by_class/86/33112.jpeg]   0.122728             86   \n",
            "2      [data/task2/images_by_class/42/43263.jpeg]   0.151605             42   \n",
            "3      [data/task2/images_by_class/80/30852.jpeg]   0.188782             80   \n",
            "4      [data/task2/images_by_class/94/36087.jpeg]   0.088798             94   \n",
            "...                                           ...        ...            ...   \n",
            "24779  [data/task2/images_by_class/27/10476.jpeg]    0.05421             27   \n",
            "24780  [data/task2/images_by_class/69/26650.jpeg]   0.104292             69   \n",
            "24781  [data/task2/images_by_class/67/25549.jpeg]   0.182475             67   \n",
            "24782   [data/task2/images_by_class/14/5501.jpeg]   0.504089             14   \n",
            "24783    [data/task2/images_by_class/5/2113.jpeg]   2.366076              5   \n",
            "\n",
            "      label_predicted  \n",
            "0                  85  \n",
            "1                  40  \n",
            "2                  44  \n",
            "3                  38  \n",
            "4                  61  \n",
            "...               ...  \n",
            "24779              61  \n",
            "24780              34  \n",
            "24781              56  \n",
            "24782               7  \n",
            "24783              19  \n",
            "\n",
            "[24784 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "# Load the predicted labels and confidence scores\n",
        "iteration = '0'\n",
        "predictions = pd.DataFrame.from_dict(all_predicted_labels[iteration],orient='index').transpose()\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06u9mblBeRUt",
        "outputId": "fd170d26-b944-4d99-ee0f-6174bd954ad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How many labels have the same value after relabling ?\n",
            "False    22782\n",
            "True       233\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"How many labels have the same value after relabling ?\")\n",
        "print((predictions['existing_label'] == predictions['label_predicted']).value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "EbY2OOOceTpg",
        "outputId": "3bfd325d-debf-4930-f31e-77cd548a222c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the confidence of those ?\n",
            "69       0.317126\n",
            "86        0.62353\n",
            "387      0.211072\n",
            "620      0.169154\n",
            "740      0.358847\n",
            "           ...   \n",
            "22567    0.059484\n",
            "22649    0.509168\n",
            "22691    0.294431\n",
            "22771    0.052772\n",
            "22881     0.23819\n",
            "Name: confidence, Length: 233, dtype: object\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f047bf614c0>"
            ]
          },
          "execution_count": 228,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"What is the confidence of those ?\")\n",
        "print(predictions[predictions['existing_label'] == predictions['label_predicted']]['confidence'])\n",
        "predictions[predictions['existing_label'] == predictions['label_predicted']]['confidence'].plot.hist(bins = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV0JhS1teWQW",
        "outputId": "c7527493-4750-4fd4-c280-9c4060bf96bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "execution_count": 230,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions_same_label = predictions[predictions['existing_label'] == predictions['label_predicted']]\n",
        "predictions_correct_label = predictions_same_label[predictions_same_label['confidence']>0.5]\n",
        "len(predictions_correct_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "iLJx33B8eYYg",
        "outputId": "3f2033af-a002-4b83-910b-d7baae4e09c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the distribution of labels different from one dataset to another\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f047b959970>"
            ]
          },
          "execution_count": 231,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"the distribution of labels different from one dataset to another\")\n",
        "predictions[predictions['existing_label'] != predictions['label_predicted']]['confidence'].plot.hist(bins = 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHLNQeikefi_"
      },
      "source": [
        "## Repeat thresholding pseudo-labels for the following iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Veirr9GWgKm-"
      },
      "source": [
        "The second iteration has a different process for analyzing the labels, we are not only interested if the new labels have the the same label as the one provided by the dataset. We are also interested to see if the first iteration gives labels equal to the second iteration. And also how high is the confidence, even if the labels are \"wrong\" ( different from dataset), but for which both models provide the same label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "metadata": {
        "id": "kC2CIK45ekNA"
      },
      "outputs": [],
      "source": [
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_second_iteration.json', 'r') as f:\n",
        "    all_predicted_labels2 = json.load(f)\n",
        "# Load the predicted labels and confidence scores\n",
        "correct_labels_df = []\n",
        "for i in range(splits):\n",
        "    predictions=pd.DataFrame.from_dict(all_predicted_labels2[str(i)],orient='index').transpose() \n",
        "    predictions_same_label = predictions[predictions['existing_label'] == predictions['label_predicted']]\n",
        "    predictions_correct_label = predictions_same_label[predictions_same_label['confidence']>0.2] \n",
        "    correct_labels_df.append(predictions_correct_label)\n",
        "df_correct2 = pd.concat(correct_labels_df)\n",
        "df_correct2['path'] = df_correct2['path'].apply(lambda x: x[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmNsoNo2eo6E",
        "outputId": "c84ce2f9-9a46-4ded-a307-d10dbd6301c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "389"
            ]
          },
          "execution_count": 328,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df_correct2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {
        "id": "3em0OOnpjFds"
      },
      "outputs": [],
      "source": [
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_first_iteration.json', 'r') as f:\n",
        "    all_predicted_labels1 = json.load(f)\n",
        "# Load the predicted labels and confidence scores\n",
        "correct_labels_df_1 = []\n",
        "for i in range(10):\n",
        "    predictions=pd.DataFrame.from_dict(all_predicted_labels1[str(i)],orient='index').transpose() \n",
        "    predictions_same_label = predictions[predictions['existing_label'] == predictions['label_predicted']]\n",
        "    predictions_correct_label = predictions_same_label[predictions_same_label['confidence']>0.2] \n",
        "    correct_labels_df_1.append(predictions_correct_label)\n",
        "\n",
        "df_correct_1 = pd.concat(correct_labels_df_1)\n",
        "df_correct_1['path'] = df_correct_1['path'].apply(lambda x: x[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 330,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "x-yiskagje-i",
        "outputId": "d9ee7e9a-8feb-4e9b-a7a1-9131adeafe35"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-83bf2cac-0690-4d68-b953-74a1237a7bad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>confidence</th>\n",
              "      <th>existing_label</th>\n",
              "      <th>label_predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>data/task2/images_by_class/19/7468.jpeg</td>\n",
              "      <td>1.041596</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>data/task2/images_by_class/34/13438.jpeg</td>\n",
              "      <td>0.309039</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>data/task2/images_by_class/30/12107.jpeg</td>\n",
              "      <td>1.208265</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>data/task2/images_by_class/34/13499.jpeg</td>\n",
              "      <td>1.241412</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>945</th>\n",
              "      <td>data/task2/images_by_class/92/35180.jpeg</td>\n",
              "      <td>11.464233</td>\n",
              "      <td>92</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4310</th>\n",
              "      <td>data/task2/images_by_class/50/19476.jpeg</td>\n",
              "      <td>1.982109</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4365</th>\n",
              "      <td>data/task2/images_by_class/52/19993.jpeg</td>\n",
              "      <td>2.803925</td>\n",
              "      <td>52</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4545</th>\n",
              "      <td>data/task2/images_by_class/38/48931.jpeg</td>\n",
              "      <td>3.05493</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4587</th>\n",
              "      <td>data/task2/images_by_class/38/15138.jpeg</td>\n",
              "      <td>1.706635</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4938</th>\n",
              "      <td>data/task2/images_by_class/93/35654.jpeg</td>\n",
              "      <td>2.009159</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>452 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83bf2cac-0690-4d68-b953-74a1237a7bad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-83bf2cac-0690-4d68-b953-74a1237a7bad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-83bf2cac-0690-4d68-b953-74a1237a7bad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                          path confidence existing_label  \\\n",
              "191    data/task2/images_by_class/19/7468.jpeg   1.041596             19   \n",
              "226   data/task2/images_by_class/34/13438.jpeg   0.309039             34   \n",
              "272   data/task2/images_by_class/30/12107.jpeg   1.208265             30   \n",
              "378   data/task2/images_by_class/34/13499.jpeg   1.241412             34   \n",
              "945   data/task2/images_by_class/92/35180.jpeg  11.464233             92   \n",
              "...                                        ...        ...            ...   \n",
              "4310  data/task2/images_by_class/50/19476.jpeg   1.982109             50   \n",
              "4365  data/task2/images_by_class/52/19993.jpeg   2.803925             52   \n",
              "4545  data/task2/images_by_class/38/48931.jpeg    3.05493             38   \n",
              "4587  data/task2/images_by_class/38/15138.jpeg   1.706635             38   \n",
              "4938  data/task2/images_by_class/93/35654.jpeg   2.009159             93   \n",
              "\n",
              "     label_predicted  \n",
              "191               19  \n",
              "226               34  \n",
              "272               30  \n",
              "378               34  \n",
              "945               92  \n",
              "...              ...  \n",
              "4310              50  \n",
              "4365              52  \n",
              "4545              38  \n",
              "4587              38  \n",
              "4938              93  \n",
              "\n",
              "[452 rows x 4 columns]"
            ]
          },
          "execution_count": 330,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_correct_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "s5LJ1aX1jhRZ",
        "outputId": "4d2faed5-7438-499f-e4f4-cb630f123f0c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f66a70d7-ee27-4adc-890b-dc2dd24369da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>confidence</th>\n",
              "      <th>existing_label</th>\n",
              "      <th>label_predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>data/task2/images_by_class/38/15011.jpeg</td>\n",
              "      <td>0.28361</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>data/task2/images_by_class/61/23360.jpeg</td>\n",
              "      <td>0.201992</td>\n",
              "      <td>61</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>data/task2/images_by_class/44/17304.jpeg</td>\n",
              "      <td>2.34158</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>672</th>\n",
              "      <td>data/task2/images_by_class/38/46636.jpeg</td>\n",
              "      <td>2.110656</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>781</th>\n",
              "      <td>data/task2/images_by_class/33/13269.jpeg</td>\n",
              "      <td>0.464516</td>\n",
              "      <td>33</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24508</th>\n",
              "      <td>data/task2/images_by_class/47/39905.jpeg</td>\n",
              "      <td>1.529039</td>\n",
              "      <td>47</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24532</th>\n",
              "      <td>data/task2/images_by_class/29/11629.jpeg</td>\n",
              "      <td>2.1298</td>\n",
              "      <td>29</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24535</th>\n",
              "      <td>data/task2/images_by_class/28/10980.jpeg</td>\n",
              "      <td>0.967887</td>\n",
              "      <td>28</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24556</th>\n",
              "      <td>data/task2/images_by_class/40/15672.jpeg</td>\n",
              "      <td>2.46378</td>\n",
              "      <td>40</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24736</th>\n",
              "      <td>data/task2/images_by_class/47/18197.jpeg</td>\n",
              "      <td>1.007002</td>\n",
              "      <td>47</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>389 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f66a70d7-ee27-4adc-890b-dc2dd24369da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f66a70d7-ee27-4adc-890b-dc2dd24369da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f66a70d7-ee27-4adc-890b-dc2dd24369da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                           path confidence existing_label  \\\n",
              "17     data/task2/images_by_class/38/15011.jpeg    0.28361             38   \n",
              "169    data/task2/images_by_class/61/23360.jpeg   0.201992             61   \n",
              "348    data/task2/images_by_class/44/17304.jpeg    2.34158             44   \n",
              "672    data/task2/images_by_class/38/46636.jpeg   2.110656             38   \n",
              "781    data/task2/images_by_class/33/13269.jpeg   0.464516             33   \n",
              "...                                         ...        ...            ...   \n",
              "24508  data/task2/images_by_class/47/39905.jpeg   1.529039             47   \n",
              "24532  data/task2/images_by_class/29/11629.jpeg     2.1298             29   \n",
              "24535  data/task2/images_by_class/28/10980.jpeg   0.967887             28   \n",
              "24556  data/task2/images_by_class/40/15672.jpeg    2.46378             40   \n",
              "24736  data/task2/images_by_class/47/18197.jpeg   1.007002             47   \n",
              "\n",
              "      label_predicted  \n",
              "17                 38  \n",
              "169                61  \n",
              "348                44  \n",
              "672                38  \n",
              "781                33  \n",
              "...               ...  \n",
              "24508              47  \n",
              "24532              29  \n",
              "24535              28  \n",
              "24556              40  \n",
              "24736              47  \n",
              "\n",
              "[389 rows x 4 columns]"
            ]
          },
          "execution_count": 331,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_correct2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "metadata": {
        "id": "YId3WwTlkXvf"
      },
      "outputs": [],
      "source": [
        "df_all1 = prep_to_merge(all_predicted_labels1, 10)\n",
        "df_all2 = prep_to_merge(all_predicted_labels2, 2)\n",
        "\n",
        "df_all2.rename({\"confidence\":\"confidence2\", \"label_predicted\":\"label_predicted2\"}, inplace=True, axis = 1)\n",
        "df_all2.drop(\"existing_label\", inplace=True,axis=1)\n",
        "merge_df = pd.merge(df_all1, df_all2, on=\"path\", how=\"left\")\n",
        "# we don't have all rows for confidence 2 so we use \"left\" merging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "82ZFbAYTnzVj",
        "outputId": "75aeb41b-60b6-440b-d6d5-3bfb6bef0dae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-773ebc8c-498f-4a80-b06e-322567b5d42a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>confidence</th>\n",
              "      <th>existing_label</th>\n",
              "      <th>label_predicted</th>\n",
              "      <th>confidence2</th>\n",
              "      <th>label_predicted2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/task2/images_by_class/3/1383.jpeg</td>\n",
              "      <td>0.36458</td>\n",
              "      <td>3</td>\n",
              "      <td>19</td>\n",
              "      <td>0.916443</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/task2/images_by_class/76/29313.jpeg</td>\n",
              "      <td>0.098717</td>\n",
              "      <td>76</td>\n",
              "      <td>59</td>\n",
              "      <td>1.734477</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/task2/images_by_class/30/12277.jpeg</td>\n",
              "      <td>0.203538</td>\n",
              "      <td>30</td>\n",
              "      <td>34</td>\n",
              "      <td>0.174259</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/task2/images_by_class/59/22781.jpeg</td>\n",
              "      <td>0.13695</td>\n",
              "      <td>59</td>\n",
              "      <td>61</td>\n",
              "      <td>1.593244</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/task2/images_by_class/88/34072.jpeg</td>\n",
              "      <td>0.259119</td>\n",
              "      <td>88</td>\n",
              "      <td>34</td>\n",
              "      <td>1.78167</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>data/task2/images_by_class/74/28286.jpeg</td>\n",
              "      <td>1.359533</td>\n",
              "      <td>74</td>\n",
              "      <td>50</td>\n",
              "      <td>0.029493</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>data/task2/images_by_class/15/45448.jpeg</td>\n",
              "      <td>3.355858</td>\n",
              "      <td>15</td>\n",
              "      <td>61</td>\n",
              "      <td>3.662585</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>data/task2/images_by_class/8/44196.jpeg</td>\n",
              "      <td>1.073675</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0.305969</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>data/task2/images_by_class/75/28828.jpeg</td>\n",
              "      <td>1.837749</td>\n",
              "      <td>75</td>\n",
              "      <td>39</td>\n",
              "      <td>1.070897</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>data/task2/images_by_class/99/40549.jpeg</td>\n",
              "      <td>3.33866</td>\n",
              "      <td>99</td>\n",
              "      <td>67</td>\n",
              "      <td>2.707126</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-773ebc8c-498f-4a80-b06e-322567b5d42a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-773ebc8c-498f-4a80-b06e-322567b5d42a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-773ebc8c-498f-4a80-b06e-322567b5d42a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                           path confidence existing_label  \\\n",
              "0        data/task2/images_by_class/3/1383.jpeg    0.36458              3   \n",
              "1      data/task2/images_by_class/76/29313.jpeg   0.098717             76   \n",
              "2      data/task2/images_by_class/30/12277.jpeg   0.203538             30   \n",
              "3      data/task2/images_by_class/59/22781.jpeg    0.13695             59   \n",
              "4      data/task2/images_by_class/88/34072.jpeg   0.259119             88   \n",
              "...                                         ...        ...            ...   \n",
              "49995  data/task2/images_by_class/74/28286.jpeg   1.359533             74   \n",
              "49996  data/task2/images_by_class/15/45448.jpeg   3.355858             15   \n",
              "49997   data/task2/images_by_class/8/44196.jpeg   1.073675              8   \n",
              "49998  data/task2/images_by_class/75/28828.jpeg   1.837749             75   \n",
              "49999  data/task2/images_by_class/99/40549.jpeg    3.33866             99   \n",
              "\n",
              "      label_predicted confidence2 label_predicted2  \n",
              "0                  19    0.916443               69  \n",
              "1                  59    1.734477               47  \n",
              "2                  34    0.174259               44  \n",
              "3                  61    1.593244               17  \n",
              "4                  34     1.78167                3  \n",
              "...               ...         ...              ...  \n",
              "49995              50    0.029493               17  \n",
              "49996              61    3.662585               47  \n",
              "49997               0    0.305969               38  \n",
              "49998              39    1.070897               99  \n",
              "49999              67    2.707126               44  \n",
              "\n",
              "[50000 rows x 6 columns]"
            ]
          },
          "execution_count": 333,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merge_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZumSZ5hCm8ns",
        "outputId": "a7527f0a-47e6-43c7-aa63-fd84881a3873"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How many rows have the label_predicted2 = label_predicted 2\n",
            "False    49251\n",
            "True       749\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"How many rows have the label_predicted2 = label_predicted 2\")\n",
        "print((merge_df['label_predicted'] == merge_df['label_predicted2']).value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMg4Og_xoanG"
      },
      "source": [
        "**Wow!** There are much more labels that have the same label prediction from 2 models compared to label predictions from only 1 model and the dataset. We can consider them also as correct and try again the process of training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "nir6eDGuo42r",
        "outputId": "2dc5f5e0-2935-4c34-a321-f2c37cdaf824"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the confidence distribution of those with same label\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f047a89cbb0>"
            ]
          },
          "execution_count": 280,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"What is the confidence distribution of those with same label\")\n",
        "merge_df[merge_df['label_predicted2'] == merge_df['label_predicted']]['confidence'].plot.hist(bins = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "x5m_1cbzqMnY",
        "outputId": "b3b667a4-cccc-42a3-bcee-f4d91038ef0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We can also notice that the confidence is very similar\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f04b0135070>"
            ]
          },
          "execution_count": 281,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"We can also notice that the confidence is very similar\")\n",
        "merge_df[merge_df['label_predicted2'] == merge_df['label_predicted']]['confidence2'].plot.hist(bins = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "noY7G4e9p5HV",
        "outputId": "76778fb4-fa8e-4cfe-fa83-2aff288019ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-282-8bc119427798>:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df_merge_correct = merge_same_label[merge_same_label['confidence']>0.2][merge_same_label['confidence2']>0.2]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-57cf4ea2-0a23-4590-8916-0f6659b4312c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>confidence</th>\n",
              "      <th>existing_label</th>\n",
              "      <th>label_predicted</th>\n",
              "      <th>confidence2</th>\n",
              "      <th>label_predicted2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>data/task2/images_by_class/63/24071.jpeg</td>\n",
              "      <td>0.218912</td>\n",
              "      <td>63</td>\n",
              "      <td>38</td>\n",
              "      <td>0.617343</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>data/task2/images_by_class/6/2774.jpeg</td>\n",
              "      <td>6.238861</td>\n",
              "      <td>6</td>\n",
              "      <td>19</td>\n",
              "      <td>0.323555</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>data/task2/images_by_class/94/45260.jpeg</td>\n",
              "      <td>0.275159</td>\n",
              "      <td>94</td>\n",
              "      <td>38</td>\n",
              "      <td>4.134336</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>data/task2/images_by_class/6/2498.jpeg</td>\n",
              "      <td>6.526378</td>\n",
              "      <td>6</td>\n",
              "      <td>19</td>\n",
              "      <td>2.381141</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>data/task2/images_by_class/49/18982.jpeg</td>\n",
              "      <td>0.599729</td>\n",
              "      <td>49</td>\n",
              "      <td>61</td>\n",
              "      <td>0.650636</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49780</th>\n",
              "      <td>data/task2/images_by_class/66/25295.jpeg</td>\n",
              "      <td>3.268991</td>\n",
              "      <td>66</td>\n",
              "      <td>52</td>\n",
              "      <td>1.482898</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49832</th>\n",
              "      <td>data/task2/images_by_class/46/17926.jpeg</td>\n",
              "      <td>2.829008</td>\n",
              "      <td>46</td>\n",
              "      <td>18</td>\n",
              "      <td>2.943778</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49838</th>\n",
              "      <td>data/task2/images_by_class/2/40844.jpeg</td>\n",
              "      <td>3.427066</td>\n",
              "      <td>2</td>\n",
              "      <td>38</td>\n",
              "      <td>4.55372</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49973</th>\n",
              "      <td>data/task2/images_by_class/73/48233.jpeg</td>\n",
              "      <td>2.605259</td>\n",
              "      <td>73</td>\n",
              "      <td>38</td>\n",
              "      <td>2.49691</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49986</th>\n",
              "      <td>data/task2/images_by_class/41/42881.jpeg</td>\n",
              "      <td>1.927168</td>\n",
              "      <td>41</td>\n",
              "      <td>38</td>\n",
              "      <td>0.841807</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>595 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57cf4ea2-0a23-4590-8916-0f6659b4312c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-57cf4ea2-0a23-4590-8916-0f6659b4312c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-57cf4ea2-0a23-4590-8916-0f6659b4312c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                           path confidence existing_label  \\\n",
              "80     data/task2/images_by_class/63/24071.jpeg   0.218912             63   \n",
              "83       data/task2/images_by_class/6/2774.jpeg   6.238861              6   \n",
              "103    data/task2/images_by_class/94/45260.jpeg   0.275159             94   \n",
              "118      data/task2/images_by_class/6/2498.jpeg   6.526378              6   \n",
              "125    data/task2/images_by_class/49/18982.jpeg   0.599729             49   \n",
              "...                                         ...        ...            ...   \n",
              "49780  data/task2/images_by_class/66/25295.jpeg   3.268991             66   \n",
              "49832  data/task2/images_by_class/46/17926.jpeg   2.829008             46   \n",
              "49838   data/task2/images_by_class/2/40844.jpeg   3.427066              2   \n",
              "49973  data/task2/images_by_class/73/48233.jpeg   2.605259             73   \n",
              "49986  data/task2/images_by_class/41/42881.jpeg   1.927168             41   \n",
              "\n",
              "      label_predicted confidence2 label_predicted2  \n",
              "80                 38    0.617343               38  \n",
              "83                 19    0.323555               19  \n",
              "103                38    4.134336               38  \n",
              "118                19    2.381141               19  \n",
              "125                61    0.650636               61  \n",
              "...               ...         ...              ...  \n",
              "49780              52    1.482898               52  \n",
              "49832              18    2.943778               18  \n",
              "49838              38     4.55372               38  \n",
              "49973              38     2.49691               38  \n",
              "49986              38    0.841807               38  \n",
              "\n",
              "[595 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "merge_same_label = merge_df[merge_df['label_predicted2'] == merge_df['label_predicted']]\n",
        "df_merge_correct = merge_same_label[merge_same_label['confidence']>0.2][merge_same_label['confidence2']>0.2]\n",
        "display(df_merge_correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvT-3Id7Fs6l"
      },
      "source": [
        "# Iteration 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "pkJt2zyOr82N"
      },
      "outputs": [],
      "source": [
        "correct_df_all = pd.concat([\n",
        "    df_merge_correct[['path','existing_label']], \n",
        "    df_correct_1[['path','existing_label']], \n",
        "    df_correct2[['path','existing_label']]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "id": "yu52lu-NsRDY"
      },
      "outputs": [],
      "source": [
        "correct_df_all = correct_df_all.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw0X_ePtKDp0",
        "outputId": "8a65e102-cb83-4956-f559-0b39c2ca7e23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/task2/images_by_class has been deleted\n",
            "data/task2/training_confident has been deleted\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [00:13<00:00, 3682.64it/s]\n"
          ]
        }
      ],
      "source": [
        "target_dir = 'data/task2/images_by_class'\n",
        "del_dir_if_exist(target_dir)\n",
        "\n",
        "target_dir = 'data/task2/training_confident'\n",
        "del_dir_if_exist(target_dir)\n",
        "make_dataset_folder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO1GIPPtKuxG",
        "outputId": "91aab622-2d47-4a70-89ad-28d77fce652e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1430/1430 [00:00<00:00, 8199.75it/s]\n"
          ]
        }
      ],
      "source": [
        "target_dir = 'data/task2/training_confident'\n",
        "# Iterate over the rows in the DataFrame\n",
        "for _, row in tqdm(correct_df_all.iterrows(), total=correct_df_all.shape[0]):\n",
        "    # Extract the path and class from the row\n",
        "    path = row['path']\n",
        "    label = row['existing_label'] \n",
        "\n",
        "    # Create the directory for the class\n",
        "    class_dir = f'{target_dir}/{label}'\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    \n",
        "    # move the file to the class directory\n",
        "    shutil.move(f\"{path}\", class_dir)\n",
        "    \n",
        "\n",
        "# we move the images because we don't want them to remain in the dataset for relabeling.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrVOGGQ7K4VH",
        "outputId": "b0ac7884-eea6-4058-db83-54f175e7e9be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "data_dir_train = 'data/task2/training_confident'\n",
        "image_dataset_train_only = datasets.ImageFolder(data_dir_train, preprocess) \n",
        "dataloader_train_only  = DataLoader(\n",
        "    image_dataset_train_only, \n",
        "    batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"],\n",
        "    shuffle = experiment_info[\"hyperparameters_data\"][\"shuffle_dataloader\"], \n",
        "    num_workers = experiment_info[\"hyperparameters_data\"][\"num_workers\"]\n",
        "    )\n",
        "\n",
        "class_names = image_dataset_train_only.classes\n",
        "dataset_size = len(image_dataset_train_only)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzeTlDqKK9LU",
        "outputId": "99c79dae-f099-4dca-d783-4ddcd337a0b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1430\n"
          ]
        }
      ],
      "source": [
        "print(dataset_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "id": "ocsGDv3yr3U9"
      },
      "outputs": [],
      "source": [
        "# Move all images considered correct into the training folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGSAqJs7MMtx"
      },
      "source": [
        "## Create datasets splits "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "id": "rpcFSHOOMMCM"
      },
      "outputs": [],
      "source": [
        "data_dir = 'data/task2/images_by_class'\n",
        "image_dataset_unshuffled = datasets.ImageFolder(data_dir, preprocess)\n",
        "image_dataset = ShuffledImageFolder(image_dataset_unshuffled)\n",
        "splits = 2\n",
        "data_sizes, loaders3, device = create_dataset_splits(splits, image_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA4Y3GPZMdrU",
        "outputId": "b268697e-fc73-404b-9979-f3f0ac27a682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train': 759, 'labeling': 759}\n",
            "num samples train 24288\n"
          ]
        }
      ],
      "source": [
        "batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"]\n",
        "print(data_sizes)\n",
        "print(\"num samples train\", data_sizes['train']*batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8iA3_TqMidR"
      },
      "source": [
        "## Train on remaining data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Jnkej2RMi3-",
        "outputId": "2161171c-298c-4069-cfd4-c10aac80945c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=False)\n",
            "  (1): Linear(in_features=1280, out_features=100, bias=True)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Download a pytorch MobileNet pretrained model\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "# change the Linear output to fit our dataset ( because model initially has 1000 classes)\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "# Setting hyperparameters\n",
        "\n",
        "model = model.to(device)\n",
        "print(model.classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "id": "Zm_iFW5VNJDl"
      },
      "outputs": [],
      "source": [
        "criterion, optimizer, exp_lr_scheduler = get_training_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62HjSwPqNLuB",
        "outputId": "59deebf5-9336-49ef-fdfb-8fe6cf636865"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/5\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/45 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|██████████| 45/45 [00:06<00:00,  6.77it/s]\n",
            "100%|██████████| 759/759 [02:14<00:00,  5.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 82.7125 Acc: 11.7022\n",
            "\n",
            "Epoch 1/5\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45/45 [00:06<00:00,  6.86it/s]\n",
            "100%|██████████| 759/759 [02:12<00:00,  5.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 50.9674 Acc: 17.8880\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45/45 [00:06<00:00,  6.89it/s]\n",
            "100%|██████████| 759/759 [02:13<00:00,  5.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 38.9500 Acc: 20.4980\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45/45 [00:06<00:00,  6.85it/s]\n",
            "100%|██████████| 759/759 [02:13<00:00,  5.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 30.0343 Acc: 22.7958\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45/45 [00:06<00:00,  6.86it/s]\n",
            "100%|██████████| 759/759 [02:12<00:00,  5.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 23.0908 Acc: 24.7312\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45/45 [00:07<00:00,  6.23it/s]\n",
            "100%|██████████| 759/759 [02:12<00:00,  5.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 17.2806 Acc: 26.5441\n",
            "\n",
            "Training complete in 14m 0s\n",
            "Best val Acc: 26.544137\n"
          ]
        }
      ],
      "source": [
        "model = train_model2(model, \n",
        "                    exp_lr_scheduler, \n",
        "                    optimizer, \n",
        "                    criterion,  \n",
        "                    data_sizes[\"train\"], \n",
        "                    dataloader_train_only,\n",
        "                    loaders3[0][\"train\"], \n",
        "                    num_epochs=6,\n",
        "                    # num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"]\n",
        "                     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {
        "id": "9MrBKA5mNyWe"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), '/gdrive/MyDrive/checkpoints/noisy_labels/model_3_it_0.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A8Bik6dOISz",
        "outputId": "426d1c5d-744b-4213-d7d7-0a28d29b3766"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/5\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45/45 [00:06<00:00,  6.89it/s]\n",
            "100%|██████████| 759/759 [02:13<00:00,  5.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.8961 Acc: 0.3373\n",
            "\n",
            "Epoch 1/5\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45/45 [00:06<00:00,  6.84it/s]\n",
            "100%|██████████| 759/759 [02:13<00:00,  5.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.8591 Acc: 0.3478\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45/45 [00:06<00:00,  6.90it/s]\n",
            "100%|██████████| 759/759 [02:12<00:00,  5.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.9236 Acc: 0.3452\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45/45 [00:06<00:00,  6.86it/s]\n",
            "100%|██████████| 759/759 [02:13<00:00,  5.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.8933 Acc: 0.3636\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45/45 [00:06<00:00,  6.91it/s]\n",
            "100%|██████████| 759/759 [02:13<00:00,  5.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.9341 Acc: 0.3531\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45/45 [00:06<00:00,  6.89it/s]\n",
            "100%|██████████| 759/759 [02:12<00:00,  5.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 150.9186 Acc: 0.3333\n",
            "\n",
            "Training complete in 13m 60s\n",
            "Best val Acc: 0.363636\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "model = model.to(device) \n",
        "model = train_model2(model, \n",
        "                exp_lr_scheduler, \n",
        "                optimizer, \n",
        "                criterion,  \n",
        "                data_sizes[\"train\"], \n",
        "                dataloader_train_only,\n",
        "                loaders3[1][\"train\"], \n",
        "                num_epochs=6,\n",
        "                # num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"]\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "id": "spnWxmbmO_K8"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), f'/gdrive/MyDrive/checkpoints/noisy_labels/model_3_it_{1}.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqCm1XO2ZIqt"
      },
      "source": [
        "## save labels for later predicting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UFfOp4gZIb3",
        "outputId": "74dd3c37-ceed-426d-b965-8e6ed909c2da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 759/759 [00:41<00:00, 18.41it/s]\n",
            "100%|██████████| 759/759 [00:38<00:00, 19.79it/s]\n"
          ]
        }
      ],
      "source": [
        "paths_labeling = {}\n",
        "for i in range(splits):\n",
        "    paths_labeling[i] = []\n",
        "    for images, labels, paths in tqdm(loaders3[i]['labeling']):\n",
        "        paths_labeling[i].append(paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "id": "UR31dj_WZMcf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Save the experiment information to a JSON file\n",
        "with open(f'/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_images_for_models_3.json', 'w') as f:\n",
        "    json.dump(paths_labeling, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzbAd0YzZOwN"
      },
      "source": [
        "## Predict new labels 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {
        "id": "8Z2oRxYMZQIa"
      },
      "outputs": [],
      "source": [
        "# Read the json of the labels that need to be predicted\n",
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_images_for_models_3.json', 'r') as f:\n",
        "    paths_labeling = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9ULO0XWZSJs",
        "outputId": "57dc6296-1aba-44e9-d19f-c402ea2d60e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 24285/24285 [02:38<00:00, 152.77it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "100%|██████████| 24285/24285 [02:33<00:00, 157.87it/s]\n"
          ]
        }
      ],
      "source": [
        "# create new dataset for pseudo-labeling\n",
        "all_predicted_labels = {}\n",
        "for iteration, paths in paths_labeling.items(): \n",
        "    paths_list = batches_to_list(paths)\n",
        "    dataset = ImageDataset(paths_list)    \n",
        "    labeling_loader = DataLoader(dataset, batch_size=1)\n",
        "    PATH = f'/gdrive/MyDrive/checkpoints/noisy_labels/model_3_it_{iteration}.pt'\n",
        "    model = load_model(PATH, 'cuda')\n",
        "    all_predicted_labels[iteration] = generate_new_labels(model, labeling_loader, 'cuda')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "id": "3sjD3v5cZUXO"
      },
      "outputs": [],
      "source": [
        "# Save the new predictions and the old predictions in a file for future \n",
        "with open(f'/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_third_iteration.json', 'w') as f:\n",
        "    json.dump(all_predicted_labels, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTGIX0LMZZyI"
      },
      "source": [
        "## Analyze pseudo lableing from third iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "id": "XBfoWTt6ZcYw"
      },
      "outputs": [],
      "source": [
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_third_iteration.json', 'r') as f:\n",
        "    all_predicted_labels = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "7sA9anWCZgvN",
        "outputId": "5cbfc1af-dbad-4656-b3c4-5c6c77c72642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How many labels have the same value after relabling ?\n",
            "False    23987\n",
            "True       298\n",
            "dtype: int64\n",
            "What is the confidence of those ?\n",
            "33       0.127792\n",
            "82       0.431086\n",
            "120      0.785051\n",
            "132      0.032602\n",
            "137      0.704218\n",
            "           ...   \n",
            "23652    0.033942\n",
            "23699    0.414606\n",
            "23757    0.422824\n",
            "24099    0.133101\n",
            "24272    0.086001\n",
            "Name: confidence, Length: 298, dtype: object\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f04bda15be0>"
            ]
          },
          "execution_count": 312,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the predicted labels and confidence scores\n",
        "predictions = pd.DataFrame.from_dict(all_predicted_labels['0'],orient='index').transpose()\n",
        "print(\"How many labels have the same value after relabling ?\")\n",
        "print((predictions['existing_label'] == predictions['label_predicted']).value_counts())\n",
        "print(\"What is the confidence of those ?\")\n",
        "print(predictions[predictions['existing_label'] == predictions['label_predicted']]['confidence'])\n",
        "predictions[predictions['existing_label'] == predictions['label_predicted']]['confidence'].plot.hist(bins = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "HTqrOqNCZvq-",
        "outputId": "ec053198-78b1-45d0-eddb-d97ca3a60dfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How many above threshold?\n",
            "124\n",
            "the distribution of labels different from one dataset to another\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f047a3bae20>"
            ]
          },
          "execution_count": 315,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "predictions_same_label = predictions[predictions['existing_label'] == predictions['label_predicted']]\n",
        "predictions_correct_label = predictions_same_label[predictions_same_label['confidence']>0.2]\n",
        "print(\"How many above threshold?\")\n",
        "print(len(predictions_correct_label))\n",
        "print(\"the distribution of labels different from one dataset to another\")\n",
        "predictions[predictions['existing_label'] != predictions['label_predicted']]['confidence'].plot.hist(bins = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "metadata": {
        "id": "Zet5pNxaqIVH"
      },
      "outputs": [],
      "source": [
        "with open('/gdrive/MyDrive/checkpoints/noisy_labels/pseudo_labeling_third_iteration.json', 'r') as f:\n",
        "    all_predicted_labels3 = json.load(f)\n",
        "# Load the predicted labels and confidence scores\n",
        "correct_labels_df3 = []\n",
        "for i in range(splits):\n",
        "    predictions=pd.DataFrame.from_dict(all_predicted_labels3[str(i)],orient='index').transpose() \n",
        "    predictions_same_label = predictions[predictions['existing_label'] == predictions['label_predicted']]\n",
        "    predictions_correct_label = predictions_same_label[predictions_same_label['confidence']>0.1] \n",
        "    correct_labels_df3.append(predictions_correct_label)\n",
        "df_correct3 = pd.concat(correct_labels_df3)\n",
        "df_correct3['path'] = df_correct3['path'].apply(lambda x: x[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 351,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvxwEITJqPsM",
        "outputId": "c4b326ef-3469-4499-b2c3-efbada73c701"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "450"
            ]
          },
          "execution_count": 351,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df_correct3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 352,
      "metadata": {
        "id": "QeSt5Alft7Jt"
      },
      "outputs": [],
      "source": [
        "df_all3 = prep_to_merge(all_predicted_labels3, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 353,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "YIvUiW75t8aw",
        "outputId": "cd018640-58fc-47dd-e48a-883b5c27462c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-73083e1d-31d8-41c3-b0a5-541a133a8aaf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>confidence3</th>\n",
              "      <th>label_predicted3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/task2/images_by_class/4/45175.jpeg</td>\n",
              "      <td>0.162594</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/task2/images_by_class/62/23557.jpeg</td>\n",
              "      <td>0.041466</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/task2/images_by_class/8/49212.jpeg</td>\n",
              "      <td>0.40802</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/task2/images_by_class/37/14708.jpeg</td>\n",
              "      <td>0.13028</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/task2/images_by_class/81/31158.jpeg</td>\n",
              "      <td>0.040965</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24280</th>\n",
              "      <td>data/task2/images_by_class/90/34530.jpeg</td>\n",
              "      <td>1.584019</td>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24281</th>\n",
              "      <td>data/task2/images_by_class/36/14287.jpeg</td>\n",
              "      <td>3.317563</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24282</th>\n",
              "      <td>data/task2/images_by_class/13/5195.jpeg</td>\n",
              "      <td>3.333233</td>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24283</th>\n",
              "      <td>data/task2/images_by_class/5/2260.jpeg</td>\n",
              "      <td>0.995843</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24284</th>\n",
              "      <td>data/task2/images_by_class/8/3536.jpeg</td>\n",
              "      <td>1.164974</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>48570 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73083e1d-31d8-41c3-b0a5-541a133a8aaf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-73083e1d-31d8-41c3-b0a5-541a133a8aaf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-73083e1d-31d8-41c3-b0a5-541a133a8aaf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                           path confidence3 label_predicted3\n",
              "0       data/task2/images_by_class/4/45175.jpeg    0.162594               34\n",
              "1      data/task2/images_by_class/62/23557.jpeg    0.041466                2\n",
              "2       data/task2/images_by_class/8/49212.jpeg     0.40802               19\n",
              "3      data/task2/images_by_class/37/14708.jpeg     0.13028               34\n",
              "4      data/task2/images_by_class/81/31158.jpeg    0.040965               24\n",
              "...                                         ...         ...              ...\n",
              "24280  data/task2/images_by_class/90/34530.jpeg    1.584019               78\n",
              "24281  data/task2/images_by_class/36/14287.jpeg    3.317563                5\n",
              "24282   data/task2/images_by_class/13/5195.jpeg    3.333233               78\n",
              "24283    data/task2/images_by_class/5/2260.jpeg    0.995843               14\n",
              "24284    data/task2/images_by_class/8/3536.jpeg    1.164974                7\n",
              "\n",
              "[48570 rows x 3 columns]"
            ]
          },
          "execution_count": 353,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_all3.rename({\"confidence\":\"confidence3\", \"label_predicted\":\"label_predicted3\"}, inplace=True, axis = 1)\n",
        "df_all3.drop(\"existing_label\", inplace=True,axis=1)\n",
        "df_all3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "metadata": {
        "id": "aH4OFRrEsaqu"
      },
      "outputs": [],
      "source": [
        "df_all1 = prep_to_merge(all_predicted_labels1, 10)\n",
        "df_all2 = prep_to_merge(all_predicted_labels2, 2)\n",
        "\n",
        "df_all2.rename({\"confidence\":\"confidence2\", \"label_predicted\":\"label_predicted2\"}, inplace=True, axis = 1)\n",
        "df_all2.drop(\"existing_label\", inplace=True,axis=1)\n",
        "merge_df = pd.merge(df_all1, df_all2, on=\"path\", how=\"left\")\n",
        "# we don't have all rows for confidence 2 so we use \"left\" merging\n",
        "merge_df = pd.merge(merge_df, df_all3, on=\"path\", how=\"left\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 359,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "EFjGU0m0tCbo",
        "outputId": "649885ee-eb86-4774-ef55-5fd256d68685"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ccf5b249-8497-4a84-aa7e-679329aabae7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>confidence</th>\n",
              "      <th>existing_label</th>\n",
              "      <th>label_predicted</th>\n",
              "      <th>confidence2</th>\n",
              "      <th>label_predicted2</th>\n",
              "      <th>confidence3</th>\n",
              "      <th>label_predicted3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/task2/images_by_class/3/1383.jpeg</td>\n",
              "      <td>0.36458</td>\n",
              "      <td>3</td>\n",
              "      <td>19</td>\n",
              "      <td>0.916443</td>\n",
              "      <td>69</td>\n",
              "      <td>0.991553</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/task2/images_by_class/76/29313.jpeg</td>\n",
              "      <td>0.098717</td>\n",
              "      <td>76</td>\n",
              "      <td>59</td>\n",
              "      <td>1.734477</td>\n",
              "      <td>47</td>\n",
              "      <td>1.321538</td>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/task2/images_by_class/30/12277.jpeg</td>\n",
              "      <td>0.203538</td>\n",
              "      <td>30</td>\n",
              "      <td>34</td>\n",
              "      <td>0.174259</td>\n",
              "      <td>44</td>\n",
              "      <td>0.154604</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/task2/images_by_class/59/22781.jpeg</td>\n",
              "      <td>0.13695</td>\n",
              "      <td>59</td>\n",
              "      <td>61</td>\n",
              "      <td>1.593244</td>\n",
              "      <td>17</td>\n",
              "      <td>1.353579</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/task2/images_by_class/88/34072.jpeg</td>\n",
              "      <td>0.259119</td>\n",
              "      <td>88</td>\n",
              "      <td>34</td>\n",
              "      <td>1.78167</td>\n",
              "      <td>3</td>\n",
              "      <td>2.434512</td>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>data/task2/images_by_class/74/28286.jpeg</td>\n",
              "      <td>1.359533</td>\n",
              "      <td>74</td>\n",
              "      <td>50</td>\n",
              "      <td>0.029493</td>\n",
              "      <td>17</td>\n",
              "      <td>1.693614</td>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>data/task2/images_by_class/15/45448.jpeg</td>\n",
              "      <td>3.355858</td>\n",
              "      <td>15</td>\n",
              "      <td>61</td>\n",
              "      <td>3.662585</td>\n",
              "      <td>47</td>\n",
              "      <td>0.348328</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>data/task2/images_by_class/8/44196.jpeg</td>\n",
              "      <td>1.073675</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0.305969</td>\n",
              "      <td>38</td>\n",
              "      <td>1.2268</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>data/task2/images_by_class/75/28828.jpeg</td>\n",
              "      <td>1.837749</td>\n",
              "      <td>75</td>\n",
              "      <td>39</td>\n",
              "      <td>1.070897</td>\n",
              "      <td>99</td>\n",
              "      <td>0.143318</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>data/task2/images_by_class/99/40549.jpeg</td>\n",
              "      <td>3.33866</td>\n",
              "      <td>99</td>\n",
              "      <td>67</td>\n",
              "      <td>2.707126</td>\n",
              "      <td>44</td>\n",
              "      <td>2.735814</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ccf5b249-8497-4a84-aa7e-679329aabae7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ccf5b249-8497-4a84-aa7e-679329aabae7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ccf5b249-8497-4a84-aa7e-679329aabae7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                           path confidence existing_label  \\\n",
              "0        data/task2/images_by_class/3/1383.jpeg    0.36458              3   \n",
              "1      data/task2/images_by_class/76/29313.jpeg   0.098717             76   \n",
              "2      data/task2/images_by_class/30/12277.jpeg   0.203538             30   \n",
              "3      data/task2/images_by_class/59/22781.jpeg    0.13695             59   \n",
              "4      data/task2/images_by_class/88/34072.jpeg   0.259119             88   \n",
              "...                                         ...        ...            ...   \n",
              "49995  data/task2/images_by_class/74/28286.jpeg   1.359533             74   \n",
              "49996  data/task2/images_by_class/15/45448.jpeg   3.355858             15   \n",
              "49997   data/task2/images_by_class/8/44196.jpeg   1.073675              8   \n",
              "49998  data/task2/images_by_class/75/28828.jpeg   1.837749             75   \n",
              "49999  data/task2/images_by_class/99/40549.jpeg    3.33866             99   \n",
              "\n",
              "      label_predicted confidence2 label_predicted2 confidence3  \\\n",
              "0                  19    0.916443               69    0.991553   \n",
              "1                  59    1.734477               47    1.321538   \n",
              "2                  34    0.174259               44    0.154604   \n",
              "3                  61    1.593244               17    1.353579   \n",
              "4                  34     1.78167                3    2.434512   \n",
              "...               ...         ...              ...         ...   \n",
              "49995              50    0.029493               17    1.693614   \n",
              "49996              61    3.662585               47    0.348328   \n",
              "49997               0    0.305969               38      1.2268   \n",
              "49998              39    1.070897               99    0.143318   \n",
              "49999              67    2.707126               44    2.735814   \n",
              "\n",
              "      label_predicted3  \n",
              "0                   19  \n",
              "1                   78  \n",
              "2                   56  \n",
              "3                    9  \n",
              "4                   78  \n",
              "...                ...  \n",
              "49995               78  \n",
              "49996               56  \n",
              "49997               70  \n",
              "49998               65  \n",
              "49999               11  \n",
              "\n",
              "[50000 rows x 8 columns]"
            ]
          },
          "execution_count": 359,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merge_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 360,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vbosYgkud9X",
        "outputId": "ffa13f6c-b05b-4167-a639-0d55d664f9b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How many rows have the label_predicted2 = label_predicted 2\n",
            "False    49251\n",
            "True       749\n",
            "dtype: int64\n",
            "False    49407\n",
            "True       593\n",
            "dtype: int64\n",
            "False    48998\n",
            "True      1002\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"How many rows have the label_predicted2 = label_predicted 2\")\n",
        "print((merge_df['label_predicted'] == merge_df['label_predicted2']).value_counts())\n",
        "print((merge_df['label_predicted'] == merge_df['label_predicted3']).value_counts())\n",
        "print((merge_df['label_predicted3'] == merge_df['label_predicted2']).value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 362,
      "metadata": {
        "id": "DYlaKxLfu58l"
      },
      "outputs": [],
      "source": [
        "next = merge_df[(merge_df['label_predicted3'] == merge_df['label_predicted2']) +\n",
        "         (merge_df['label_predicted'] == merge_df['label_predicted2'])\n",
        "         ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 363,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "1dgRdfdUwL5y",
        "outputId": "2bf915f5-6604-4375-8b69-3d697b8d6241"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-363-3e57bbe9fc6c>:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df_merge_correct = next[next['confidence']>0.1][next['confidence2']>0.1][next['confidence3']>0.1]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a7e246c4-63a9-4112-ac67-c5e799898afc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>confidence</th>\n",
              "      <th>existing_label</th>\n",
              "      <th>label_predicted</th>\n",
              "      <th>confidence2</th>\n",
              "      <th>label_predicted2</th>\n",
              "      <th>confidence3</th>\n",
              "      <th>label_predicted3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>data/task2/images_by_class/80/30622.jpeg</td>\n",
              "      <td>0.151909</td>\n",
              "      <td>80</td>\n",
              "      <td>77</td>\n",
              "      <td>2.467196</td>\n",
              "      <td>18</td>\n",
              "      <td>1.251756</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>data/task2/images_by_class/85/43432.jpeg</td>\n",
              "      <td>1.882455</td>\n",
              "      <td>85</td>\n",
              "      <td>84</td>\n",
              "      <td>1.669616</td>\n",
              "      <td>18</td>\n",
              "      <td>0.996252</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>data/task2/images_by_class/41/16197.jpeg</td>\n",
              "      <td>0.313494</td>\n",
              "      <td>41</td>\n",
              "      <td>7</td>\n",
              "      <td>0.859657</td>\n",
              "      <td>19</td>\n",
              "      <td>0.751059</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>data/task2/images_by_class/8/3452.jpeg</td>\n",
              "      <td>0.205941</td>\n",
              "      <td>8</td>\n",
              "      <td>30</td>\n",
              "      <td>1.193492</td>\n",
              "      <td>70</td>\n",
              "      <td>1.340853</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>data/task2/images_by_class/45/17408.jpeg</td>\n",
              "      <td>0.100038</td>\n",
              "      <td>45</td>\n",
              "      <td>2</td>\n",
              "      <td>0.83005</td>\n",
              "      <td>2</td>\n",
              "      <td>2.378393</td>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49908</th>\n",
              "      <td>data/task2/images_by_class/12/47122.jpeg</td>\n",
              "      <td>2.657788</td>\n",
              "      <td>12</td>\n",
              "      <td>98</td>\n",
              "      <td>0.412509</td>\n",
              "      <td>19</td>\n",
              "      <td>0.713494</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49916</th>\n",
              "      <td>data/task2/images_by_class/10/45198.jpeg</td>\n",
              "      <td>1.309713</td>\n",
              "      <td>10</td>\n",
              "      <td>61</td>\n",
              "      <td>0.985352</td>\n",
              "      <td>19</td>\n",
              "      <td>0.489649</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49925</th>\n",
              "      <td>data/task2/images_by_class/93/35542.jpeg</td>\n",
              "      <td>2.562606</td>\n",
              "      <td>93</td>\n",
              "      <td>16</td>\n",
              "      <td>3.665642</td>\n",
              "      <td>19</td>\n",
              "      <td>3.110206</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49963</th>\n",
              "      <td>data/task2/images_by_class/28/11267.jpeg</td>\n",
              "      <td>2.91602</td>\n",
              "      <td>28</td>\n",
              "      <td>67</td>\n",
              "      <td>5.470263</td>\n",
              "      <td>19</td>\n",
              "      <td>1.925581</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49969</th>\n",
              "      <td>data/task2/images_by_class/86/33057.jpeg</td>\n",
              "      <td>2.476339</td>\n",
              "      <td>86</td>\n",
              "      <td>67</td>\n",
              "      <td>0.199136</td>\n",
              "      <td>63</td>\n",
              "      <td>0.176967</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>882 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7e246c4-63a9-4112-ac67-c5e799898afc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a7e246c4-63a9-4112-ac67-c5e799898afc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a7e246c4-63a9-4112-ac67-c5e799898afc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                           path confidence existing_label  \\\n",
              "133    data/task2/images_by_class/80/30622.jpeg   0.151909             80   \n",
              "178    data/task2/images_by_class/85/43432.jpeg   1.882455             85   \n",
              "258    data/task2/images_by_class/41/16197.jpeg   0.313494             41   \n",
              "303      data/task2/images_by_class/8/3452.jpeg   0.205941              8   \n",
              "362    data/task2/images_by_class/45/17408.jpeg   0.100038             45   \n",
              "...                                         ...        ...            ...   \n",
              "49908  data/task2/images_by_class/12/47122.jpeg   2.657788             12   \n",
              "49916  data/task2/images_by_class/10/45198.jpeg   1.309713             10   \n",
              "49925  data/task2/images_by_class/93/35542.jpeg   2.562606             93   \n",
              "49963  data/task2/images_by_class/28/11267.jpeg    2.91602             28   \n",
              "49969  data/task2/images_by_class/86/33057.jpeg   2.476339             86   \n",
              "\n",
              "      label_predicted confidence2 label_predicted2 confidence3  \\\n",
              "133                77    2.467196               18    1.251756   \n",
              "178                84    1.669616               18    0.996252   \n",
              "258                 7    0.859657               19    0.751059   \n",
              "303                30    1.193492               70    1.340853   \n",
              "362                 2     0.83005                2    2.378393   \n",
              "...               ...         ...              ...         ...   \n",
              "49908              98    0.412509               19    0.713494   \n",
              "49916              61    0.985352               19    0.489649   \n",
              "49925              16    3.665642               19    3.110206   \n",
              "49963              67    5.470263               19    1.925581   \n",
              "49969              67    0.199136               63    0.176967   \n",
              "\n",
              "      label_predicted3  \n",
              "133                 18  \n",
              "178                 18  \n",
              "258                 19  \n",
              "303                 70  \n",
              "362                 78  \n",
              "...                ...  \n",
              "49908               19  \n",
              "49916               19  \n",
              "49925               19  \n",
              "49963               19  \n",
              "49969               63  \n",
              "\n",
              "[882 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_merge_correct = next[next['confidence']>0.1][next['confidence2']>0.1][next['confidence3']>0.1]\n",
        "display(df_merge_correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfGR6AoGzpbE"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnTlyuE9wLF-"
      },
      "source": [
        "### Prepare unlabeled dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oL3eDGtwQNU",
        "outputId": "6319bd46-3712-4f27-f0f2-82b5838dba46"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5001/5001 [00:00<00:00, 31199.33it/s]\n"
          ]
        }
      ],
      "source": [
        "dir_data = 'data/task2/val_data'\n",
        "# copy data to allow dataloader to read it easier\n",
        "dest_dir = 'data/task2/val_data/0'\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "# Iterate over all the files in the source directory\n",
        "for file in tqdm(os.listdir(dir_data)):\n",
        "    if file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".png\"):\n",
        "        # Construct the source and destination paths\n",
        "        src_path = os.path.join(dir_data, file)\n",
        "        dst_path = os.path.join(dest_dir, file)\n",
        "        # Move the file\n",
        "        shutil.move(src_path, dst_path) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "a1RFD1utwWGL"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import DatasetFolder\n",
        "\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(experiment_info[\"image_processing\"][\"resize\"]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=experiment_info[\"image_processing\"][\"mean\"],\n",
        "                         std=experiment_info[\"image_processing\"][\"std\"]),\n",
        "    ])\n",
        "\n",
        "dir_data = 'data/task2/val_data'\n",
        "image_dataset_unlabeled = datasets.ImageFolder(root=dir_data, transform=preprocess)\n",
        "\n",
        "dataloader_unlabeled  = torch.utils.data.DataLoader(\n",
        "    image_dataset_unlabeled, \n",
        "    batch_size = 1, \n",
        "    )\n",
        "\n",
        "dataset_size = len(image_dataset_unlabeled)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "FLxGkNPcytku"
      },
      "outputs": [],
      "source": [
        "def make_eval_predictions(model, image_dataset_unlabeled, dataloader_unlabeled):\n",
        "    predicted_labels = {\n",
        "    \"path\":[],\n",
        "    \"confidence\":[],\n",
        "    \"label\":[]\n",
        "    }\n",
        "\n",
        "    # Each epoch has a training and validation phase \n",
        "    model.eval()   # Set model to evaluate mode \n",
        "\n",
        "    # Iterate over data.\n",
        "    img_path_generator = ((image, path) for (path,_) , (image, _) in zip(image_dataset_unlabeled.samples, dataloader_unlabeled))\n",
        "\n",
        "    for inputs, path in tqdm(img_path_generator, total=dataset_size):\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(False): # we don't want to train\n",
        "            outputs = model(inputs)\n",
        "            confidence, preds = torch.max(outputs, 1) \n",
        "        predicted_labels[\"path\"].append(path) \n",
        "        predicted_labels[\"confidence\"].append(confidence.item())\n",
        "        predicted_labels[\"label\"].append(preds.item())\n",
        "    return predicted_labels "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0eBLrahrkvM"
      },
      "source": [
        "### Train a single model on all the correct labels only for evaluation\n",
        "Also compare this with training on the whole dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVuYWTr-tcJN",
        "outputId": "66a69569-f398-46af-f4f7-6b4a1313cd53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/task2/images_by_class has been deleted\n"
          ]
        }
      ],
      "source": [
        "target_dir = 'data/task2/images_by_class'\n",
        "del_dir_if_exist(target_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_quCksYWtcJO",
        "outputId": "b9b82987-5dc6-4420-e90e-d8b5352f9e6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/task2/training_confident has been deleted\n"
          ]
        }
      ],
      "source": [
        "# Check if the directory exists, to recreate it instead of messing it up\n",
        "target_dir = 'data/task2/training_confident'\n",
        "del_dir_if_exist(target_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt7k_xhHtcJO",
        "outputId": "2c02300a-41d4-4398-e8b8-833c60d42339"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [00:10<00:00, 4568.15it/s]\n"
          ]
        }
      ],
      "source": [
        "make_dataset_folder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-m3ANMVtcJP",
        "outputId": "46a03a9b-b7c2-4078-a44c-e4f113dfe75f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3970/3970 [00:01<00:00, 3182.87it/s]\n"
          ]
        }
      ],
      "source": [
        "# Train on 2 datasets at the same time, one with high confidence labels\n",
        "# this dataset will only be used during training, never forgetting labels\n",
        "# the second dataset will iteratively forget labels and train on the remaining labels.\n",
        "# Define the base directory\n",
        "target_dir = 'data/task2/training_confident'\n",
        "# Iterate over the rows in the DataFrame\n",
        "for _, row in tqdm(correct_df_all.iterrows(), total=correct_df_all.shape[0]):\n",
        "    # Extract the path and class from the row\n",
        "    path = row['path']\n",
        "    label = row['existing_label'] \n",
        "\n",
        "    # Create the directory for the class\n",
        "    class_dir = f'{target_dir}/{label}'\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    \n",
        "    # move the file to the class directory\n",
        "    shutil.move(f\"{path}\", class_dir)\n",
        "    \n",
        "\n",
        "# we move the images because we don't want them to remain in the dataset for relabeling.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "O266GLXcrqyR"
      },
      "outputs": [],
      "source": [
        "# TODO: create the 2 datasets\n",
        "\n",
        "data_dir_train = 'data/task2/training_confident'\n",
        "image_dataset_train_only = datasets.ImageFolder(data_dir_train, preprocess) \n",
        "dataloader_train_only  = DataLoader(\n",
        "    image_dataset_train_only, \n",
        "    batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"],\n",
        "    shuffle = experiment_info[\"hyperparameters_data\"][\"shuffle_dataloader\"], \n",
        "    num_workers = experiment_info[\"hyperparameters_data\"][\"num_workers\"]\n",
        "    )\n",
        "\n",
        "class_names = image_dataset_train_only.classes\n",
        "dataset_size = len(image_dataset_train_only)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dSTW4C4vK2u",
        "outputId": "fc73a05a-de9a-4370-8989-29c4ddd886a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3970\n"
          ]
        }
      ],
      "source": [
        "print(dataset_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mngC6ZIsvV2U",
        "outputId": "cc1461d3-9f15-4ca4-f231-a0fc31c28c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=False)\n",
            "  (1): Linear(in_features=1280, out_features=100, bias=True)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Download a pytorch MobileNet pretrained model\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "# change the Linear output to fit our dataset ( because model initially has 1000 classes)\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "# Setting hyperparameters\n",
        "\n",
        "model = model.to(device)\n",
        "print(model.classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "id": "Y3AgFetjvYcY"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=experiment_info[\"hyperparameters_training\"][\"learning_rate\"],\n",
        "    )\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(\n",
        "    optimizer, \n",
        "    step_size=experiment_info[\"hyperparameters_training\"][\"scheduler_step_size\"], \n",
        "    gamma=experiment_info[\"hyperparameters_training\"][\"scheduler_gamma\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ47VxvRvZxu",
        "outputId": "7fc4b973-72b0-41a3-9756-dd25045a2bae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 38/38 [00:05<00:00,  6.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 6.5748 Acc: 0.2058\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 38/38 [00:05<00:00,  6.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 4.9462 Acc: 0.5072\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 38/38 [00:05<00:00,  6.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 4.0197 Acc: 0.6986\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 38/38 [00:05<00:00,  6.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 3.2882 Acc: 0.8545\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 38/38 [00:05<00:00,  6.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 2.6456 Acc: 1.0433\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 38/38 [00:05<00:00,  6.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 2.0811 Acc: 1.2412\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 38/38 [00:05<00:00,  6.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 1.6234 Acc: 1.3735\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 38/38 [00:05<00:00,  6.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 1.2442 Acc: 1.4600\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 38/38 [00:05<00:00,  6.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 1.1899 Acc: 1.4705\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 38/38 [00:05<00:00,  6.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 1.1671 Acc: 1.4862\n",
            "\n",
            "Training complete in 0m 57s\n",
            "Best val Acc: 1.486239\n"
          ]
        }
      ],
      "source": [
        "model = train_model3(model, \n",
        "                    exp_lr_scheduler, \n",
        "                    optimizer, \n",
        "                    criterion,  \n",
        "                    data_sizes[\"train\"], \n",
        "                    dataloader_train_only,\n",
        "                    num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "D-nRCGD1we3J"
      },
      "outputs": [],
      "source": [
        "path_m = f'/gdrive/MyDrive/checkpoints/noisy_labels/eval_model2.pt'\n",
        "torch.save(model.state_dict(), path_m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubt6EFWwwE8q",
        "outputId": "5ae48f99-cedc-41e4-a380-a452fa2bcad7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5000it [00:44, 113.15it/s]\n"
          ]
        }
      ],
      "source": [
        "predicted_labels = make_eval_predictions(model, image_dataset_unlabeled, dataloader_unlabeled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "KA3wrD_myNpb"
      },
      "outputs": [],
      "source": [
        "to_csv(predicted_labels, \"eval2.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj5efjupxV74"
      },
      "source": [
        "### Train model directly on all images "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1bMpEmExikL",
        "outputId": "053538a4-7654-445a-a8b4-880b85416ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/task2/images_by_class has been deleted\n"
          ]
        }
      ],
      "source": [
        "target_dir = 'data/task2/images_by_class'\n",
        "del_dir_if_exist(target_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYSCm9_JxikL",
        "outputId": "410eeb9b-e709-4754-a15c-042acf8b1b2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/task2/training_confident has been deleted\n"
          ]
        }
      ],
      "source": [
        "# Check if the directory exists, to recreate it instead of messing it up\n",
        "target_dir = 'data/task2/training_confident'\n",
        "del_dir_if_exist(target_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j4k5oqJxikM",
        "outputId": "70970ef9-2ee1-4da6-fb85-1ff52fdc06af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [00:11<00:00, 4461.58it/s]\n"
          ]
        }
      ],
      "source": [
        "make_dataset_folder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blbcqNulxZ-Y",
        "outputId": "7637a39d-be98-488e-c1a1-9e673ab066b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# TODO: create the 2 datasets\n",
        "\n",
        "data_dir_train = 'data/task2/images_by_class'\n",
        "image_dataset_train_only = datasets.ImageFolder(data_dir_train, preprocess) \n",
        "dataloader_train_only  = DataLoader(\n",
        "    image_dataset_train_only, \n",
        "    batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"],\n",
        "    shuffle = experiment_info[\"hyperparameters_data\"][\"shuffle_dataloader\"], \n",
        "    num_workers = experiment_info[\"hyperparameters_data\"][\"num_workers\"]\n",
        "    )\n",
        "\n",
        "class_names = image_dataset_train_only.classes\n",
        "dataset_size = len(image_dataset_train_only)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5zGYVgsxt6N",
        "outputId": "eddb38ba-b1d6-4669-ab68-146776eff0fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=False)\n",
            "  (1): Linear(in_features=1280, out_features=100, bias=True)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Download a pytorch MobileNet pretrained model\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "# change the Linear output to fit our dataset ( because model initially has 1000 classes)\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "# Setting hyperparameters\n",
        "\n",
        "model = model.to(device)\n",
        "print(model.classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsXVQbxgx2Kh",
        "outputId": "59fe2e18-58cb-45e2-c70c-4556bd0d5560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:47<00:00,  6.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.8403 Acc: 0.7884\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:46<00:00,  6.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.8761 Acc: 0.7665\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:36<00:00,  7.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.7891 Acc: 0.7600\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:31<00:00,  7.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.8233 Acc: 0.8052\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:31<00:00,  7.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.6771 Acc: 0.7613\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:31<00:00,  7.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.8680 Acc: 0.7316\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:31<00:00,  7.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.7473 Acc: 0.8039\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:31<00:00,  7.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.8817 Acc: 0.7613\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:32<00:00,  7.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.7556 Acc: 0.7290\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1563/1563 [03:31<00:00,  7.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 301.7398 Acc: 0.7561\n",
            "\n",
            "Training complete in 35m 53s\n",
            "Best val Acc: 0.805161\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = train_model3(model, \n",
        "                    exp_lr_scheduler, \n",
        "                    optimizer, \n",
        "                    criterion,  \n",
        "                    data_sizes[\"train\"], \n",
        "                    dataloader_train_only,\n",
        "                    num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "Gd4z81Y6x5YZ"
      },
      "outputs": [],
      "source": [
        "path_m = f'/gdrive/MyDrive/checkpoints/noisy_labels/just_train.pt'\n",
        "torch.save(model.state_dict(), path_m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-dKq24XzDI2",
        "outputId": "91b34ffd-27d6-4fa4-8eac-f0b9ebddf673"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:42<00:00, 116.77it/s]\n"
          ]
        }
      ],
      "source": [
        "predicted_labels = make_eval_predictions(model, image_dataset_unlabeled, dataloader_unlabeled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "E8S6x_8pyVTJ"
      },
      "outputs": [],
      "source": [
        "to_csv(predicted_labels, \"just_train.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 23:03:10) [MSC v.1916 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07868acd1960497680b904b81fd0eaf9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07c37bb6b22b4c8abe8d16b61e28e1db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07868acd1960497680b904b81fd0eaf9",
            "max": 14212972,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a56ed7ebe21c4011a23a63cad606d40c",
            "value": 14212972
          }
        },
        "0b6b0668f3764a7296945547803e2688": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c39c58da42ce4c2ca76546673443f0fd",
              "IPY_MODEL_b02ccf11c1714786a67bfca3446e9b9d",
              "IPY_MODEL_284de89238b54efd8d834cfc5cdef224"
            ],
            "layout": "IPY_MODEL_4d1e4cd757a749bda0f6fbcede536076"
          }
        },
        "15a23332521441cab29f7acedc079b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba437891973d4629b4881ccb11f044ae",
            "placeholder": "​",
            "style": "IPY_MODEL_a75d7f60d1c24475a1540bc1cfe94f7b",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 62.9MB/s]"
          }
        },
        "284de89238b54efd8d834cfc5cdef224": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7adf26ae5d2e430e946d5576135d893a",
            "placeholder": "​",
            "style": "IPY_MODEL_69db9e55793a4d87bd7b0c96efc53170",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 46.0MB/s]"
          }
        },
        "4d1e4cd757a749bda0f6fbcede536076": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dd784f55bb14f6ba9274ec956ec8061": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "69db9e55793a4d87bd7b0c96efc53170": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6eed1376f1f24e8abbe2d4ea0ca3fcb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71a16d1906924e738fc1baa304d94811",
            "placeholder": "​",
            "style": "IPY_MODEL_a74028e913de43d3b1eae97433d91882",
            "value": "100%"
          }
        },
        "71a16d1906924e738fc1baa304d94811": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7adf26ae5d2e430e946d5576135d893a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "816420dd910b475faa99a8e9ef6eebd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e3263acdec647e594b2140e749bd110": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a56ed7ebe21c4011a23a63cad606d40c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a74028e913de43d3b1eae97433d91882": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a75d7f60d1c24475a1540bc1cfe94f7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afee6006e2324bac8bbc9c291e638617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b02ccf11c1714786a67bfca3446e9b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e3263acdec647e594b2140e749bd110",
            "max": 14212972,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4dd784f55bb14f6ba9274ec956ec8061",
            "value": 14212972
          }
        },
        "ba437891973d4629b4881ccb11f044ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c39c58da42ce4c2ca76546673443f0fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd637c7b9a5645dfb198f017092d098a",
            "placeholder": "​",
            "style": "IPY_MODEL_afee6006e2324bac8bbc9c291e638617",
            "value": "100%"
          }
        },
        "dd637c7b9a5645dfb198f017092d098a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb68d62d599a462f92aaca0ab0b854e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6eed1376f1f24e8abbe2d4ea0ca3fcb1",
              "IPY_MODEL_07c37bb6b22b4c8abe8d16b61e28e1db",
              "IPY_MODEL_15a23332521441cab29f7acedc079b16"
            ],
            "layout": "IPY_MODEL_816420dd910b475faa99a8e9ef6eebd4"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
