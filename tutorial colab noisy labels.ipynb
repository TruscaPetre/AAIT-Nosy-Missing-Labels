{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TruscaPetre/AAIT-Nosy-Missing-Labels/blob/main/tutorial%20colab%20noisy%20labels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaRT8CyQPpcv"
      },
      "source": [
        "# Noisy labels problem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOThg22n_IX7"
      },
      "source": [
        "## Introduction theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJxksI3i_IX-"
      },
      "source": [
        "Before starting to implement anything we should think about the problem first. We don't know which of the training samples are wrong, and we cannot try to correct them by hand because there are too many of them and the images are not very clear anyway.\n",
        "\n",
        "We need some Noise-aware training techniques. Some ideas are:\n",
        "- training using a loss function that is designed to be robust to label noise. Such as:\n",
        "  - focal loss, it down-weighs the contribution of easy examples and puts more ephasis on difficult examples, which can make it resistant to noise. `torch.nn.FocalLoss`\n",
        "  - Generalized Cross-Entropy loss, which includes additional hyperparameters that allow the model to learn the noise rate and label corruption matrix. `torch.nn.GCE`\n",
        "- bootstrapping\n",
        "- self-ensembling to learn multiple models that are more robust to noise\n",
        "- Confidence based method, because predicting the confidence may allow humans to discard the least confidence examples. ( but we are not creating a real world model, we are getting tested automatically with a test set, so this method cannot work.)\n",
        "- Pseudo-labeling, using the model's own predictions to re-label a portion of the training data. Similar to self-training, we eliminate completely labels of some samples and we try to predict them. In case the labels are the same after relabling, there is a higher chance they are correct. We would gradually relable the images into a better dataset. \n",
        "    - Or Computing a probability that the labels are misslabeled, than using that probability to weigh heavier on the labels that have a higher chance of being correct during training. `torch.nn.CrossEntropyLoss`\n",
        "- Noise tolerant algorithms: complex deep learning models are more tolerant to noise. (this is not a very solid technique but should be used together with others.)\n",
        "\n",
        "References:\n",
        "- Focal loss: This loss function was first introduced in the paper \"Focal Loss for Dense Object Detection\" by Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár (https://arxiv.org/abs/1708.02002).\n",
        "- Generalized cross-entropy loss: This loss function was proposed in the paper \"Learning with Noisy Labels\" by Mengye Ren, Elad Hazan, and Yoram Singer (https://arxiv.org/abs/1609.03683).\n",
        "- Bootstrapping: This technique involves training multiple models on different subsets of the training data, and then combining their predictions to make a final prediction. It was first introduced in the paper \"Bagging Predictors\" by Leo Breiman (https://link.springer.com/article/10.1023/A:1018054314350).\n",
        "- Self-ensembling: This technique involves training multiple models on the same data, and then using their predictions to create a final prediction. It was first introduced in the paper \"Self-Ensembling for Visual Domain Adaptation\" by Sergey Zagoruyko and Nikos Komodakis (https://arxiv.org/abs/1706.05208).\n",
        "- Pseudo-labeling: This technique involves using the model's own predictions to label a portion of the training data. It was first introduced in the paper \"Semi-Supervised Learning with Deep Generative Models\" by Diederik Kingma, Danilo Jimenez Rezende, Shakir Mohamed, and Max Welling (https://arxiv.org/abs/1406.5298).\n",
        "- Noise-aware training: This refers to techniques that are specifically designed to handle label noise in the training data. One example of such a technique is the generalized cross-entropy loss function, which was introduced in the paper \"Learning with Noisy Labels\" by Mengye Ren, Elad Hazan, and Yoram Singer (https://arxiv.org/abs/1609.03683).\n",
        "- Confidence based methods: This refers to techniques that involve training a model to predict the confidence or probability of each class label, rather than just the class label itself. This can allow the model to identify examples that are less confident and potentially more prone to noise. One example of a confidence-based method is the method of \"bootstrapping,\" which was introduced in the paper \"Bagging Predictors\" by Leo Breiman (https://link.springer.com/article/10.1023/A:1018054314350)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeBHHsZt_IYU"
      },
      "source": [
        "## Pseudo Labeling solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnkncVA__IYW"
      },
      "source": [
        "\n",
        "This technique is barely described above\n",
        "\n",
        "The first question is what portion of the dataset should I \"forget\" the labels and try predict new ones(by training on the remaining ones)? \n",
        "\n",
        "Making the worse assumption that half of the labels are wrong. Than we should leave out a smaller portion of the labels to be re-labeled.\n",
        "But because we have a rather large dataset of 500 images per class and that the model we are using is a rather complex one which is already pretrained, we may increase the number of labels for re-labeling. \n",
        "\n",
        "According to previous argument I chose to separate the dataset in 10 parts. \n",
        "I will leave 10% of the dataset out and only use the rest of 90%. \n",
        "Similar to a 10-fold cross validation, but instead of validating we are re-labeling the remaining 10% of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmzRZhui_IYY"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x_I1fk99_IYZ"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "import shutil\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import SubsetRandomSampler, Dataset, DataLoader\n",
        "from torchvision import transforms, datasets \n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import itertools\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yyOZc59J_IYc"
      },
      "outputs": [],
      "source": [
        "task2_id = \"1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wCcacF4f_IYe",
        "outputId": "4c066fd1-cf53-4fcb-94f4-5541675e8eae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-31 13:47:10--  https://docs.google.com/uc?export=download&confirm=t&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.24.100, 74.125.24.102, 74.125.24.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.24.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-8k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hpf1s7iefa506dfesahfhq54i3jfcceg/1672494375000/08997952672865575084/*/1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL?e=download&uuid=d022b4db-2461-4b44-9673-dce9b63509c3 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-31 13:47:11--  https://doc-04-8k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hpf1s7iefa506dfesahfhq54i3jfcceg/1672494375000/08997952672865575084/*/1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL?e=download&uuid=d022b4db-2461-4b44-9673-dce9b63509c3\n",
            "Resolving doc-04-8k-docs.googleusercontent.com (doc-04-8k-docs.googleusercontent.com)... 172.253.118.132, 2404:6800:4003:c05::84\n",
            "Connecting to doc-04-8k-docs.googleusercontent.com (doc-04-8k-docs.googleusercontent.com)|172.253.118.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75324736 (72M) [application/x-gzip]\n",
            "Saving to: ‘task2.tar.gz’\n",
            "\n",
            "task2.tar.gz        100%[===================>]  71.83M  30.7MB/s    in 2.3s    \n",
            "\n",
            "2022-12-31 13:47:14 (30.7 MB/s) - ‘task2.tar.gz’ saved [75324736/75324736]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# replace here your ide &id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\"\n",
        "# replace here your id 'https://docs.google.com/uc?export=download&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL'\n",
        "# replace here your target name -O task1.tar.gz &&\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1GUF43k4PJX8YvNUWJbE1RnmXeI7nVbOL\" -O task2.tar.gz && rm -rf /tmp/cookies.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1bkbvqeF_IYf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!mkdir data\n",
        "!mv task2.tar.gz ./data\n",
        "!tar -xzvf \"/content/data/task2.tar.gz\" -C \"/content/data/\"     #[run this cell to extract tar.gz files]\n",
        "# this may take 12 seconds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_info = {\n",
        "    \"iteration\": 1,\n",
        "    \"image_processing\":{\n",
        "        \"resize\":224,\n",
        "        \"mean\":[0.485, 0.456, 0.406],\n",
        "        \"std\":[0.229, 0.224, 0.225],\n",
        "    },\n",
        "    \"hyperparameters_data\": {\n",
        "        \"batch_size\":32,\n",
        "        \"shuffle_dataloader\":True,\n",
        "        \"num_workers\":4\n",
        "    },\n",
        "    \"random_seeds\":{\n",
        "        \"torch_seed\":42,\n",
        "        \"numpy_seed\":42,\n",
        "        \"cuda_seed\":42,\n",
        "\n",
        "    },\n",
        "    \"hyperparameters_training\":{\n",
        "        \"learning_rate\": 0.0001,\n",
        "        \"scheduler_step_size\":7,\n",
        "        \"scheduler_gamma\":0.1,\n",
        "        \"num_epochs\":10, \n",
        "    },\n",
        "    \"total_unlabeled\":26445,\n",
        "}\n",
        "     "
      ],
      "metadata": {
        "id": "DLqmpUlecPCx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euHUUz0w_IYg"
      },
      "source": [
        "## Mount drive \n",
        "( in order to save each iteration of leave out labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-R4ZCnSB_IYh",
        "outputId": "c80126d9-b5c6-4c97-f8bf-cf32ef54b5cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_j0gmw68_IYi",
        "outputId": "e9af3428-22a5-4779-e7f3-e1902be0f26f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Google Drive!"
          ]
        }
      ],
      "source": [
        "with open('/gdrive/MyDrive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat '/gdrive/MyDrive/foo.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0w0JOuB5_IYj"
      },
      "outputs": [],
      "source": [
        "!rm '/gdrive/MyDrive/foo.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21Vc-VIr_IYk"
      },
      "source": [
        "## Seting up things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JQr89Fhv_IYl"
      },
      "outputs": [],
      "source": [
        "# Set up seeds for reproducibility\n",
        "np.random.seed(experiment_info[\"random_seeds\"][\"numpy_seed\"])\n",
        "torch.manual_seed(experiment_info[\"random_seeds\"][\"torch_seed\"])\n",
        "torch.cuda.manual_seed_all(experiment_info[\"random_seeds\"][\"cuda_seed\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "M51baF3B_IYn"
      },
      "outputs": [],
      "source": [
        "dir_data = 'data/task2/train_data/'\n",
        "# Read the annotations file into a DataFrame\n",
        "df = pd.read_csv(f'{dir_data}annotations.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "n9G-lt1HbzUp",
        "outputId": "ec3605a6-1e67-444e-9a9a-03c945d1308c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     renamed_path  label_idx\n",
              "0          task2/train_data/images/labeled/0.jpeg          0\n",
              "1          task2/train_data/images/labeled/1.jpeg          0\n",
              "2          task2/train_data/images/labeled/2.jpeg          0\n",
              "3          task2/train_data/images/labeled/3.jpeg          0\n",
              "4          task2/train_data/images/labeled/4.jpeg          0\n",
              "...                                           ...        ...\n",
              "49995  task2/train_data/images/labeled/49995.jpeg          5\n",
              "49996  task2/train_data/images/labeled/49996.jpeg         94\n",
              "49997  task2/train_data/images/labeled/49997.jpeg         24\n",
              "49998  task2/train_data/images/labeled/49998.jpeg         85\n",
              "49999  task2/train_data/images/labeled/49999.jpeg          5\n",
              "\n",
              "[50000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6e42356e-d77c-44ee-b9a5-cbe8fff16251\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>renamed_path</th>\n",
              "      <th>label_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>task2/train_data/images/labeled/0.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>task2/train_data/images/labeled/1.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>task2/train_data/images/labeled/2.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>task2/train_data/images/labeled/3.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>task2/train_data/images/labeled/4.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>task2/train_data/images/labeled/49995.jpeg</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>task2/train_data/images/labeled/49996.jpeg</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>task2/train_data/images/labeled/49997.jpeg</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>task2/train_data/images/labeled/49998.jpeg</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>task2/train_data/images/labeled/49999.jpeg</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e42356e-d77c-44ee-b9a5-cbe8fff16251')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6e42356e-d77c-44ee-b9a5-cbe8fff16251 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6e42356e-d77c-44ee-b9a5-cbe8fff16251');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YhxmBea3_IYo",
        "outputId": "04d18b4f-5d4b-4675-9159-6f4670284dee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/task2/images_by_class has been deleted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:11<00:00, 4343.12it/s]\n"
          ]
        }
      ],
      "source": [
        "# Define the base directory\n",
        "\n",
        "base_dir = 'data/task2/train_data/images'\n",
        "# Check if the directory exists, to recreate it instead of messing it up\n",
        "target_dir = 'data/task2/images_by_class'\n",
        "\n",
        "if os.path.exists(target_dir):\n",
        "    # Use rmtree to delete the directory and all its contents\n",
        "    shutil.rmtree(target_dir)\n",
        "    print(f'{target_dir} has been deleted')\n",
        "else:\n",
        "    print(f'{target_dir} does not exist')\n",
        "\n",
        "# Iterate over the rows in the DataFrame\n",
        "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    # Extract the path and class from the row\n",
        "    path = row['renamed_path']\n",
        "    label = row['label_idx']\n",
        "    \n",
        "    # Create the directory for the class\n",
        "    class_dir = f'{target_dir}/{label}'\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    \n",
        "    # Copy the file to the class directory\n",
        "    shutil.copy(f\"data/{path}\", class_dir) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create datset splits for pseudo-labeling"
      ],
      "metadata": {
        "id": "FSoVcX3tdmlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(experiment_info[\"image_processing\"][\"resize\"]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=experiment_info[\"image_processing\"][\"mean\"],\n",
        "                         std=experiment_info[\"image_processing\"][\"std\"]),\n",
        "    ])"
      ],
      "metadata": {
        "id": "pOGkTh1scJR_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ShuffledImageFolder(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.indices = torch.randperm(len(dataset))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path, _ = self.dataset.samples[self.indices[index]]\n",
        "        image, label = self.dataset[self.indices[index]]\n",
        "        return image, label, path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "data_dir = 'data/task2/images_by_class'\n",
        "image_dataset_unshuffled = datasets.ImageFolder(data_dir, preprocess)\n",
        "image_dataset = ShuffledImageFolder(image_dataset_unshuffled)"
      ],
      "metadata": {
        "id": "7iflapmiM4QO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = len(image_dataset)\n",
        "\n",
        "# Split the image_dataset into 10 parts\n",
        "part_size = num_samples // 10\n",
        "parts = [list(range(i * part_size, (i + 1) * part_size)) for i in range(10)]\n",
        "\n",
        "batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"]\n",
        "# Use one part for creating new labels and the other 9 parts for training the model\n",
        "loaders = {}\n",
        "for i in range(10):\n",
        "    # Get the indices for the training set, which are not in the labeling set\n",
        "    train_indices = [index for j, part in enumerate(parts) if j != i for index in part]\n",
        "    labeling_indices = parts[i]\n",
        "\n",
        "    # Create a sampler for the training set and the labeling set\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    labeling_sampler = SubsetRandomSampler(labeling_indices)\n",
        "\n",
        "    # Use the samplers to create data loaders for the training set and the labeling set\n",
        "    train_loader = DataLoader(image_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    labeling_loader = DataLoader(image_dataset, batch_size=batch_size, sampler=labeling_sampler)\n",
        "    loaders[i] = {\n",
        "        \"train\":train_loader, \n",
        "        \"labeling\":labeling_loader\n",
        "    }\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data_sizes = {\n",
        "    \"train\" : len(loaders[0][\"train\"]), # this is the number of batches not images\n",
        "    \"labeling\" : len(loaders[0][\"labeling\"]),\n",
        "}"
      ],
      "metadata": {
        "id": "aMpLE-QAvxFc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sanity check\n",
        "See how many labels are in each training set\n",
        "Visualize the distribution of labels"
      ],
      "metadata": {
        "id": "J1WLWbz0VBiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(len(loaders[0][\"train\"])*batch_size)\n",
        "print(len(loaders[0][\"labeling\"])*batch_size)\n"
      ],
      "metadata": {
        "id": "OrjSwJ29xbgv",
        "outputId": "6736d904-0171-410a-9774-726ea84f64a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45024\n",
            "5024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check, verify the distribution of labels in training and labeling sets\n",
        "labels_for_distribution = [label_batch for _,label_batch,_ in tqdm(loaders[0][\"train\"])]"
      ],
      "metadata": {
        "id": "8GIbW5b8TW-B",
        "outputId": "f8d448c9-864d-4df6-d739-070cf41ceec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [01:23<00:00, 16.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_distrib = torch.cat(labels_for_distribution)"
      ],
      "metadata": {
        "id": "VkOog_OqUEON"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts, bins, patches = plt.hist(labels_distrib, bins = 100)\n",
        "\n",
        "# Set x-axis label\n",
        "plt.xlabel('Labels')\n",
        "plt.xticks(bins, bins.astype(int))\n",
        "# Set y-axis label\n",
        "plt.ylabel('Number of Observations')\n",
        "\n",
        "# Set plot title\n",
        "plt.title('Distribution of Labels')  "
      ],
      "metadata": {
        "id": "z1Mj6kPMuCPN",
        "outputId": "7eea9428-09c3-49f3-fbfd-6d84b798d4a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Distribution of Labels')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on the remaining data "
      ],
      "metadata": {
        "id": "dgSVdENHZedr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a pytorch MobileNet pretrained model\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "# change the Linear output to fit our dataset ( because model initially has 1000 classes)\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "# Setting hyperparameters\n",
        "\n",
        "model = model.to(device)\n",
        "print(model.classifier)\n",
        "     "
      ],
      "metadata": {
        "id": "C69DiUz3ZkEV",
        "outputId": "2eab0188-0052-48d7-de51-10af01ec0845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252,
          "referenced_widgets": [
            "eb68d62d599a462f92aaca0ab0b854e3",
            "6eed1376f1f24e8abbe2d4ea0ca3fcb1",
            "07c37bb6b22b4c8abe8d16b61e28e1db",
            "15a23332521441cab29f7acedc079b16",
            "816420dd910b475faa99a8e9ef6eebd4",
            "71a16d1906924e738fc1baa304d94811",
            "a74028e913de43d3b1eae97433d91882",
            "07868acd1960497680b904b81fd0eaf9",
            "a56ed7ebe21c4011a23a63cad606d40c",
            "ba437891973d4629b4881ccb11f044ae",
            "a75d7f60d1c24475a1540bc1cfe94f7b"
          ]
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb68d62d599a462f92aaca0ab0b854e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=False)\n",
            "  (1): Linear(in_features=1280, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "toHy-rW4_IYr"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, scheduler, optimizer, criterion, dataset_size, dataloader):\n",
        "                     \n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over data.\n",
        "    for inputs, labels, _ in tqdm(dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "    \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_loss = running_loss / dataset_size\n",
        "    epoch_acc = running_corrects.double() / dataset_size\n",
        "    return model, epoch_loss, epoch_acc\n",
        "\n",
        "def train_model(model, *args, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()  # Set model to training mode\n",
        "        model, epoch_loss, epoch_acc = train_loop(model, *args)\n",
        "        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # deep copy the model\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=experiment_info[\"hyperparameters_training\"][\"learning_rate\"],\n",
        "    )\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(\n",
        "    optimizer, \n",
        "    step_size=experiment_info[\"hyperparameters_training\"][\"scheduler_step_size\"], \n",
        "    gamma=experiment_info[\"hyperparameters_training\"][\"scheduler_gamma\"],\n",
        "    )\n",
        "     "
      ],
      "metadata": {
        "id": "8Mprko1yaAI-"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(model, \n",
        "                    exp_lr_scheduler, \n",
        "                    optimizer, \n",
        "                    criterion,  \n",
        "                    data_sizes[\"train\"], \n",
        "                    loaders[0][\"train\"], \n",
        "                    num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])"
      ],
      "metadata": {
        "id": "vmCqNZ-saHr3",
        "outputId": "badfeed4-5771-4c1f-eca6-47f93af79d12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:26<00:00,  5.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 72.5423 Acc: 13.7484\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 45.0769 Acc: 19.0071\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:18<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 36.1820 Acc: 20.9026\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 29.3312 Acc: 22.5622\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 23.8855 Acc: 24.1471\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1407/1407 [04:16<00:00,  5.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 19.0713 Acc: 25.5572\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 1147/1407 [03:28<00:47,  5.43it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), '/gdrive/MyDrive/checkpoints/noisy_labels/model_it_0.pt')"
      ],
      "metadata": {
        "id": "L5s4zlU1j_Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = {\n",
        "    \"path\":[],\n",
        "    \"confidence\":[],\n",
        "    \"label\":[]\n",
        "}\n",
        "\n",
        "# Each epoch has a training and validation phase \n",
        "model.eval()   # Set model to evaluate mode \n",
        "\n",
        "# Iterate over data.\n",
        "for inputs, _, path in tqdm(loaders[0][\"train\"]):\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    # forward\n",
        "    with torch.set_grad_enabled(False): # we don't want to train\n",
        "        outputs = model(inputs)\n",
        "        confidence, preds = torch.max(outputs, 1) \n",
        "    predicted_labels[\"path\"].append(path) \n",
        "    predicted_labels[\"confidence\"].append(confidence.item())\n",
        "    predicted_labels[\"label\"].append(preds.item())\n",
        "\n"
      ],
      "metadata": {
        "id": "f6Ug33D9kJqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the experiment information from the JSON file\n",
        "num_iteration = 0\n",
        "# Save the experiment information to a JSON file\n",
        "with open(f'/gdrive/MyDrive/checkpoints/iteration_{num_iteration}_images.json', 'w') as f:\n",
        "    json.dump(predicted_labels, f)"
      ],
      "metadata": {
        "id": "-8HUiGnjk1zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeat process for the other iterations"
      ],
      "metadata": {
        "id": "jIZWJhNplVOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4lRKFM-_IYt"
      },
      "outputs": [],
      "source": [
        "# TODO:  save each iteration labels preferably a dataframe into a csv\n",
        "# TODO, during the training, for each iteration, \n",
        "# add a new set of labels to the csv that tells us the annotations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: at the end of the training phase\n",
        "# use the predictions made by the models \n",
        "# to choose the images with the highest confidence\n",
        "# by selecting the images that consistently have the same label\n",
        "# this set of images will become an always used \n",
        "# set in the next training phase, it should not be forgetten again\n",
        "# because there is a high confidence that these labels have the \n",
        "# correct labels \n",
        "# because I have leave 9 out technique,\n",
        "# I will have 9 labels for each image\n",
        "# and also the original label.\n",
        "# the probability that those labels are correct can be \n",
        "# simply counting the label that occurs most often\n",
        "# than dividing that number by 10"
      ],
      "metadata": {
        "id": "1DsisAptHYxA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb68d62d599a462f92aaca0ab0b854e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6eed1376f1f24e8abbe2d4ea0ca3fcb1",
              "IPY_MODEL_07c37bb6b22b4c8abe8d16b61e28e1db",
              "IPY_MODEL_15a23332521441cab29f7acedc079b16"
            ],
            "layout": "IPY_MODEL_816420dd910b475faa99a8e9ef6eebd4"
          }
        },
        "6eed1376f1f24e8abbe2d4ea0ca3fcb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71a16d1906924e738fc1baa304d94811",
            "placeholder": "​",
            "style": "IPY_MODEL_a74028e913de43d3b1eae97433d91882",
            "value": "100%"
          }
        },
        "07c37bb6b22b4c8abe8d16b61e28e1db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07868acd1960497680b904b81fd0eaf9",
            "max": 14212972,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a56ed7ebe21c4011a23a63cad606d40c",
            "value": 14212972
          }
        },
        "15a23332521441cab29f7acedc079b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba437891973d4629b4881ccb11f044ae",
            "placeholder": "​",
            "style": "IPY_MODEL_a75d7f60d1c24475a1540bc1cfe94f7b",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 62.9MB/s]"
          }
        },
        "816420dd910b475faa99a8e9ef6eebd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71a16d1906924e738fc1baa304d94811": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a74028e913de43d3b1eae97433d91882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07868acd1960497680b904b81fd0eaf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a56ed7ebe21c4011a23a63cad606d40c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba437891973d4629b4881ccb11f044ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a75d7f60d1c24475a1540bc1cfe94f7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}