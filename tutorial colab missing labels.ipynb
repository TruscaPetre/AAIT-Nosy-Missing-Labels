{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TruscaPetre/AAIT-Nosy-Missing-Labels/blob/main/tutorial%20colab%20missing%20labels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Missing Labels problem"
      ],
      "metadata": {
        "id": "eI4ZU9oa5eh8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pc4qypVPpcu"
      },
      "source": [
        "## Theory about missing labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZCBR60FPpcu"
      },
      "source": [
        "\n",
        "- The classification problem can be tackled using semi-supervised learning methods. There are 3 propular approaches to address semi-supervised learning problems:\n",
        "  - self training\n",
        "  - co-training\n",
        "  - graph-based models\n",
        "- self training \n",
        "  - What is the process?\n",
        "    - First train on labeled data\n",
        "    - Make predictions on the untrained data\n",
        "    - From those predictions, only extract those with high confidence and move them to the training data\n",
        "    - Repeat the process until convergence or no high-confidence exmples left in the unlabeled set.\n",
        "  - Disadvantage is that you can still generate incorrect predictions and the mistakes can be amplified in the subsequent trainings of the model.\n",
        "- co-training\n",
        "  - requires two feature representations associated with the dataset which serve as two different views of the data\n",
        "  - The representations are dissimilar and conditionally independet, but they can provide complementary information about the data. \n",
        "  - We cannot do this for image classification because we have only visual representations of the data.\n",
        "- graph-based models\n",
        "  - labeled and unlabeled samples are represented as different nodes in a graph\n",
        "  - the edges in this graph denote the similarity between nodes.\n",
        "  - The assumption in this approach is that nodes with strong edges are likely to share the same label.\n",
        "  - The algorithm to compute the labels is:\n",
        "    - The unlabeled nodes can be labeled using random-walk over the graph. Based on the strength of the edges.\n",
        "    - The walk ends when a labeled node is reached \n",
        "    - A probability that the random walker started at a particular unlabeled node given that it ended at a specific labeled node is computed. i.e. 2 poitns are similar if they have indistinguishabel starting points.\n",
        "\n",
        "\n",
        "Reference: \n",
        "- https://www.kdnuggets.com/2019/11/tips-class-imbalance-missing-labels.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports, libraries, datasets, preparations"
      ],
      "metadata": {
        "id": "_lQts5FFgIx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import shutil\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import SubsetRandomSampler, Dataset, DataLoader\n",
        "from torchvision import transforms, datasets \n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import itertools\n",
        "\n"
      ],
      "metadata": {
        "id": "IhZAai2_gjxi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task1_id = \"1dO1vqCoJm2xwrnr171A6_eW7ikd-alrd\""
      ],
      "metadata": {
        "id": "OjQSLPl1gQJm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace here your ide &id=1dO1vqCoJm2xwrnr171A6_eW7ikd-alrd\"\n",
        "# replace here your id 'https://docs.google.com/uc?export=download&id=1dO1vqCoJm2xwrnr171A6_eW7ikd-alrd'\n",
        "# replace here your target name -O task1.tar.gz &&\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1dO1vqCoJm2xwrnr171A6_eW7ikd-alrd' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1dO1vqCoJm2xwrnr171A6_eW7ikd-alrd\" -O task1.tar.gz && rm -rf /tmp/cookies.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcKpxyMqgUPw",
        "outputId": "95458a82-9949-4e4b-e2f0-4db5d6d28bfa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-10 17:36:24--  https://docs.google.com/uc?export=download&confirm=t&id=1dO1vqCoJm2xwrnr171A6_eW7ikd-alrd\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.203.138, 172.217.203.102, 172.217.203.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.203.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0g-8k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/rt8q1ditccjjnpo83bpmo4dbcid6ckt8/1673372175000/08997952672865575084/*/1dO1vqCoJm2xwrnr171A6_eW7ikd-alrd?e=download&uuid=394fb3f7-3475-4426-8dec-fe72e4ec0e86 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-01-10 17:36:24--  https://doc-0g-8k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/rt8q1ditccjjnpo83bpmo4dbcid6ckt8/1673372175000/08997952672865575084/*/1dO1vqCoJm2xwrnr171A6_eW7ikd-alrd?e=download&uuid=394fb3f7-3475-4426-8dec-fe72e4ec0e86\n",
            "Resolving doc-0g-8k-docs.googleusercontent.com (doc-0g-8k-docs.googleusercontent.com)... 172.217.204.132, 2607:f8b0:400c:c15::84\n",
            "Connecting to doc-0g-8k-docs.googleusercontent.com (doc-0g-8k-docs.googleusercontent.com)|172.217.204.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75378011 (72M) [application/x-gzip]\n",
            "Saving to: ‘task1.tar.gz’\n",
            "\n",
            "task1.tar.gz        100%[===================>]  71.89M   134MB/s    in 0.5s    \n",
            "\n",
            "2023-01-10 17:36:25 (134 MB/s) - ‘task1.tar.gz’ saved [75378011/75378011]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!mkdir data\n",
        "!mv task1.tar.gz ./data\n",
        "!tar -xzvf \"/content/data/task1.tar.gz\" -C \"/content/data/\"     #[run this cell to extract tar.gz files]\n",
        "# this may take 12 seconds"
      ],
      "metadata": {
        "id": "zeNl_38DgUKU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r /content/data/task1"
      ],
      "metadata": {
        "id": "b1TIiMGvNsCu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up logging"
      ],
      "metadata": {
        "id": "pvhmgZwAewOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This experiment will take long, because there are many trianig periods involved and many changes going on in the data. There are some things that could be helpful to track along the way. So we set up a logger that will do this.\n",
        "\n",
        "We are going to track: \n",
        "- all the hyperparameters for training.\n",
        "- all the seeds for RNG\n",
        "- the images that have been labeled at each iteration. \n",
        "- performance metrics ( we should see them increase through each iteration )\n",
        "\n",
        "We are going to use a dictionary that will save all the data from each iteration of the experiment. Than save that dictionary into a json file for each iteration. \n",
        "\n",
        "There is a json file in the github repository with all the parameters for the first experiment.\n",
        "\n",
        "Some comments about setting those parameters:\n",
        "- The number of epochs is only 10 because according to the training only on the dataset with labeled data, this is the point where the validation set is reaching a saturation. In order to achieve the results faster from self-training, we should keep this number as small as possible."
      ],
      "metadata": {
        "id": "COa9lK9UMopU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_info = {\n",
        "    \"iteration\": 1,\n",
        "    \"image_processing\":{\n",
        "        \"resize\":224,\n",
        "        \"mean\":[0.485, 0.456, 0.406],\n",
        "        \"std\":[0.229, 0.224, 0.225],\n",
        "    },\n",
        "    \"hyperparameters_data\": {\n",
        "        \"batch_size\":32,\n",
        "        \"shuffle_dataloader\":True,\n",
        "        \"num_workers\":4\n",
        "    },\n",
        "    \"random_seeds\":{\n",
        "        \"torch_seed\":42,\n",
        "        \"numpy_seed\":42,\n",
        "        \"cuda_seed\":42,\n",
        "\n",
        "    },\n",
        "    \"hyperparameters_training\":{\n",
        "        \"learning_rate\": 0.0001,\n",
        "        \"scheduler_step_size\":7,\n",
        "        \"scheduler_gamma\":0.1,\n",
        "        \"num_epochs\":10, \n",
        "    },\n",
        "    \"total_unlabeled\":26445,\n",
        "}"
      ],
      "metadata": {
        "id": "9SmFrNSuOwdX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dictionary to a file\n",
        "with open(\"experiment_info.json\", \"w\") as f:\n",
        "    json.dump(experiment_info, f)"
      ],
      "metadata": {
        "id": "LPHtsWEvO5wS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(experiment_info[\"random_seeds\"][\"numpy_seed\"])\n",
        "torch.manual_seed(experiment_info[\"random_seeds\"][\"torch_seed\"])\n",
        "torch.cuda.manual_seed_all(experiment_info[\"random_seeds\"][\"cuda_seed\"])"
      ],
      "metadata": {
        "id": "rvkgC-zJaTqC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the experiment information from the JSON file\n",
        "with open('experiment_info.json', 'r') as f:\n",
        "    loaded_experiment_info = json.load(f)\n",
        "\n",
        "display(loaded_experiment_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jzRL9HznbF9V",
        "outputId": "9dcef919-4b49-4a03-a461-382c2e139e5c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'iteration': 1,\n",
              " 'image_processing': {'resize': 224,\n",
              "  'mean': [0.485, 0.456, 0.406],\n",
              "  'std': [0.229, 0.224, 0.225]},\n",
              " 'hyperparameters_data': {'batch_size': 32,\n",
              "  'shuffle_dataloader': True,\n",
              "  'num_workers': 4},\n",
              " 'random_seeds': {'torch_seed': 42, 'numpy_seed': 42, 'cuda_seed': 42},\n",
              " 'hyperparameters_training': {'learning_rate': 0.0001,\n",
              "  'scheduler_step_size': 7,\n",
              "  'scheduler_gamma': 0.1,\n",
              "  'num_epochs': 10},\n",
              " 'total_unlabeled': 26445}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount drive"
      ],
      "metadata": {
        "id": "mMp1fV2xezNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each iteration of self-training takes about 1 hour on a GPU. So you might want to save and restart the training a few times. In order to restart the model where we where left of, we are connecting google drive and saving the model there."
      ],
      "metadata": {
        "id": "neHJVMiEe0hW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNowNASxfOSW",
        "outputId": "3c84aeec-54c5-46cb-c4f1-34146efdaf26"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/gdrive/MyDrive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat '/gdrive/MyDrive/foo.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vMEAFjFfOSX",
        "outputId": "26306526-dda8-4e2c-91c0-26468d1501ff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Google Drive!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm '/gdrive/MyDrive/foo.txt'"
      ],
      "metadata": {
        "id": "oApaDsjmhHlz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare datasets"
      ],
      "metadata": {
        "id": "PEYc29buibzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir_data = 'data/task1/train_data/'\n",
        "# Read the annotations file into a DataFrame\n",
        "df = pd.read_csv(f'{dir_data}annotations.csv')"
      ],
      "metadata": {
        "id": "0EBMTlxkK9vZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Organize data for pytorch training ( only once )\n",
        "# Define the base directory\n",
        "base_dir = 'data/task1/labeled'\n",
        "\n",
        "# Iterate over the rows in the DataFrame\n",
        "for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    # Extract the path and class from the row\n",
        "    path = row['sample']\n",
        "    label = row['label']\n",
        "    \n",
        "    # Create the directory for the class\n",
        "    class_dir = f'{base_dir}/{label}'\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    \n",
        "    # Copy the file to the class directory\n",
        "    shutil.copy(f\"data/{path}\", class_dir) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnhwuNg6hnZA",
        "outputId": "55befcb1-9285-40b3-e61d-d241d6706465"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23555/23555 [00:05<00:00, 4083.94it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(experiment_info[\"image_processing\"][\"resize\"]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=experiment_info[\"image_processing\"][\"mean\"],\n",
        "                         std=experiment_info[\"image_processing\"][\"std\"]),\n",
        "    ])\n",
        "\n",
        "data_dir = 'data/task1/labeled'\n",
        "image_dataset = datasets.ImageFolder(data_dir, preprocess) \n",
        "dataloader  = DataLoader(\n",
        "    image_dataset, \n",
        "    batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"],\n",
        "    shuffle = experiment_info[\"hyperparameters_data\"][\"shuffle_dataloader\"], \n",
        "    num_workers = experiment_info[\"hyperparameters_data\"][\"num_workers\"]\n",
        "    )\n",
        "\n",
        "class_names = image_dataset.classes\n",
        "dataset_size = len(image_dataset)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "sGgy_3qljJsb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self Training Solution"
      ],
      "metadata": {
        "id": "aIMWvJWPgl8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on labeled data"
      ],
      "metadata": {
        "id": "FB1MI4vtgX2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download model"
      ],
      "metadata": {
        "id": "ROA_n_aAiY5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a pytorch MobileNet pretrained model\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "# change the Linear output to fit our dataset\n",
        "\n",
        "# the model has initially 1000 outputs\n",
        "# print(model.classifier)\n",
        "# > Sequential(\n",
        "#   (0): Dropout(p=0.2)\n",
        "#   (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
        "# )\n",
        "\n",
        "model.classifier[1] = nn.Linear(1280, 100)\n",
        "print(model.classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "a275e3abe13e468980a250b54e91cce5",
            "cd626424afd5436083780d3cde02183d",
            "0e0f18dedf33441b94d1c9af20145937",
            "89bd4155836b4534833cc9bbd4fe49c1",
            "10e839197ffd46ac96eaf4516560aa18",
            "de5364f2f3734bccbe99ccb326f583e0",
            "e7841a1807b6457b8b256198c9f7bae8",
            "ce6888951733437397f2758cff71350b",
            "8e74c9da734048169485cb0cd5ce858c",
            "cfe4982d664d445bbc8366d25c1c8bb3",
            "846994e6f25b4dba80a1f88d54de5471"
          ]
        },
        "id": "eOfEof_ggfun",
        "outputId": "88121926-6edd-49d4-fd81-42ae8dd6addf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a275e3abe13e468980a250b54e91cce5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=False)\n",
            "  (1): Linear(in_features=1280, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start training"
      ],
      "metadata": {
        "id": "Hq_hNMyajIJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(model, scheduler, optimizer, criterion, dataset_size, dataloader):\n",
        "                     \n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over data.\n",
        "    for inputs, labels in tqdm(dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "    \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_loss = running_loss / dataset_size\n",
        "    epoch_acc = running_corrects.double() / dataset_size\n",
        "    return model, epoch_loss, epoch_acc\n",
        "\n",
        "def train_model(model, *args, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()  # Set model to training mode\n",
        "        model, epoch_loss, epoch_acc = train_loop(model, *args)\n",
        "        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # deep copy the model\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "D2UyibBtifi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting hyperparameters\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=experiment_info[\"hyperparameters_training\"][\"learning_rate\"],\n",
        "    )\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(\n",
        "    optimizer, \n",
        "    step_size=experiment_info[\"hyperparameters_training\"][\"scheduler_step_size\"], \n",
        "    gamma=experiment_info[\"hyperparameters_training\"][\"scheduler_gamma\"],\n",
        "    )"
      ],
      "metadata": {
        "id": "Sr6RKnpHig2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(model, \n",
        "                    exp_lr_scheduler, \n",
        "                    optimizer, \n",
        "                    criterion,  \n",
        "                    dataset_size, \n",
        "                    dataloader, \n",
        "                    num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySuW76bXioXR",
        "outputId": "26308e2a-b705-4f0b-a676-c1f2105ccf41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 798/798 [01:46<00:00,  7.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.6056 Acc: 0.4191\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 798/798 [01:44<00:00,  7.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.4541 Acc: 0.6384\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 798/798 [01:44<00:00,  7.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.0391 Acc: 0.7274\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 62/798 [00:08<01:44,  7.01it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-223-6eecbd260d82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = train_model(model, \n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mdataset_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-221-4ab3db6dd98f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, *args)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set model to training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-221-4ab3db6dd98f>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, scheduler, optimizer, criterion, dataset_size, dataloader)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             adam(params_with_grad,\n\u001b[0m\u001b[1;32m    235\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    301\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapturable\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), '/gdrive/MyDrive/checkpoints/missing_labels/model_it_0.pt')"
      ],
      "metadata": {
        "id": "gOwyuRb5dgoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict new set of labels"
      ],
      "metadata": {
        "id": "7LBF-Ncmfwbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the source and destination directories\n",
        "source_dir = 'data/task1/train_data/images/unlabeled'\n",
        "destination_dir = 'data/task1/train_data/images/unlabeled/0'\n",
        "os.makedirs(destination_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "VGAMOM3cqaA4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over all the files in the source directory\n",
        "for file in tqdm(os.listdir(source_dir)):\n",
        "    if file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".png\"):\n",
        "        # Construct the source and destination paths\n",
        "        src_path = os.path.join(source_dir, file)\n",
        "        dst_path = os.path.join(destination_dir, file)\n",
        "        # Move the file\n",
        "        shutil.move(src_path, dst_path) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW1t-HsprUFY",
        "outputId": "f454bb6f-59dd-4c51-809f-8b2f6581c909"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26446/26446 [00:00<00:00, 30292.54it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check to see that all images have been moved to unlabeled/0 dir\n",
        "len(os.listdir(destination_dir)) # 26445"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6lI3CK_spou",
        "outputId": "81a72191-ce6b-4792-8217-8f5044c061ee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26445"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new dataloader with unlabeled data.\n",
        "# Dataset folder is a subclass of ImageFolder \n",
        "# Which will allow us to return also the path of the image\n",
        "# because we need it to know which images should be moved \n",
        "\n",
        "from torchvision.datasets import DatasetFolder\n",
        "\n",
        "data_dir = 'data/task1/train_data/images/unlabeled'\n",
        "image_dataset_unlabeled = datasets.ImageFolder(root=data_dir, transform=preprocess)\n",
        "\n",
        "dataloader_unlabeled  = torch.utils.data.DataLoader(\n",
        "    image_dataset_unlabeled, \n",
        "    batch_size = 1, \n",
        "    )\n",
        "\n",
        "dataset_size = len(image_dataset_unlabeled)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "fKVLIQKgnHNU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = {\n",
        "    \"path\":[],\n",
        "    \"confidence\":[],\n",
        "    \"label\":[]\n",
        "}\n",
        "\n",
        "# Each epoch has a training and validation phase \n",
        "model.eval()   # Set model to evaluate mode \n",
        "\n",
        "# Iterate over data.\n",
        "img_path_generator = ((image, path) for (path,_) , (image, _) in zip(image_dataset_unlabeled.samples, dataloader_unlabeled))\n",
        "\n",
        "for inputs, path in tqdm(img_path_generator, total=dataset_size):\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    # forward\n",
        "    with torch.set_grad_enabled(False): # we don't want to train\n",
        "        outputs = model(inputs)\n",
        "        confidence, preds = torch.max(outputs, 1) \n",
        "    predicted_labels[\"path\"].append(path) \n",
        "    predicted_labels[\"confidence\"].append(confidence.item())\n",
        "    predicted_labels[\"label\"].append(preds.item())\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qrr3m5F-m6yT",
        "outputId": "16434383-919f-4556-ee26-f7986c7568b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26445/26445 [03:43<00:00, 118.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the predictions"
      ],
      "metadata": {
        "id": "S2EtsR38FTOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a distribution of the convidence scores. To find a threshold\n",
        "counts, bins, patches = plt.hist(predicted_labels[\"confidence\"], bins = 20)\n",
        "\n",
        "# Set x-axis label\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.xticks(bins, bins.astype(int))\n",
        "# Set y-axis label\n",
        "plt.ylabel('Number of Observations')\n",
        "\n",
        "# Set plot title\n",
        "plt.title('Distribution of Confidence Scores')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-BACmxY85fu",
        "outputId": "1926ebb2-2461-4945-b958-a2653482e299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Distribution of Confidence Scores')"
            ]
          },
          "metadata": {},
          "execution_count": 172
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MSSEdHR_Mar",
        "outputId": "bda6ecf8-e6ba-4cf0-a3e6-03ab6f90459b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2.2711072  -0.63155334  1.00800052  2.64755437  4.28710823  5.92666209\n",
            "  7.56621594  9.2057698  10.84532366 12.48487751 14.12443137 15.76398523\n",
            " 17.40353909 19.04309294 20.6826468  22.32220066 23.96175451 25.60130837\n",
            " 27.24086223 28.88041608 30.51996994]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the top X bins but not more than 10% of the unlabeled dataset\n",
        "# to move into the labeled dataset\n",
        "top10 = experiment_info[\"total_unlabeled\"]*0.1\n",
        "if sum(counts)>top10:\n",
        "    for x in range(20):\n",
        "        top10-=counts[-x-1]\n",
        "        if top10 < 0:\n",
        "            break\n",
        "    n_keep = int(sum(counts[-x:]))\n",
        "else:\n",
        "    n_keep = int(sum(counts))\n",
        "print(n_keep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osEAvXC9FiNb",
        "outputId": "96825cf6-4517-463e-807a-0c1ebc16f06e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the confidence scores in descending order\n",
        "sorted_confidences = np.sort(predicted_labels[\"confidence\"])[::-1]\n",
        "\n",
        "# Keep the first n_keep scores\n",
        "top_confidences = sorted_confidences[:n_keep]\n",
        "\n",
        "# Find the indices of the top confidences in the original list of confidences\n",
        "top_confidence_indices = [i for i, c in enumerate(predicted_labels[\"confidence\"]) if c in top_confidences]\n",
        "\n",
        "# Use the indices to select the corresponding paths and labels\n",
        "top_confidences = [predicted_labels[\"confidence\"][i] for i in top_confidence_indices]\n",
        "top_paths = [predicted_labels[\"path\"][i] for i in top_confidence_indices]\n",
        "top_labels = [predicted_labels[\"label\"][i] for i in top_confidence_indices]"
      ],
      "metadata": {
        "id": "oNkR_xrpHbCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iteration_predictions = {\n",
        "    \"path\":top_paths,\n",
        "    \"confidence\":top_confidences,\n",
        "    \"label\":top_labels\n",
        "}"
      ],
      "metadata": {
        "id": "X3u7tKzgIHaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the experiment information from the JSON file\n",
        "num_iteration = 0\n",
        "# Save the experiment information to a JSON file\n",
        "with open(f'iteration_{num_iteration}_images.json', 'w') as f:\n",
        "    json.dump(iteration_predictions, f)"
      ],
      "metadata": {
        "id": "CUV2nr83I8cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the experiment information from the JSON file\n",
        "with open(f'iteration_{num_iteration}_images.json', 'r') as f:\n",
        "    iteration_predictions = json.load(f)\n",
        "\n",
        "df_labeled = pd.DataFrame(iteration_predictions)\n"
      ],
      "metadata": {
        "id": "0XWDIovwJPVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the images above the trashold in the dataset\n",
        "# Iterate through the selected rows and move the images\n",
        "for index, row in tqdm(df_labeled.iterrows(),total=n_keep):\n",
        "    parts = row['path'].split(\".\")\n",
        "    parts[0]+=f\"_it_{num_iteration}\"\n",
        "    new_name = \".\".join(parts).split('/')[-1]\n",
        "    shutil.move(row['path'], f\"data/task1/labeled/{row['label']}/{new_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZC0SypYKjiO",
        "outputId": "ecaf9355-e79a-4c75-f030-96987deeda67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1962/1962 [00:01<00:00, 1453.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check to see that all images have been moved to unlabeled/0 dir\n",
        " # \n",
        "assert experiment_info[\"total_unlabeled\"] - n_keep == len(os.listdir(destination_dir)) # 26445 - 1962 = 24483"
      ],
      "metadata": {
        "id": "Igt88y-iRsTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repeat the following iterations of self-training"
      ],
      "metadata": {
        "id": "iPJNQ8vCJbya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset():\n",
        "    data_dir = 'data/task1/labeled'\n",
        "    preprocess = transforms.Compose([\n",
        "    transforms.Resize(experiment_info[\"image_processing\"][\"resize\"]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=experiment_info[\"image_processing\"][\"mean\"],\n",
        "                         std=experiment_info[\"image_processing\"][\"std\"]),\n",
        "    ])\n",
        "    image_dataset = datasets.ImageFolder(data_dir, preprocess) \n",
        "    dataloader  = DataLoader(\n",
        "        image_dataset, \n",
        "        batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"],\n",
        "        shuffle = experiment_info[\"hyperparameters_data\"][\"shuffle_dataloader\"], \n",
        "        num_workers = experiment_info[\"hyperparameters_data\"][\"num_workers\"]\n",
        "        )\n",
        "\n",
        "    class_names = image_dataset.classes\n",
        "    dataset_size = len(image_dataset)\n",
        "    return class_names, dataset_size, dataloader\n",
        " \n",
        "\n",
        "def get_hyper_parameters():\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=experiment_info[\"hyperparameters_training\"][\"learning_rate\"],\n",
        "        )\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(\n",
        "        optimizer, \n",
        "        step_size=experiment_info[\"hyperparameters_training\"][\"scheduler_step_size\"], \n",
        "        gamma=experiment_info[\"hyperparameters_training\"][\"scheduler_gamma\"],\n",
        "        )\n",
        "    return criterion, optimizer, exp_lr_scheduler\n",
        "\n",
        "def get_unlabled_dataset():\n",
        "    data_dir = 'data/task1/train_data/images/unlabeled'\n",
        "    image_dataset_unlabeled = datasets.ImageFolder(root=data_dir, transform=preprocess)\n",
        "    dataloader_unlabeled  = torch.utils.data.DataLoader(\n",
        "        image_dataset_unlabeled, \n",
        "        batch_size = 1, \n",
        "        )\n",
        "    dataset_size_unlabeled = len(image_dataset_unlabeled)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    return dataset_size_unlabeled, dataloader_unlabeled, image_dataset_unlabeled\n",
        "\n",
        "def predict_new_labels(model):\n",
        "    dataset_size_unlabeled, dataloader_unlabeled, image_dataset_unlabeled = get_unlabled_dataset()\n",
        "    predicted_labels = {\n",
        "        \"path\":[],\n",
        "        \"confidence\":[],\n",
        "        \"label\":[]\n",
        "    }\n",
        "\n",
        "    # Each epoch has a training and validation phase \n",
        "    model.eval()   # Set model to evaluate mode \n",
        "\n",
        "    # Iterate over data.\n",
        "    img_path_generator = ((image, path) for (path,_) , (image, _) in zip(image_dataset_unlabeled.samples, dataloader_unlabeled))\n",
        "\n",
        "    for inputs, path in tqdm(img_path_generator, total=dataset_size_unlabeled):\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(False): # we don't want to train\n",
        "            outputs = model(inputs)\n",
        "            confidence, preds = torch.max(outputs, 1) \n",
        "        predicted_labels[\"path\"].append(path) \n",
        "        predicted_labels[\"confidence\"].append(confidence.item())\n",
        "        predicted_labels[\"label\"].append(preds.item())\n",
        "        \n",
        "    return predicted_labels\n",
        "\n",
        "def plot_distribution():\n",
        "    # Plot a distribution of the convidence scores. To find a threshold\n",
        "    counts, bins, patches = plt.hist(predicted_labels[\"confidence\"], bins = 20)\n",
        "\n",
        "    # Set x-axis label\n",
        "    plt.xlabel('Confidence Score')\n",
        "    plt.xticks(bins, bins.astype(int))\n",
        "    # Set y-axis label\n",
        "    plt.ylabel('Number of Observations')\n",
        "\n",
        "    # Set plot title\n",
        "    plt.title('Distribution of Confidence Scores')\n",
        "    return counts, bins, patches\n",
        "\n",
        "def compute_keep(counts):\n",
        "    top10 = experiment_info[\"total_unlabeled\"]*0.1\n",
        "    if sum(counts)>top10:\n",
        "        for x in range(20):\n",
        "            top10-=counts[-x-1]\n",
        "            if top10 < 0:\n",
        "                break\n",
        "        n_keep = int(sum(counts[-x:]))\n",
        "    else:\n",
        "        n_keep = int(sum(counts))\n",
        "    return n_keep\n",
        "\n",
        "def extract_kept_images(predicted_labels, n_keep):\n",
        "    # Sort the confidence scores in descending order\n",
        "    sorted_confidences = np.sort(predicted_labels[\"confidence\"])[::-1]\n",
        "\n",
        "    # Keep the first n_keep scores\n",
        "    top_confidences = sorted_confidences[:n_keep]\n",
        "\n",
        "    # Find the indices of the top confidences in the original list of confidences\n",
        "    top_confidence_indices = [i for i, c in enumerate(predicted_labels[\"confidence\"]) if c in top_confidences]\n",
        "\n",
        "    # Use the indices to select the corresponding paths and labels\n",
        "    top_confidences = [predicted_labels[\"confidence\"][i] for i in top_confidence_indices]\n",
        "    top_paths = [predicted_labels[\"path\"][i] for i in top_confidence_indices]\n",
        "    top_labels = [predicted_labels[\"label\"][i] for i in top_confidence_indices]\n",
        "    iteration_predictions = {\n",
        "        \"path\":top_paths,\n",
        "        \"confidence\":top_confidences,\n",
        "        \"label\":top_labels\n",
        "    }\n",
        "    return iteration_predictions\n",
        "\n",
        "def save_images_predicted_json(iteration_predictions, num_iteration):\n",
        "    # Save the experiment information to a JSON file\n",
        "    with open(f'/gdrive/MyDrive/checkpoints/missing_labels/iteration_{num_iteration}_images.json', 'w') as f:\n",
        "        json.dump(iteration_predictions, f)\n",
        "\n",
        "def move_images_predicted(num_iteration):\n",
        "    # Load the experiment information from the JSON file\n",
        "    with open(f'/gdrive/MyDrive/checkpoints/missing_labels/iteration_{num_iteration}_images.json', 'r') as f:\n",
        "        iteration_predictions = json.load(f)\n",
        "\n",
        "    df_labeled = pd.DataFrame(iteration_predictions)\n",
        "    # Move the images above the trashold in the dataset\n",
        "    # Iterate through the selected rows and move the images\n",
        "    for index, row in tqdm(df_labeled.iterrows(),total=n_keep):\n",
        "        parts = row['path'].split(\".\")\n",
        "        parts[0]+=f\"_it_{num_iteration}\"\n",
        "        new_name = \".\".join(parts).split('/')[-1]\n",
        "        shutil.move(row['path'], f\"data/task1/labeled/{row['label']}/{new_name}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LJ2-pwXZPaXq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_new_model(num_epochs=experiment_info[\"hyperparameters_training\"][\"num_epochs\"]):\n",
        "    # Download a pytorch MobileNet pretrained model\n",
        "    model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "    # change the Linear output to fit our dataset\n",
        "\n",
        "    # the model has initially 1000 outputs\n",
        "    # print(model.classifier)\n",
        "    # > Sequential(\n",
        "    #   (0): Dropout(p=0.2)\n",
        "    #   (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
        "    # )\n",
        "\n",
        "    model.classifier[1] = nn.Linear(1280, 100) \n",
        "\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(experiment_info[\"image_processing\"][\"resize\"]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=experiment_info[\"image_processing\"][\"mean\"],\n",
        "                            std=experiment_info[\"image_processing\"][\"std\"]),\n",
        "        ])\n",
        "\n",
        "    data_dir = 'data/task1/labeled'\n",
        "    image_dataset = datasets.ImageFolder(data_dir, preprocess) \n",
        "    dataloader  = DataLoader(\n",
        "        image_dataset, \n",
        "        batch_size = experiment_info[\"hyperparameters_data\"][\"batch_size\"],\n",
        "        shuffle = experiment_info[\"hyperparameters_data\"][\"shuffle_dataloader\"], \n",
        "        num_workers = experiment_info[\"hyperparameters_data\"][\"num_workers\"]\n",
        "        )\n",
        "\n",
        "    class_names = image_dataset.classes\n",
        "    dataset_size = len(image_dataset)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Setting hyperparameters\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=experiment_info[\"hyperparameters_training\"][\"learning_rate\"],\n",
        "        )\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(\n",
        "        optimizer, \n",
        "        step_size=experiment_info[\"hyperparameters_training\"][\"scheduler_step_size\"], \n",
        "        gamma=experiment_info[\"hyperparameters_training\"][\"scheduler_gamma\"],\n",
        "        )\n",
        "\n",
        "    model = train_model(model, \n",
        "                        exp_lr_scheduler, \n",
        "                        optimizer, \n",
        "                        criterion,  \n",
        "                        dataset_size, \n",
        "                        dataloader, \n",
        "                        num_epochs=num_epochs)\n",
        "    return model\n",
        "\n",
        "def label_and_move_images(model):\n",
        "    predicted_labels = predict_new_labels(model)\n",
        "    counts, bins, patches = plot_distribution()\n",
        "    n_keep = compute_keep(counts)\n",
        "    iteration_predictions = extract_kept_images(predicted_labels, n_keep)\n",
        "    save_images_predicted_json(iteration_predictions, num_iteration)\n",
        "    move_images_predicted(num_iteration)"
      ],
      "metadata": {
        "id": "Y6eYd4I9w9BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_iteration = 1"
      ],
      "metadata": {
        "id": "1HyvHrKF0TMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_m = f'/gdrive/MyDrive/checkpoints/missing_labels/model_it_{num_iteration}.pt'\n",
        "torch.save(model.state_dict(), path_m)"
      ],
      "metadata": {
        "id": "5s64vUmX0Gu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for num_iteration in range(2,10):\n",
        "    model = train_new_model()\n",
        "    path_m = f'/gdrive/MyDrive/checkpoints/missing_labels/model_it_{num_iteration}.pt'\n",
        "    torch.save(model.state_dict(), path_m)\n",
        "    label_and_move_images(model)    \n",
        "    print(num_iteration, \" completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "65lE3k70eTKo",
        "outputId": "bf2c0b3d-81a9-40f3-d33a-d73fb21b84b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 880/880 [01:54<00:00,  7.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.6766 Acc: 0.3875\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 880/880 [01:54<00:00,  7.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.5529 Acc: 0.5828\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 880/880 [01:57<00:00,  7.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.1413 Acc: 0.6680\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 880/880 [01:55<00:00,  7.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.8654 Acc: 0.7364\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 880/880 [01:55<00:00,  7.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6489 Acc: 0.7946\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 880/880 [01:57<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.4791 Acc: 0.8468\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 880/880 [01:55<00:00,  7.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.3545 Acc: 0.8908\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 880/880 [01:57<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.1969 Acc: 0.9528\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 880/880 [01:55<00:00,  7.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.1522 Acc: 0.9698\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 880/880 [01:57<00:00,  7.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.1335 Acc: 0.9772\n",
            "\n",
            "Training complete in 19m 20s\n",
            "Best val Acc: 0.977181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|████████▌ | 21866/25517 [02:59<00:29, 121.74it/s]\n",
            " 83%|████████▎ | 2161/2617 [00:00<00:00, 11199.60it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2  completed\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 947/947 [02:06<00:00,  7.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.6762 Acc: 0.3626\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 947/947 [02:04<00:00,  7.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.5669 Acc: 0.5460\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 947/947 [02:06<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.1853 Acc: 0.6293\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 947/947 [02:04<00:00,  7.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.9233 Acc: 0.6977\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 947/947 [02:06<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.7201 Acc: 0.7580\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 947/947 [02:04<00:00,  7.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.5570 Acc: 0.8092\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 947/947 [02:05<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.4369 Acc: 0.8515\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 947/947 [02:04<00:00,  7.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.2512 Acc: 0.9330\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 947/947 [02:06<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.2046 Acc: 0.9523\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 947/947 [02:04<00:00,  7.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.1784 Acc: 0.9621\n",
            "\n",
            "Training complete in 20m 53s\n",
            "Best val Acc: 0.962106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 19705/25517 [02:44<00:48, 120.10it/s]\n",
            " 83%|████████▎ | 2161/2617 [00:00<00:00, 10389.65it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3  completed\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1015/1015 [02:13<00:00,  7.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.6719 Acc: 0.3456\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1015/1015 [02:14<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.5832 Acc: 0.5189\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1015/1015 [02:15<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.2159 Acc: 0.5959\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1015/1015 [02:13<00:00,  7.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.9741 Acc: 0.6591\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1015/1015 [02:15<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.7754 Acc: 0.7226\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1015/1015 [02:13<00:00,  7.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6152 Acc: 0.7772\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1015/1015 [02:15<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.4873 Acc: 0.8257\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1015/1015 [02:13<00:00,  7.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.2888 Acc: 0.9184\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1015/1015 [02:14<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.2379 Acc: 0.9418\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1015/1015 [02:15<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.2131 Acc: 0.9526\n",
            "\n",
            "Training complete in 22m 23s\n",
            "Best val Acc: 0.952551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▉   | 17544/25517 [02:27<01:07, 118.95it/s]\n",
            " 83%|████████▎ | 2161/2617 [00:00<00:00, 10996.24it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4  completed\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1082/1082 [02:23<00:00,  7.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.7114 Acc: 0.3250\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1082/1082 [02:22<00:00,  7.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.6168 Acc: 0.4911\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1082/1082 [02:25<00:00,  7.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.2549 Acc: 0.5693\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1082/1082 [02:24<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.0167 Acc: 0.6354\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1082/1082 [02:24<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.8222 Acc: 0.6952\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1082/1082 [02:24<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6701 Acc: 0.7481\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1082/1082 [02:22<00:00,  7.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.5307 Acc: 0.8028\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1082/1082 [02:24<00:00,  7.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.3211 Acc: 0.9076\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1082/1082 [02:24<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.2688 Acc: 0.9317\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1082/1082 [02:23<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.2399 Acc: 0.9442\n",
            "\n",
            "Training complete in 23m 59s\n",
            "Best val Acc: 0.944189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 15383/25517 [02:11<01:26, 116.91it/s]\n",
            " 83%|████████▎ | 2161/2617 [00:00<00:00, 11075.54it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5  completed\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1150/1150 [02:33<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.7081 Acc: 0.3128\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1150/1150 [02:32<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.6393 Acc: 0.4718\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1150/1150 [02:32<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.2826 Acc: 0.5468\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1150/1150 [02:31<00:00,  7.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.0472 Acc: 0.6114\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1150/1150 [02:32<00:00,  7.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.8531 Acc: 0.6734\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1150/1150 [02:32<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.7017 Acc: 0.7301\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1150/1150 [02:32<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.5747 Acc: 0.7839\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1150/1150 [02:32<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.3548 Acc: 0.8953\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1150/1150 [02:32<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.2987 Acc: 0.9219\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1150/1150 [02:32<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.2652 Acc: 0.9371\n",
            "\n",
            "Training complete in 25m 25s\n",
            "Best val Acc: 0.937136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 13222/25517 [01:51<01:43, 118.59it/s]\n",
            " 83%|████████▎ | 2161/2617 [00:00<00:00, 10611.32it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6  completed\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1217/1217 [02:41<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.7454 Acc: 0.2994\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1217/1217 [02:41<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.6744 Acc: 0.4538\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1217/1217 [02:41<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.3258 Acc: 0.5289\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1217/1217 [02:39<00:00,  7.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.0866 Acc: 0.5878\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1217/1217 [02:41<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.9080 Acc: 0.6495\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1217/1217 [02:41<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.7519 Acc: 0.7058\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1217/1217 [02:41<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6245 Acc: 0.7601\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1217/1217 [02:41<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.3916 Acc: 0.8781\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1217/1217 [02:41<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.3325 Acc: 0.9084\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1217/1217 [02:40<00:00,  7.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.2999 Acc: 0.9223\n",
            "\n",
            "Training complete in 26m 53s\n",
            "Best val Acc: 0.922289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 11061/25517 [01:34<02:03, 116.87it/s]\n",
            " 83%|████████▎ | 2161/2617 [00:00<00:00, 11350.82it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7  completed\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1285/1285 [02:50<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.7233 Acc: 0.2946\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1285/1285 [02:48<00:00,  7.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.6822 Acc: 0.4406\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1285/1285 [02:50<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.3402 Acc: 0.5114\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1285/1285 [02:50<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.1167 Acc: 0.5729\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1285/1285 [02:48<00:00,  7.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.9325 Acc: 0.6336\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1285/1285 [02:50<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.7853 Acc: 0.6882\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1285/1285 [02:50<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6571 Acc: 0.7439\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1285/1285 [02:50<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.4237 Acc: 0.8660\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1285/1285 [02:48<00:00,  7.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.3596 Acc: 0.8986\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1285/1285 [02:50<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.3271 Acc: 0.9149\n",
            "\n",
            "Training complete in 28m 21s\n",
            "Best val Acc: 0.914915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▍      | 8900/25517 [01:13<02:17, 120.56it/s]\n",
            " 83%|████████▎ | 2161/2617 [00:00<00:00, 9987.67it/s] \n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8  completed\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1352/1352 [02:59<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.7281 Acc: 0.2901\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1352/1352 [02:59<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.7085 Acc: 0.4309\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1352/1352 [02:59<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.3777 Acc: 0.4976\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1352/1352 [02:59<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.1525 Acc: 0.5570\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1352/1352 [02:57<00:00,  7.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.9735 Acc: 0.6154\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1352/1352 [02:59<00:00,  7.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.8240 Acc: 0.6725\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1352/1352 [02:59<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6995 Acc: 0.7238\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1352/1352 [02:59<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.4589 Acc: 0.8497\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1352/1352 [02:59<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.3908 Acc: 0.8833\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1352/1352 [02:57<00:00,  7.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.3575 Acc: 0.9009\n",
            "\n",
            "Training complete in 29m 51s\n",
            "Best val Acc: 0.900904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▋       | 6739/25517 [00:59<02:44, 114.21it/s]\n",
            " 83%|████████▎ | 2161/2617 [00:00<00:00, 10843.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9  completed\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_iteration = 10\n",
        "model = train_new_model() \n",
        "path_m = f'/gdrive/MyDrive/checkpoints/missing_labels/model_it_{num_iteration}.pt'\n",
        "label_and_move_images(model)    \n",
        "print(num_iteration, \" completed\")\n",
        "\n",
        "for num_iteration in range(11,15):\n",
        "    destination_dir = 'data/task1/train_data/images/unlabeled/0'\n",
        "    if len(os.listdir(destination_dir)) == 0:\n",
        "        break\n",
        "    model = train_new_model() \n",
        "    # it may be tempting to train longer, but this may create bias due to \n",
        "    # using wrong labels that we have previously generated.\n",
        "    path_m = f'/gdrive/MyDrive/checkpoints/missing_labels/model_it_{num_iteration}.pt'\n",
        "    torch.save(model.state_dict(), path_m)\n",
        "    label_and_move_images(model)    \n",
        "    print(num_iteration, \" completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tR1_xmJ34hbX",
        "outputId": "74a524a6-919c-4872-9434-288953df4980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1499/1499 [03:21<00:00,  7.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.8150 Acc: 0.2674\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1499/1499 [03:19<00:00,  7.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.8394 Acc: 0.3972\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1499/1499 [03:18<00:00,  7.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.4894 Acc: 0.4638\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1499/1499 [03:20<00:00,  7.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.2566 Acc: 0.5212\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1499/1499 [03:18<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.0734 Acc: 0.5779\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1499/1499 [03:19<00:00,  7.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.9229 Acc: 0.6267\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1499/1499 [03:18<00:00,  7.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.7938 Acc: 0.6839\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1499/1499 [03:19<00:00,  7.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.5339 Acc: 0.8169\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1499/1499 [03:18<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.4656 Acc: 0.8512\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1499/1499 [03:18<00:00,  7.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.4269 Acc: 0.8710\n",
            "\n",
            "Training complete in 33m 14s\n",
            "Best val Acc: 0.871003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2061/2061 [00:17<00:00, 120.60it/s]\n",
            " 79%|███████▉  | 2061/2617 [00:00<00:00, 11304.16it/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11  completed\n",
            "Epoch 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [03:27<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.8613 Acc: 0.2604\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [03:26<00:00,  7.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.8961 Acc: 0.3888\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [03:26<00:00,  7.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.5528 Acc: 0.4516\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [03:27<00:00,  7.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.3079 Acc: 0.5101\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [03:27<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.1136 Acc: 0.5699\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [03:27<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.9555 Acc: 0.6221\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [03:29<00:00,  7.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.8283 Acc: 0.6692\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [03:29<00:00,  7.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.5589 Acc: 0.8065\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [03:28<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.4854 Acc: 0.8417\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [03:27<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.4461 Acc: 0.8634\n",
            "\n",
            "Training complete in 34m 37s\n",
            "Best val Acc: 0.863400\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-249-ca7a7efb8bb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpath_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'/gdrive/MyDrive/checkpoints/missing_labels/model_it_{num_iteration}.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlabel_and_move_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" completed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-247-c9e76bc4bace>\u001b[0m in \u001b[0;36mlabel_and_move_images\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlabel_and_move_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_new_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mn_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_keep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0miteration_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_kept_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_keep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-244-ab7f365fc473>\u001b[0m in \u001b[0;36mpredict_new_labels\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_new_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mdataset_size_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_dataset_unlabeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_unlabled_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     predicted_labels = {\n\u001b[1;32m     54\u001b[0m         \u001b[0;34m\"path\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-244-ab7f365fc473>\u001b[0m in \u001b[0;36mget_unlabled_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_unlabled_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/task1/train_data/images/unlabeled'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mimage_dataset_unlabeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     dataloader_unlabeled  = torch.utils.data.DataLoader(\n\u001b[1;32m     44\u001b[0m         \u001b[0mimage_dataset_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# is potentially overridden and thus could have a different logic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The class_to_idx parameter cannot be None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mextensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"Supported extensions are: {extensions if isinstance(extensions, str) else ', '.join(extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Found no valid file for the classes 0. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfMUlEQVR4nO3debgdVZnv8e+PhDlAQhIwhJAwKSIyGRRBERCVOTbKJA2oYNTmKl5ECIgtijYgF7B9VBQECUiDCDII2opAQBsZAjKFQUIaSCJDCIGEeXrvH2udyuZwzj51htp1ht/nefZzaq+qteqt2sl+96pVgyICMzMzgGXqDsDMzPoPJwUzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk8IQI+lnkr7VR22tI+l5ScPy+xmSDu2LtnN7f5B0cF+11431fk/S05KeqGHd20p6KO/XTzbbB5ImSQpJw1sdpw1eTgqDiKRHJL0kaYmkZyXdJOlLkorPOSK+FBEnlGxrp2bLRMRjETEiIt7og9iPl/Srdu3vEhHTe9t2N+NYB/g6sHFEvKOTZVaV9ENJj+Uv74fz+zF9EMJ3gR/n/Xp5HfugNyRNkXSnpMU5sV4nad2647LynBQGnz0iYhVgInAScDRwdl+vZBD/Ol0HWBgRT3U0U9JywLXAe4CdgVWBDwILgff3wfonArP6oJ2Wk7QBcB4pqa4GrAv8BOj1j4aGdajxR45VICL8GiQv4BFgp3Zl7wfeBDbJ788FvpenxwBXAc8CzwB/If1QOD/XeQl4HjgKmAQEcAjwGHBjQ9nw3N4M4ETgVmAxcAWwep63PTCvo3hJX66vAq/l9d3V0N6heXoZ4DjgUeAp0pfPanleWxwH59ieBr7ZZD+tlusvyO0dl9vfKW/zmzmOczuoeyjwJDCiSfvvzrE/S/qC37Nh3rmkL8qrgSXALcD6ed7D7fb78u32wTDg/+XtmwMc1m7/r0b6AfA4MB/4HjAsz/ss8NdcfxHwv8AuDXGtDvwS+Geef3nDvN2BO/P23ARs2sl2fxq4s8l+GQYcm7dzCXA7MCHP2wa4DXgu/92mod4M4PvA/+R9swGwEXAN6d/tg8A+DcvvCtyX1zEfOLLu/5sD6VV7AH714YfZQVLI5Y8BX87T57I0KZwI/AxYNr8+DKijtlj6xXsesDKwIh0nhfnAJnmZS4Ff5Xnb00lSyNPHty3bML/xC/HzwGxgPWAE8Fvg/HaxnZXj2gx4BXh3J/vpPFLCWiXX/QdwSGdxtqt7ETC9yfxlc5zHAssBO+Yvp3c17P+2XsVw4ALgos4+w3b74EvAA8AE0pf49e32/2XAz/O+X4OUnL+Y532WlHS/QPpy/jIpAbR93lcDvwZG5W34SC7fgpSEP5DrHZxjXL6DbV8PeBk4HdiBdokT+AZwD/AuQPlzGp23ZRFwYN4n++f3oxv2wWOk3tlwUvKbC3wuv9+ClCg3zss/Dnw4T48Ctqz7/+ZAerkbNjT8k/Qfr73XgHHAxIh4LSL+Evl/UhPHR8QLEfFSJ/PPj4h7I+IF4FvAPm0D0b10AHBaRMyJiOeBY4D92h3G+k5EvBQRdwF3kb503iLHsh9wTEQsiYhHgFNJX0hljCZ96XRma1LSOikiXo2I60i9sf0blrksIm6NiNdJSWHzkuveB/hhRMyNiGdISb1tu9Yk/UL+Wv58niJ9Oe/XUP/RiDgr0hjQdNJnv6akccAuwJciYlH+t3BDrjMV+HlE3BIRb0Qa33glb+dbRMQcUlIdD1wMPC3pXEkj8iKHAsdFxIOR3BURC4HdgIci4vyIeD0iLiQlvz0amj83ImblfbYz8EhE/DIv/3fSD5C987KvARtLWjVvzx0l96/hMYWhYjypm93eKaRftX+SNEfStBJtze3G/EdJvzr7YgB2rdxeY9vDgTUbyhrPFnqR9OXc3pgcU/u2xpeMYyHpy7RZnHMj4s0m7ZeJs9O227XbZiJpux7PJxk8S+o1rNHReiPixTw5gtTzeCYiFnWwzonA19vazO1OyLG8TUTcHBH7RMRYUs9zO+CbefYE0qGjjrbr0XZl7fdZ43ZPBD7QLqYDgLYTAz5FSpCPSrpB0gc7itU65qQwyEnaivSf66/t5+Vfyl+PiPWAPYEjJH20bXYnTXbVk5jQML0O6Vfb08ALwEoNcQ0Dxnaj3X+Svgwa236ddHy/O57OMbVva37J+n8GPiFp5SZxTmg3GNqd9pt5nLfv3zZzSb/gx0TEyPxaNSLeU6LducDqkkZ2Mu/7DW2OjIiV8q/5piLiNtJhvk0a2lq/g0Xbf7bw9n3W+O9jLnBDu5hGRMSX29YbEVNICfFyUq/FSnJSGKTyaZO7k46B/yoi7ulgmd0lbSBJpAG+N0gDnZC+bNfrwar/VdLGklYinV55ST5c8Q9gBUm7SVqWNLi7fEO9J4FJTc4suRD4v5LWzYcj/gP4dT6cUFqO5WLg+5JWkTQROAL4VfOahfNJX0qXStpI0jKSRks6VtKupIHjF4GjJC0raXvSYZCLuhNnJy4GvippbUmjgKJnFxGPA38CTs2f/TKS1pf0ka4azXX/APxU0qgc93Z59lnAlyR9IJ/5s3L+DFdp346kD0n6gqQ18vuNSD82bs6L/AI4QdKGua1NJY0Gfg+8U9JnJA2XtC+wMemwW0euyssfmGNdVtJWkt4taTlJB0haLSJeI53w8GYn7VgHnBQGn99JWkL64vomcBppQK4jG5J++T4P/A34aURcn+edCByXu+dHdmP955MGU58AVgC+ChARzwH/RvpimE/qOcxrqPeb/HehpI6OAZ+T276RdObMy8BXuhFXo6/k9c8h9aD+K7ffpYh4hXSW0gOks18WkwZ0xwC3RMSrpCSwC6lX8lPgoIh4oIexNjoL+CNpvOQO0q/wRgeRBrfvIw3UXkLzQ12NDiT1oB4gDSx/DSAiZpIGp3+c25xNGrTuyLOkJHCPpOeB/yYNfv8gzz+NlNj+RNpvZwMr5nGF3Umnsi4kne22e0Q83dFKImIJ8HHSeMk/Sf/WTmbpj4wDgUckLSYNzh9Qch8YS888MDMzc0/BzMyWclIwM7OCk4KZmRWcFMzMrDCgb2o2ZsyYmDRpUt1hmJkNKLfffvvT+QLDtxnQSWHSpEnMnDmz7jDMzAYUSe2vIC/48JGZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVBvQVzWZmAJOmXd3juo+ctFsfRjLwuadgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzAq+IZ6Z9Yne3JQOfGO6/sJJwcyGNN9h9a18+MjMzAruKZhZv9Dbw0/WN9xTMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzK/jsIzOzHhqM1zi4p2BmZoXKewqShgEzgfkRsbukdYGLgNHA7cCBEfGqpOWB84D3AQuBfSPikarjMxtsBuOvV2udVvQUDgfub3h/MnB6RGwALAIOyeWHAIty+el5OTMza6FKewqS1gZ2A74PHCFJwI7AZ/Ii04HjgTOAKXka4BLgx5IUEVFljGa2lK8qtqp7Cj8EjgLezO9HA89GxOv5/TxgfJ4eD8wFyPOfy8u/haSpkmZKmrlgwYIqYzczG3IqSwqSdgeeiojb+7LdiDgzIiZHxOSxY8f2ZdNmZkNelYePtgX2lLQrsAKwKvCfwEhJw3NvYG1gfl5+PjABmCdpOLAaacDZzMxapLKeQkQcExFrR8QkYD/guog4ALge+HRe7GDgijx9ZX5Pnn+dxxPMzFqrjusUjiYNOs8mjRmcncvPBkbn8iOAaTXEZmY2pLXkiuaImAHMyNNzgPd3sMzLwN6tiMfMzDrmK5rNzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlboVlKQNErSplUFY2Zm9eoyKUiaIWlVSasDdwBnSTqt+tDMzKzVyvQUVouIxcBewHkR8QFgp2rDMjOzOpS5onm4pHHAPsA3K47HzGxI6O2zK6p6Sl6ZnsJ3gT8CsyPiNknrAQ9VEo2ZmdWqy55CRPwG+E3D+znAp6oMyszM6tFlUpA0FvgCMKlx+Yj4fHVhmZlZHcqMKVwB/AX4M/BGteGYmVmdyiSFlSLi6MojMTOz2pUZaL4qP1LTzMwGuTJJ4XBSYnhZ0pL8Wlx1YGZm1nplzj5apRWBmJlZ/Uo9jlPSnsB2+e2MiLiqupDMzKwuZe59dBLpENJ9+XW4pBOrDszMzFqvTE9hV2DziHgTQNJ04O/AMVUGZmZmrVf21tkjG6ZXqyIQMzOrX5mewonA3yVdD4g0tjCt0qjMzKwWZc4+ulDSDGCrXHR0RDxRaVRmZlaLTg8fSdoo/90SGAfMy6+1cpmZmQ0yzXoKRwBTgVM7mBfAjpVEZGZmtek0KUTE1Dy5S0S83DhP0gqVRmVmZrUoc/bRTSXLzMxsgOu0pyDpHcB4YEVJW5DOPAJYFVipBbGZmVmLNRtT+ATwWWBt4LSG8iXAsRXGZGZmNWk2pjAdmC7pUxFxaQtjMjOzmpS5TuFSSbsB7wFWaCj/bpWBmQ1lk6ZdXXcINkSVuSHez4B9ga+QxhX2BiZWHJeZmdWgzNlH20TEQcCiiPgO8EHgndWGZWZmdSiTFF7Kf1+UtBbwGukKZzMzG2TK3BDvKkkjgVOAO0hXM59VaVRmZlaLLnsKEXFCRDybz0CaCGwUEf/eVT1JK0i6VdJdkmZJ+k4uX1fSLZJmS/q1pOVy+fL5/ew8f1LvNs3MzLqrzEDz3ZKOlbR+RLwSEc+VbPsVYMeI2AzYHNhZ0tbAycDpEbEBsAg4JC9/CGncYgPg9LycmZm1UJkxhT2A14GLJd0m6UhJ63RVKZLn89tl86vtRnqX5PLpwCfz9JT8njz/o5LarqI2M7MWKHP46NGI+EFEvA/4DLAp8L9lGpc0TNKdwFPANcDDwLMR8XpeZB7pVhrkv3PzOl8HngNGd9DmVEkzJc1csGBBmTDMzKykUo/jlDRR0lHARcBGwFFl6kXEGxGxOelWGe/PdXslIs6MiMkRMXns2LG9bc7MzBp0efaRpFtIh34uBvaOiDndXUlEPJsf5/lBYKSk4bk3sDYwPy82H5gAzJM0nPQs6IXdXZeZmfVc06QgaRngtxHR7UFfSWOB13JCWBH4GGnw+Hrg06Rex8HAFbnKlfn93/L86yIiurtes/7At6mwgarp4aOIeJN0W4ueGAdcL+lu4Dbgmoi4CjgaOELSbNKYwdl5+bOB0bn8CGBaD9drZmY9VObitT9LOhL4NfBCW2FEPNOsUkTcDWzRQfkc0vhC+/KX6XkCMjOzPlAmKeyb/x7WUBbAen0fjpmZ1anMrbPXbUUgZmZWvzJXNK8k6ThJZ+b3G0ravfrQzMys1cpcp/BL4FVgm/x+PvC9yiIyM7PalEkK60fED0i3zCYiXiQ9bMfMzAaZMknh1XydQQBIWp90szszMxtkypx99G3gv4EJki4AtgU+W2VQZmZWjzJnH10j6Q5ga9Jho8Mj4unKIzMzs5Yrc/bRtsDLEXE1MBI4VtLEyiMzM7OWKzOmcAbp+cybkW4/8TBwXqVRmZlZLcokhdfzjemmAD+JiJ8Aq1QblpmZ1aHMQPMSSccABwIfzndOXbbasMzMrA5legr7kk5B/XxEPEF6BsIplUZlZma1KPM4zieA/wJGSdoDeDUiPKZgZjYIlTn76FDgVmAv0sNvbpb0+aoDMzOz1iszpvANYIuIWAggaTRwE3BOlYGZmVnrlRlTWAgsaXi/BD872cxsUOq0pyDpiDw5G7hF0hWk+x9NAe5uQWxmZtZizQ4ftV2L8HB+tbmiunDMzKxOnSaFiPhO27SkEbns+VYEZWZm9Wg6piDpy5IeAx4FHpX0qKR/a01oZmbWap0mBUnHAXsA20fE6IgYDewA7JLnmZnZINOsp3AgsFdEzGkryNP7AAdVHZiZmbVes6QQEfFyB4UvAW9WF5KZmdWlWVKYL+mj7Qsl7Qg8Xl1IZmZWl2anpH4VuELSX4Hbc9lk0uM4p1QdmJmZtV6nPYWImAVsAtwITMqvG4FN8jwzMxtkmt77KI8p+B5HNiRNmnZ13SGYtVyZex+ZmdkQ4aRgZmaFZhevXZv/nty6cMzMrE7NxhTGSdoG2FPSRYAaZ0bEHZVGZmZmLdcsKfw78C3SM5lPazcvgB2rCsrMzOrR7C6plwCXSPpWRJzQwpjMzKwmXT6OMyJOkLQnsF0umhERV1UblpmZ1aHLs48knQgcDtyXX4dL+o+qAzMzs9Yrc0rqbsDHIuKciDgH2BnYvatKkiZIul7SfZJmSTo8l68u6RpJD+W/o3K5JP1I0mxJd0vasjcbZmZm3Vf2OoWRDdOrlazzOvD1iNgY2Bo4TNLGwDTg2ojYELg2vwfYBdgwv6YCZ5Rcj5mZ9ZEuxxSAE4G/S7qedFrqdiz9Iu9URDxOvptqRCyRdD8wnnQzve3zYtOBGcDRufy8iAjgZkkjJY3L7ZiZWQuUGWi+UNIMYKtcdHREPNGdlUiaBGwB3AKs2fBF/wSwZp4eD8xtqDYvl70lKUiaSupJsM4663QnDDMz60KZnkLbr/4re7ICSSOAS4GvRcRiaek1cBERkqI77UXEmcCZAJMnT+5WXTMza67Sex9JWpaUEC6IiN/m4icljcvzxwFP5fL5wISG6mvnMjMza5HKkoJSl+Bs4P6IaLwi+krg4Dx9MHBFQ/lB+SykrYHnPJ5gZtZaTQ8fSRoGzIqIjXrQ9rbAgcA9ku7MZccCJwEXSzoEeBTYJ8/7PbArMBt4EfhcD9ZpZma90NVDdt6Q9KCkdSLise40HBF/pd1N9Bq87dnP+ayjw7qzDjMz61tlBppHAbMk3Qq80FYYEXtWFpWZmdWiTFL4VuVRmJlZv1DmOoUbJE0ENoyIP0taCRhWfWhmZtZqZW6I9wXgEuDnuWg8cHmVQZmZWT3KnJJ6GOlMosUAEfEQsEaVQZmZWT3KJIVXIuLVtjeShpOevGZmZoNMmaRwg6RjgRUlfQz4DfC7asMyM7M6lEkK04AFwD3AF0kXmR1XZVBmZlaPMmcfvSlpOukOpwE8mC80MzOzQabLpCBpN+BnwMOkK5TXlfTFiPhD1cGZmVlrlbl47VRgh4iYDSBpfeBqwEnBzGyQKTOmsKQtIWRzgCUVxWNmZjXqtKcgaa88OVPS74GLSWMKewO3tSA2MzNrsWaHj/ZomH4S+EieXgCsWFlEZmZWm06TQkT4eQZmZkNMmbOP1gW+AkxqXN63zraBYNK0q+sOwWxAKXP20eWkx2r+Dniz2nDMzKxOZZLCyxHxo8ojMTOz2pVJCv8p6dvAn4BX2goj4o7KojIzs1qUSQrvBQ4EdmTp4aPI783MbBApkxT2BtZrvH22mZkNTmWuaL4XGFl1IGZmVr8yPYWRwAOSbuOtYwo+JdXMbJApkxS+XXkUZmbWL5R5nsINrQjEzMzqV+aK5iUsfSbzcsCywAsRsWqVgZmZWeuV6Sms0jYtScAUYOsqgzIzs3qUOfuoEMnlwCcqisfMzGpU5vDRXg1vlwEmAy9XFpGZmdWmzNlHjc9VeB14hHQIyczMBpkyYwp+roKZ2RDR7HGc/96kXkTECRXEY2ZmNWrWU3ihg7KVgUOA0YCTgrWEH5Rj1jrNHsd5atu0pFWAw4HPARcBp3ZWz8zMBq6mYwqSVgeOAA4ApgNbRsSiVgRmZmat12xM4RRgL+BM4L0R8XzLojIzs1o0u3jt68BawHHAPyUtzq8lkhZ31bCkcyQ9JenehrLVJV0j6aH8d1Qul6QfSZot6W5JW/Z2w8zMrPs6TQoRsUxErBgRq0TEqg2vVUre9+hcYOd2ZdOAayNiQ+Da/B5gF2DD/JoKnNHdDTEzs97r1m0uuiMibgSeaVc8hTQ2Qf77yYby8/JtNG4GRkoaV1VsZmbWscqSQifWjIjH8/QTwJp5ejwwt2G5ebnsbSRNlTRT0swFCxZUF6mZ2RDU6qRQiIhg6S25u1PvzIiYHBGTx44dW0FkZmZDV6uTwpNth4Xy36dy+XxgQsNya+cyMzNroVYnhSuBg/P0wcAVDeUH5bOQtgaeazjMZGZmLVLmLqk9IulCYHtgjKR5pGc9nwRcLOkQ4FFgn7z474FdgdnAi6Qrp83MrMUqSwoRsX8nsz7awbIBHFZVLGZmVk5tA81mZtb/OCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZoXKLl4zazRp2tV1h2BmJbinYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzK/g2F1aKb1NhNjS4p2BmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUzMyv4NhdDiG9VYWZdcU/BzMwKTgpmZlZwUjAzs4KTgpmZFTzQPMB4sNjMqtSvegqSdpb0oKTZkqbVHY+Z2VDTb3oKkoYBPwE+BswDbpN0ZUTcV29kHfMvdjMbjPpNUgDeD8yOiDkAki4CpgCVJAV/qZuZvV1/SgrjgbkN7+cBH2i/kKSpwNT89nlJD/ZwfWOAp3tYt7f1h1rdOtc9EOvWuW5v88Coi07uVf2Jnc3oT0mhlIg4Ezizt+1ImhkRk+uoP9Tq1rnugVi3znV7mwdG3b6o35n+NNA8H5jQ8H7tXGZmZi3Sn5LCbcCGktaVtBywH3BlzTGZmQ0p/ebwUUS8Lun/AH8EhgHnRMSsClfZ20NQvak/1OrWue6BWLfOdXubB0bdvqjfIUVEFe2amdkA1J8OH5mZWc2cFMzMrDAkk4KkKZLulnSnpJmSPtSiugfkuvdIuknSZq2oW+e6JW0k6W+SXpF0ZDdj7nHdOtfdm9u11FW3znUPtW2WdI6kpyTd2914+6J+lyJiyL2AESwdT9kUeKBFdbcBRuXpXYBbWlG3znUDawBbAd8HjuxmzD2uW9e6SSdJPAysBywH3AVs3J/rDtS4B/A2bwdsCdzb3X/TfVG/q9eQ7ClExPOR9y6wMlB6tL2XdW+KiEX57c2kazEqr1vnuiPiqYi4DXitdLB9ULfGdRe3a4mIV4G227X057oDNe4Buc0RcSPwTDfi7NP6XRmSSQFA0r9IegC4Gvh8q+o2OAT4Qw116173YNfR7VrG9/O6da57KG5zv9ZvrlNotYi4DLhM0nbACcBOragLIGkH0pdr6fGIvqhb97rNrP8bMj0FSYflweE7Ja3VVp67YutJGtOKupI2BX4BTImIhd2JuTt161x3Z/urjN7UrXvdWW9u11JX3TrXPRS3uX+rYqCiv7+ADVg6WLwl6cNUC+quA8wGtulBzD2uW/e6cxvH04PB4t7WbfW6Sb3vOcC6LB2AfE9/rjtQ4x6o25zrT6IXA8W9rd+07Soa7e8v4GhgFnAn8DfgQy2q+wtgUa57JzCzFXXrXDfwDtLx1sXAs3l61arr1rluYFfgH6SzU77Zzc+plroDNe6BuM3AhcDjpJMY5gGHdHO9varf1cu3uTAzs8KQGVMwM7OuOSmYmVnBScHMzApOCmZmVnBSMDOzgpOCDSiS3iHpIkkPS7pd0u8lvbOHbX1Y0qx8sdp4SZd0stwMSX3+gPQS8W0t6ZYc3/2Sjm91DDb0DNnbXNjAI0nAZcD0iNgvl20GrEk6X7y7DgBOjIhf5fef7pNA+850YJ+IuEvSMOBdvW1Q0rCIeKP3odlg5Z6CDSQ7AK9FxM/aCiLiroj4i5JTJN2r9NyHfQEkbZ9/6V8i6QFJF+RlDwX2AU7IZZPa7k8vacXcG7lf0mXAim3rk/Rxpecs3CHpN5JG5PJHJH0nl98jaaNcPkLSL3PZ3ZI+1ayddtYgXaRERLwREfd10eb+uexeSSc3xPy8pFMl3QV8UNK/Sro190B+nhOOGeCkYAPLJsDtnczbC9gc2Ix0g8JTJI3L87YAvgZsTLr//bYR8QvgSuAbEXFAu7a+DLwYEe8Gvg28DyDf4+o4YKeI2BKYCRzRUO/pXH4G0PZgnm8Bz0XEeyNiU+C6Eu20OR14UNJlkr4oaYUmba4FnAzsmPfDVpI+mZdfmfQMjM2AhcC+eR9sDrxB6jGZAT58ZIPHh4AL86GRJyXdQHpIzmLg1oiYByDpTtJ9Y/7apK3tgB8BRMTdku7O5VuTEsv/pCNZLEe61Umb3+a/t5OSFKQEtV/bAhGxSNLuXbTTtux3JV0AfBz4DLA/sH0nbW4HzIiIBXk7L8jbcTnpi//SvPhHSUnutrzuFYGnmuwLG2KcFGwgmUXPjvu/0jD9Bj3/dy/gmojYv4v1dLWOrtopRMTDwBmSzgIWSBrdnYCzlxvGEUQakzmmB+3YEODDRzaQXAcsL2lqW4GkTSV9GPgLsK+kYZLGkn4l39rD9dxI+mWOpE1Ij12F9NS5bSVtkOetXOLMp2uAwxriHVW2HUm75cF1gA1JyebZTtq8FfiIpDF5jGB/4IYO4rkW+LSkNXLd1SVN7GIbbAhxUrABI9LdG/8F2CmfkjoLOBF4gnRW0t2kWxhfBxwVEU/0cFVnACMk3Q98lzyOkQ/NfBa4MB9S+huwURdtfQ8YlQd/7wJ26EY7B5LGFO4EzgcOyL/4O2rzcWAacH3eB7dHxBXtG8yD1ccBf8rrvgYY1345G7p8l1QzMyu4p2BmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFf4/5t76txjWJJgAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_and_move_images(model)    \n",
        "print(num_iteration, \" completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "m3_-e6MkA4Xv",
        "outputId": "b585ef9a-53e2-4b00-96aa-86f23180e210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4578/4578 [00:39<00:00, 115.50it/s]\n",
            " 96%|█████████▌| 2517/2617 [00:00<00:00, 11057.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10  completed\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "destination_dir = 'data/task1/train_data/images/unlabeled/0'\n",
        "len(os.listdir(destination_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSyTEN6T4JU1",
        "outputId": "af22c32e-49ce-49a3-f9ba-e80fa7ed92d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = get_unlabled_dataset()\n",
        "predicted_labels = predict_new_labels(model, *args)\n",
        "counts, bins, patches = plot_distribution()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "QTjWycj04rUb",
        "outputId": "85edf4da-6761-4eb1-95d4-c33d1d38c00a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 4578/25517 [00:47<03:37, 96.17it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in tqdm(df_labeled.iterrows(),total=n_keep):\n",
        "    parts = row['path'].split(\".\")\n",
        "    parts[0]+=f\"_it_{num_iteration}\"\n",
        "    new_name = \".\".join(parts).split('/')[-1]\n",
        "    dest_path = f\"data/task1/labeled/{row['label']}/{new_name}\"\n",
        "    print(dest_path)\n",
        "    break\n",
        "    shutil.move(row['path'], dest_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL-QNcuce7oA",
        "outputId": "64030e82-c5e2-4248-b0d2-dcbc7820909d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2617 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/task1/labeled/1/10008_it_2.jpeg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def undo_changes(num_iteration):\n",
        "    d_dir = \"data/task1/labeled/\"\n",
        "    for d in os.listdir(d_dir):\n",
        "        for i in os.listdir(d_dir+\"/\"+d):\n",
        "            if f\"_it_{num_iteration}\" in i:\n",
        "                new_name = i.split(\".\")\n",
        "                old_name = new_name[0][:-5]+\".\"+new_name[1]\n",
        "                shutil.move(f\"{d_dir}/{d}/{i}\", f\"data/task1/train_data/images/unlabeled/0/{old_name}\")\n",
        "            "
      ],
      "metadata": {
        "id": "iU8rO8uQfp7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uW1vKBQjPpcw"
      },
      "outputs": [],
      "source": [
        "# k-fold cross-validation,  \n",
        "# or importance sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting for evaluation"
      ],
      "metadata": {
        "id": "Kw7yy7_JvR11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: prepare dataset for evaluation\n",
        "# TODO: load model for evaluation\n",
        "# TODO: check out the labeling requirements for evaluaiton."
      ],
      "metadata": {
        "id": "YjINPHqdwima"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring other solutions"
      ],
      "metadata": {
        "id": "afy8oG2ARxYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering the images \n",
        "Using a DNN for feature extraction, than clustering the images on those features. "
      ],
      "metadata": {
        "id": "hqERGlUR1LwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import shutil\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import SubsetRandomSampler, Dataset, DataLoader\n",
        "from torchvision import transforms, datasets \n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import itertools"
      ],
      "metadata": {
        "id": "Npf3odl1K-3z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a pytorch MobileNet pretrained model\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
        "# remove the last fully connected layer ( this is the classification layer)\n",
        "\n",
        "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "\n",
        "# Freeze the model so the pre-trained weights are not updated during training\n",
        "for param in model.parameters():\n",
        "    param.requiresGrad = False"
      ],
      "metadata": {
        "id": "QU_5cAKc1NEV",
        "outputId": "9c9f6b3f-dae7-4250-e78e-ca5ace8e84af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting features from labeled data"
      ],
      "metadata": {
        "id": "i5ZRg2mx62Xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "IVPEo6akO5kA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the experiment information from the JSON file\n",
        "with open('experiment_info.json', 'r') as f:\n",
        "    experiment_info = json.load(f)"
      ],
      "metadata": {
        "id": "dvSOJ660NP_v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(experiment_info[\"image_processing\"][\"resize\"]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=experiment_info[\"image_processing\"][\"mean\"],\n",
        "                         std=experiment_info[\"image_processing\"][\"std\"]),\n",
        "    ])\n",
        "\n",
        "data_dir = 'data/task1/labeled'\n",
        "image_dataset = datasets.ImageFolder(data_dir, preprocess) \n",
        "\n",
        "dataloader  = DataLoader(\n",
        "    image_dataset, \n",
        "    batch_size = 1,\n",
        "    shuffle = experiment_info[\"hyperparameters_data\"][\"shuffle_dataloader\"], \n",
        "    num_workers = experiment_info[\"hyperparameters_data\"][\"num_workers\"]\n",
        "    )\n",
        "\n",
        "class_names = image_dataset.classes\n",
        "dataset_size = len(image_dataset)"
      ],
      "metadata": {
        "id": "_6Ptz03d9rcr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "img_path_generator_labeled = ((image, path) for (path,_) , (image, label) in zip(image_dataset.samples, dataloader))\n",
        "\n",
        "features = {\"image_id\":[], \"features\":[]}\n",
        "\n",
        "model = model.to(\"cuda\")\n",
        "\n",
        "# Iterate over data.\n",
        "for inputs, image_id in tqdm(img_path_generator_labeled, total = dataset_size):\n",
        "    inputs = inputs.to(\"cuda\")\n",
        "\n",
        "    # forward\n",
        "    with torch.set_grad_enabled(False):\n",
        "        output = model(inputs)\n",
        "    features[\"image_id\"].append(image_id)\n",
        "    features[\"features\"].append(output)\n",
        "    "
      ],
      "metadata": {
        "id": "GNee8iBf7Foe",
        "outputId": "fc7ef463-2e85-408e-d573-910054b267cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23555/23555 [04:07<00:00, 95.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "img_path_generator_labeled = ((image, path) for (path,_) , (image, label) in zip(image_dataset.samples, dataloader))\n",
        "\n",
        "features = {\"image_id\":[], \"features\":[]}\n",
        "\n",
        "model = model.to(\"cuda\")\n",
        "\n",
        "# Iterate over data.\n",
        "for inputs, image_id in tqdm(img_path_generator_labeled, total = dataset_size):\n",
        "    inputs = inputs.to(\"cuda\")\n",
        "\n",
        "    # forward\n",
        "    with torch.set_grad_enabled(False):\n",
        "        output = model(inputs)\n",
        "    # features[\"image_id\"].append(image_id)\n",
        "    # features[\"features\"].append(output)\n",
        "    "
      ],
      "metadata": {
        "id": "z-5jrBEXfN17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features[\"features\"] = [f.to('cpu').numpy() for f in features[\"features\"]]"
      ],
      "metadata": {
        "id": "3oUTXP5rBL4d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = features[\"features\"][0].shape\n",
        "print(f\"shape of features {s}\")\n",
        "print(\"number of pixels in 1 image\", 64*64 *3)\n",
        "print(\"number of features\")\n",
        "features[\"features\"][0].shape[1] * (features[\"features\"][0].shape[2]**2)"
      ],
      "metadata": {
        "id": "IhX4M1FYOSJY",
        "outputId": "d1a4779a-b5e8-4d49-925d-8d027957aaad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of features (1, 1280, 7, 7)\n",
            "number of pixels in 1 image 12288\n",
            "number of features\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62720"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "12_000_000 / (62_720*4) # images would fit in memory"
      ],
      "metadata": {
        "id": "cVSyCNCItCw8",
        "outputId": "9c3a86e7-959e-46b8-bece-90430522fd5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47.83163265306123"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(features[\"features\"]) # 23555"
      ],
      "metadata": {
        "id": "OW5IUUnDv8th",
        "outputId": "309bf393-d756-49fc-c61a-8ea6fa70896b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23555"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "ids = []\n",
        "for id in tqdm(features[\"image_id\"]):\n",
        "    label = id.split(\".\")[0].split('/')[-2]\n",
        "    id_image_labeled = id.split(\".\")[0].split('/')[-1]\n",
        "    labels.append(int(label))\n",
        "    ids.append(id_image_labeled)"
      ],
      "metadata": {
        "id": "B4E0VdLBx-2K",
        "outputId": "6328a0ab-835b-49ae-b9ab-26cb0d917990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23555/23555 [00:00<00:00, 631955.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.asarray(labels)\n",
        "X_train = np.asarray(features[\"features\"])\n",
        "X_train = X_train.reshape((X_train.shape[0], -1))"
      ],
      "metadata": {
        "id": "e0xOSwUEx9a2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an IPCA object\n",
        "ipca = IncrementalPCA(n_components=100, batch_size=32)\n",
        "\n",
        "# Fit the IPCA to your training data\n",
        "ipca.fit(X_train)\n",
        "\n",
        "# Use the IPCA to transform the training and test data\n",
        "X_train_pca = ipca.transform(X_train)\n",
        "# X_test_pca = ipca.transform(X_test)"
      ],
      "metadata": {
        "id": "-YhSO_4R0Bn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_pca,\n",
        "                                                    np.asarray(labels), \n",
        "                                                    test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the k-NN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "nbrs = NearestNeighbors(n_neighbors=5,\n",
        "                        algorithm='kd_tree')\n",
        "\n",
        "nbrs.fit(X_train, y_train)\n",
        "# Fit the classifier to the training data\n",
        "# knn.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "rvbYa_leEJTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create an instance of the KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "# Train your model on the transformed data\n",
        "knn.fit(X_train_pca, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Print the accuracy of the classifier\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')"
      ],
      "metadata": {
        "id": "UVBNNQM-vu99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extract features from unlabeled data\n",
        "features_unlabeled = model(X_unlabeled).detach().numpy()\n",
        "\n",
        "# Predict labels for unlabeled data\n",
        "predictions = knn.predict(features_unlabeled)"
      ],
      "metadata": {
        "id": "SYscwGVlyf4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting features from unlabeled data"
      ],
      "metadata": {
        "id": "FBKYJNcc-E77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check to see that all images have been moved to unlabeled/0 dir\n",
        "assert len(os.listdir(destination_dir)) == 26445, \" You need to create the proper unlabeled dataset, like in previous method\" # 26445\n",
        "# set the source and destination directories \n",
        "# in the data/task1/train_data/images/unlabeled"
      ],
      "metadata": {
        "id": "OZIBh6tB_CLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new dataloader with unlabeled data.\n",
        "# Dataset folder is a subclass of ImageFolder \n",
        "# Which will allow us to return also the path of the image\n",
        "# because we need it to know which images should be moved \n",
        "\n",
        "from torchvision.datasets import DatasetFolder\n",
        "\n",
        "data_dir = 'data/task1/train_data/images/unlabeled'\n",
        "image_dataset_unlabeled = datasets.ImageFolder(root=data_dir, transform=preprocess)\n",
        "\n",
        "dataloader_unlabeled  = torch.utils.data.DataLoader(\n",
        "    image_dataset_unlabeled, \n",
        "    batch_size = 1, \n",
        "    )\n",
        "\n",
        "dataset_size = len(image_dataset_unlabeled)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "HDgHhl6c_CLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_unlabeled = {\"image_id\":[], \"features\":[]}\n",
        "\n",
        "# Each epoch has a training and validation phase \n",
        "model.eval()   # Set model to evaluate mode \n",
        "\n",
        "model = model.to(device)\n",
        "# Iterate over data.\n",
        "img_path_generator = ((image, path) for (path,_) , (image, _) in zip(image_dataset_unlabeled.samples, dataloader_unlabeled))\n",
        "\n",
        "for inputs, path in tqdm(img_path_generator, total=len(image_dataset_unlabeled)):\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    # forward\n",
        "    with torch.set_grad_enabled(False): # we don't want to train\n",
        "        outputs = model(inputs) \n",
        "    features_unlabeled[\"image_id\"].append(path)\n",
        "    features_unlabeled[\"features\"].append(output)\n",
        "    \n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "OhAsvDB8_CLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'/gdrive/MyDrive/checkpoints/missing_labels/features_unlabeled.json', 'w') as f:\n",
        "    json.dump(features, f)\n",
        " \n",
        "# Load the experiment information from the JSON file\n",
        "with open(f'/gdrive/MyDrive/checkpoints/missing_labels/features_unlabeled.json', 'r') as f:\n",
        "    features = json.load(f)"
      ],
      "metadata": {
        "id": "mdwLhyliLdOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, id in enumerate(features_unlabeled[\"image_id\"]):\n",
        "    label = id.split(\".\")[0].split('/')[-2]\n",
        "    id_image_labeled = id.split(\".\")[0].split('/')[-1]\n",
        "    # TODO: add 200000 to the images that are unlabeled \n",
        "    # such that I don't get duplicated labels\n",
        "    print(label, id_image_labeled)\n",
        "    print(id)\n",
        "    if i == 3:\n",
        "        break"
      ],
      "metadata": {
        "id": "F3dYljNWGKt_",
        "outputId": "f99d0729-9e6d-4454-da4c-10b8561e3c6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0\n",
            "data/task1/train_data/images/unlabeled/0/0.jpeg\n",
            "0 1\n",
            "data/task1/train_data/images/unlabeled/0/1.jpeg\n",
            "0 10\n",
            "data/task1/train_data/images/unlabeled/0/10.jpeg\n",
            "0 100\n",
            "data/task1/train_data/images/unlabeled/0/100.jpeg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-ensembling \n",
        "Based on the models we have created previously, we will create an ensemble of models. And we will take the outputs of all the previous models as features for another model of a few fully connected layers. \n",
        "\n",
        "The reasoning behind this approach is that ensembling the models may eliminate the biases between them. And give us a higher accuracy."
      ],
      "metadata": {
        "id": "eFcJHihA9XcB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HZGmlGc-9amz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT0e5c1wPpcw"
      },
      "source": [
        "## Test the model on the new dataset "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to test if the 2 models are better, we cannot just use the labels we have predicted\n",
        "# we need to either choose the validation set from the labels we already know are true\n",
        "# or we could use another dataset only for testing to compare that self-training is better \n",
        "# than just training on a smaller dataset with all labels given."
      ],
      "metadata": {
        "id": "16kjJl1S5ZtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo7mrv5XgHa6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# This will shuffle the images in the dataset \n",
        "# before they are returned to the data loader,\n",
        "# which should help ensure that the validation and training sets\n",
        "# are more balanced.\n",
        "class ShuffledImageFolder(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.indices = torch.randperm(len(dataset))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.dataset[self.indices[index]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "data_dir = 'data/task1/labeled'\n",
        "image_dataset_unshuffled = datasets.ImageFolder(data_dir, preprocess)\n",
        "image_dataset = ShuffledImageFolder(image_dataset_unshuffled)\n",
        "\n",
        "# print(image_datasets)\n",
        "# We don't want to create a single dataset because we want to have a dataset for evaluation also\n",
        "# dataloader  = torch.utils.data.DataLoader(image_dataset, batch_size = 4, shuffle = True, num_workers = 4)\n",
        "\n",
        "# split data into train and val\n",
        "\n",
        "dataset_size = len(image_dataset)\n",
        "print(dataset_size)\n",
        "split = int(dataset_size*0.8)\n",
        "train_size = split\n",
        "print(train_size)\n",
        "val_size = dataset_size - split\n",
        "print(val_size)\n",
        "assert val_size+train_size == dataset_size\n",
        "class_names = image_dataset.dataset.classes\n",
        "\n",
        "# Create a sampler for the training set\n",
        "train_sampler = SubsetRandomSampler(range(split))\n",
        "\n",
        "# Create a sampler for the valuation set\n",
        "val_sampler = SubsetRandomSampler(range(split, dataset_size))\n",
        "\n",
        "# Create DataLoaders for the training and valuation sets\n",
        "train_dataloader = DataLoader(image_dataset, batch_size=32, sampler=train_sampler)\n",
        "val_dataloader = DataLoader(image_dataset, batch_size=32, sampler=val_sampler, shuffle=False)\n",
        "dataloaders = {\n",
        "    \"train\":train_dataloader,\n",
        "    \"val\":val_dataloader,\n",
        "}\n",
        "dataset_sizes = {\n",
        "    \"train\":train_size,\n",
        "    \"val\":val_size,\n",
        "}\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in tqdm(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "diofrUY_jsMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting hyperparameters\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "KoH-8q7gjvjv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 23:03:10) [MSC v.1916 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
      }
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a275e3abe13e468980a250b54e91cce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd626424afd5436083780d3cde02183d",
              "IPY_MODEL_0e0f18dedf33441b94d1c9af20145937",
              "IPY_MODEL_89bd4155836b4534833cc9bbd4fe49c1"
            ],
            "layout": "IPY_MODEL_10e839197ffd46ac96eaf4516560aa18"
          }
        },
        "cd626424afd5436083780d3cde02183d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de5364f2f3734bccbe99ccb326f583e0",
            "placeholder": "​",
            "style": "IPY_MODEL_e7841a1807b6457b8b256198c9f7bae8",
            "value": "100%"
          }
        },
        "0e0f18dedf33441b94d1c9af20145937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce6888951733437397f2758cff71350b",
            "max": 14212972,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e74c9da734048169485cb0cd5ce858c",
            "value": 14212972
          }
        },
        "89bd4155836b4534833cc9bbd4fe49c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfe4982d664d445bbc8366d25c1c8bb3",
            "placeholder": "​",
            "style": "IPY_MODEL_846994e6f25b4dba80a1f88d54de5471",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 106MB/s]"
          }
        },
        "10e839197ffd46ac96eaf4516560aa18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de5364f2f3734bccbe99ccb326f583e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7841a1807b6457b8b256198c9f7bae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce6888951733437397f2758cff71350b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e74c9da734048169485cb0cd5ce858c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfe4982d664d445bbc8366d25c1c8bb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "846994e6f25b4dba80a1f88d54de5471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}